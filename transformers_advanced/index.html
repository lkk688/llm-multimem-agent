
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../multi_modal_LM/">
      
      
        <link rel="next" href="../inference_optimization/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>Advanced Transformer Techniques - Multimodal Memory LLM and AI Agent</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4af4bdda.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#modern-transformer-modifications-and-optimizations" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-header__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Multimodal Memory LLM and AI Agent
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Advanced Transformer Techniques
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-nav__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Multimodal Memory LLM and AI Agent
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core Modules
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Core Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../self-supervised/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embeddings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodal Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Frameworks and Architectures
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../memory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tool Calling and Agent Capabilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi_modal_LM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Modal Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Advanced Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Advanced Transformer Techniques
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Advanced Transformer Techniques
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architectural-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Architectural Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architectural Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#limitations-of-the-original-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations of the Original Transformer Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-xl" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer-XL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reformer" class="md-nav__link">
    <span class="md-ellipsis">
      Reformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linformer" class="md-nav__link">
    <span class="md-ellipsis">
      Linformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performer" class="md-nav__link">
    <span class="md-ellipsis">
      Performer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fnet" class="md-nav__link">
    <span class="md-ellipsis">
      FNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Sparse Transformers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanism-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanism Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention Mechanism Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-query-attention-mqa" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Query Attention (MQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      Grouped-Query Attention (GQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-level-attention-mla" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Level Attention (MLA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sliding-window-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Sliding Window Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#positional-encoding-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Positional Encoding Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Positional Encoding Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rotary-positional-encoding-rope" class="md-nav__link">
    <span class="md-ellipsis">
      Rotary Positional Encoding (RoPE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      ALiBi (Attention with Linear Biases)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-optimization-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Training and Optimization Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training and Optimization Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mixture-of-experts-moe" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture of Experts (MoE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mixture of Experts (MoE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-norm-vs-post-norm" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-Norm vs Post-Norm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-analysis-and-comparisons" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Analysis and Comparisons
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance Analysis and Comparisons">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computational-complexity-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Complexity Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-usage-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Usage Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Scaling Behavior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-vs-efficiency-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      Quality vs Efficiency Trade-offs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-guidelines-and-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Guidelines and Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Guidelines and Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#choosing-the-right-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing the Right Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-implementation-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Common Implementation Patterns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-profiling" class="md-nav__link">
    <span class="md-ellipsis">
      Debugging and Profiling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-directions-and-research-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions and Research Trends
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Future Directions and Research Trends">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#emerging-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Emerging Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Emerging Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mamba-and-state-space-models" class="md-nav__link">
    <span class="md-ellipsis">
      Mamba and State Space Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retnet-retentive-networks" class="md-nav__link">
    <span class="md-ellipsis">
      RetNet (Retentive Networks)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-of-depths-mod" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture of Depths (MoD)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#research-frontiers" class="md-nav__link">
    <span class="md-ellipsis">
      Research Frontiers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comprehensive-references-and-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Comprehensive References and Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Comprehensive References and Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#foundational-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Foundational Papers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-implementations" class="md-nav__link">
    <span class="md-ellipsis">
      Model Implementations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-optimization-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization Tools
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_architecture_evolution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT Architecture Evolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../physical_ai_autonomous_driving/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physical AI in Autonomous Driving
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notebooks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Notebooks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../notebooks/memory_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Example
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architectural-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Architectural Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architectural Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#limitations-of-the-original-transformer-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations of the Original Transformer Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-xl" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer-XL
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#reformer" class="md-nav__link">
    <span class="md-ellipsis">
      Reformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linformer" class="md-nav__link">
    <span class="md-ellipsis">
      Linformer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performer" class="md-nav__link">
    <span class="md-ellipsis">
      Performer
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fnet" class="md-nav__link">
    <span class="md-ellipsis">
      FNet
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Sparse Transformers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#attention-mechanism-optimizations" class="md-nav__link">
    <span class="md-ellipsis">
      Attention Mechanism Optimizations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Attention Mechanism Optimizations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#flashattention" class="md-nav__link">
    <span class="md-ellipsis">
      FlashAttention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-query-attention-mqa" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Query Attention (MQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      Grouped-Query Attention (GQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-level-attention-mla" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Level Attention (MLA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sliding-window-attention" class="md-nav__link">
    <span class="md-ellipsis">
      Sliding Window Attention
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#positional-encoding-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Positional Encoding Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Positional Encoding Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#rotary-positional-encoding-rope" class="md-nav__link">
    <span class="md-ellipsis">
      Rotary Positional Encoding (RoPE)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alibi-attention-with-linear-biases" class="md-nav__link">
    <span class="md-ellipsis">
      ALiBi (Attention with Linear Biases)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-optimization-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Training and Optimization Innovations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Training and Optimization Innovations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mixture-of-experts-moe" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture of Experts (MoE)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mixture of Experts (MoE)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pre-norm-vs-post-norm" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-Norm vs Post-Norm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#performance-analysis-and-comparisons" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Analysis and Comparisons
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Performance Analysis and Comparisons">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#computational-complexity-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      Computational Complexity Comparison
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-usage-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Memory Usage Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#scaling-behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Scaling Behavior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#quality-vs-efficiency-trade-offs" class="md-nav__link">
    <span class="md-ellipsis">
      Quality vs Efficiency Trade-offs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-guidelines-and-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Guidelines and Best Practices
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Guidelines and Best Practices">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#choosing-the-right-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Choosing the Right Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-checklist" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Checklist
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-implementation-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      Common Implementation Patterns
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-profiling" class="md-nav__link">
    <span class="md-ellipsis">
      Debugging and Profiling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-directions-and-research-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions and Research Trends
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Future Directions and Research Trends">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#emerging-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Emerging Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Emerging Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mamba-and-state-space-models" class="md-nav__link">
    <span class="md-ellipsis">
      Mamba and State Space Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#retnet-retentive-networks" class="md-nav__link">
    <span class="md-ellipsis">
      RetNet (Retentive Networks)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-of-depths-mod" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture of Depths (MoD)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#research-frontiers" class="md-nav__link">
    <span class="md-ellipsis">
      Research Frontiers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#comprehensive-references-and-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Comprehensive References and Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Comprehensive References and Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#foundational-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Foundational Papers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Resources
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-implementations" class="md-nav__link">
    <span class="md-ellipsis">
      Model Implementations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-optimization-tools" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Optimization Tools
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      Conclusion
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="modern-transformer-modifications-and-optimizations">Modern Transformer Modifications and Optimizations</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#architectural-innovations">Architectural Innovations</a></li>
<li><a href="#limitations-of-the-original-transformer-architecture">Limitations of Original Transformer</a></li>
<li><a href="#transformer-xl">Transformer-XL</a></li>
<li><a href="#reformer">Reformer</a></li>
<li><a href="#linformer">Linformer</a></li>
<li><a href="#performer">Performer</a></li>
<li><a href="#fnet">FNet</a></li>
<li><a href="#sparse-transformers">Sparse Transformers</a></li>
<li><a href="#attention-mechanism-optimizations">Attention Mechanism Optimizations</a></li>
<li><a href="#flashattention">FlashAttention</a></li>
<li><a href="#multi-query-attention-mqa">Multi-Query Attention (MQA)</a></li>
<li><a href="#grouped-query-attention-gqa">Grouped-Query Attention (GQA)</a></li>
<li><a href="#multi-level-attention-mla">Multi-Level Attention (MLA)</a></li>
<li><a href="#sliding-window-attention">Sliding Window Attention</a></li>
<li><a href="#xformers-memory-efficient-attention">Xformers Memory-Efficient Attention</a></li>
<li><a href="#training-and-scaling-innovations">Training and Scaling Innovations</a></li>
<li><a href="#rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</a></li>
<li><a href="#alibi-attention-with-linear-biases">ALiBi (Attention with Linear Biases)</a></li>
<li><a href="#decoupled-knowledge-and-position-encoding">Decoupled Knowledge and Position Encoding</a></li>
<li><a href="#mixture-of-experts-moe">Mixture of Experts (MoE)</a></li>
<li><a href="#normalization-techniques">Normalization Techniques</a></li>
<li><a href="#rmsnorm">RMSNorm</a></li>
<li><a href="#pre-normalization-vs-post-normalization">Pre-normalization vs. Post-normalization</a></li>
<li><a href="#performance-comparisons">Performance Comparisons</a></li>
<li><a href="#implementation-guidelines">Implementation Guidelines</a></li>
<li><a href="#future-directions">Future Directions</a></li>
<li><a href="#references">References</a></li>
</ol>
<h2 id="introduction">Introduction</h2>
<p>The Transformer architecture, introduced by Vaswani et al. in "Attention Is All You Need" (2017), has become the foundation of modern natural language processing and beyond. <mcreference link="https://jalammar.github.io/illustrated-transformer/" index="1">1</mcreference> However, the original architecture has several limitations that have driven extensive research into modifications and optimizations. This comprehensive guide explores the most significant advances in Transformer architectures, from efficiency improvements to scaling innovations.</p>
<p><img alt="Transformer Architecture" src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png" /> <mcreference link="https://jalammar.github.io/illustrated-transformer/" index="1">1</mcreference></p>
<p><em>Figure 1: The standard Transformer architecture showing encoder-decoder structure with self-attention and feed-forward layers.</em></p>
<p>The evolution of Transformer architectures can be categorized into several key areas:</p>
<ul>
<li><strong>Efficiency Improvements</strong>: Reducing computational complexity and memory usage through innovations like FlashAttention <mcreference link="https://arxiv.org/abs/2205.14135" index="2">2</mcreference></li>
<li><strong>Scaling Innovations</strong>: Enabling larger models and longer sequences with techniques like Mixture of Experts <mcreference link="https://huggingface.co/blog/moe" index="3">3</mcreference></li>
<li><strong>Training Optimizations</strong>: Improving training stability and convergence</li>
<li><strong>Architectural Refinements</strong>: Enhancing model expressiveness and capability with emerging alternatives like State Space Models <mcreference link="https://arxiv.org/abs/2312.00752" index="4">4</mcreference></li>
</ul>
<p>Each modification addresses specific limitations while often introducing new trade-offs, making the choice of architecture dependent on the specific use case and constraints. Modern developments have pushed the boundaries from the original 512-token context windows to models capable of processing millions of tokens efficiently.</p>
<h2 id="architectural-innovations">Architectural Innovations</h2>
<h3 id="limitations-of-the-original-transformer-architecture">Limitations of the Original Transformer Architecture</h3>
<p>Before exploring solutions, it's crucial to understand the fundamental limitations that drive architectural innovations:</p>
<p><strong>1. Quadratic Complexity</strong></p>
<p>The self-attention mechanism has <span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span> computational and memory complexity with respect to sequence length <span class="arithmatex">\(<span class="arithmatex">\(n\)</span>\)</span>. For a sequence of length <span class="arithmatex">\(<span class="arithmatex">\(n\)</span>\)</span> with embedding dimension <span class="arithmatex">\(<span class="arithmatex">\(d\)</span>\)</span>, the attention computation requires:</p>
<div class="arithmatex">\[\text{Memory} = O(n^2 + nd) \quad \text{Computation} = O(n^2d + nd^2)\]</div>
<p>This quadratic scaling becomes prohibitive for long sequences. For example, processing a 10K token sequence requires 100× more attention computation than a 1K token sequence.</p>
<p><strong>2. Fixed Context Window</strong></p>
<p>Standard Transformers process fixed-length sequences, typically limited by memory constraints. This creates several issues:
- <strong>Context Fragmentation</strong>: Long documents must be split into chunks, losing cross-chunk dependencies
- <strong>Positional Encoding Limitations</strong>: Models cannot generalize to sequences longer than training data
- <strong>Information Bottleneck</strong>: Important context may be lost when truncating sequences</p>
<p><strong>3. Memory Inefficiency</strong></p>
<p>Beyond attention matrices, Transformers require substantial memory for:
- <strong>Activation Storage</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(L \cdot n \cdot d)\)</span>\)</span> for <span class="arithmatex">\(<span class="arithmatex">\(L\)</span>\)</span> layers during backpropagation
- <strong>Gradient Computation</strong>: Additional memory for storing gradients
- <strong>KV Cache</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(L \cdot n \cdot d)\)</span>\)</span> for autoregressive generation</p>
<p><strong>4. Inference Latency</strong></p>
<p>Autoregressive generation requires sequential token production, leading to:
- <strong>Sequential Dependency</strong>: Each token depends on all previous tokens
- <strong>Memory Bandwidth Bottleneck</strong>: Repeatedly loading large KV caches
- <strong>Underutilized Parallelism</strong>: Cannot fully leverage parallel computing resources</p>
<p><strong>Research Directions and Solutions:</strong></p>
<table>
<thead>
<tr>
<th>Problem</th>
<th>Research Direction</th>
<th>Example Solutions</th>
<th>Complexity Reduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Quadratic Complexity</td>
<td>Efficient Attention</td>
<td>Linformer, Reformer, Performer, Sparse Transformers</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2) \rightarrow O(n \log n)\)</span>\)</span> or <span class="arithmatex">\(<span class="arithmatex">\(O(n)\)</span>\)</span></td>
</tr>
<tr>
<td>Fixed Context Window</td>
<td>Recurrence &amp; Memory</td>
<td>Transformer-XL, Compressive Transformers</td>
<td>Infinite theoretical context</td>
</tr>
<tr>
<td>Position Encoding</td>
<td>Alternative Representations</td>
<td>RoPE, ALiBi, T5 relative positions</td>
<td>Better extrapolation</td>
</tr>
<tr>
<td>Memory Inefficiency</td>
<td>Parameter Efficiency</td>
<td>Reversible layers, Gradient checkpointing, LoRA</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(L \cdot n \cdot d) \rightarrow O(n \cdot d)\)</span>\)</span></td>
</tr>
<tr>
<td>Inference Latency</td>
<td>Parallelization &amp; Caching</td>
<td>Speculative decoding, KV-caching, MQA/GQA</td>
<td>Reduced memory bandwidth</td>
</tr>
</tbody>
</table>
<h3 id="transformer-xl">Transformer-XL</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/1901.02860">Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/kimiyoung/transformer-xl">kimiyoung/transformer-xl</a>
- 🤗 <strong>HuggingFace</strong>: <a href="https://huggingface.co/docs/transformers/model_doc/transfo-xl">Transformer-XL Documentation</a></p>
<p><strong>Motivation:</strong> Enable Transformers to handle arbitrarily long sequences and capture dependencies beyond fixed context windows.</p>
<p><strong>Core Innovation:</strong> Transformer-XL introduces two key mechanisms:</p>
<ol>
<li><strong>Segment-Level Recurrence</strong>: Information flows between consecutive segments</li>
<li><strong>Relative Positional Encoding</strong>: Position information is relative rather than absolute</li>
</ol>
<p><strong>Mathematical Formulation:</strong></p>
<p>For the <span class="arithmatex">\(<span class="arithmatex">\(\tau\)</span>\)</span>-th segment, the hidden states are computed as:</p>
<div class="arithmatex">\[\mathbf{h}_\tau^{(n)} = \text{TransformerLayer}\left(\mathbf{h}_\tau^{(n-1)}, \text{SG}(\mathbf{h}_{\tau-1}^{(n-1)})\right)\]</div>
<p>where:
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{h}_\tau^{(n)}\)</span>\)</span>: Hidden state for segment <span class="arithmatex">\(<span class="arithmatex">\(\tau\)</span>\)</span> at layer <span class="arithmatex">\(<span class="arithmatex">\(n\)</span>\)</span>
- <span class="arithmatex">\(<span class="arithmatex">\(\text{SG}(\cdot)\)</span>\)</span>: Stop-gradient operation to prevent backpropagation through previous segments
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{h}_{\tau-1}^{(n-1)}\)</span>\)</span>: Cached hidden state from the previous segment</p>
<p><strong>Relative Positional Encoding:</strong></p>
<p>The attention score incorporates relative position information:</p>
<div class="arithmatex">\[A_{i,j} = \mathbf{q}_i^\top \mathbf{k}_j + \mathbf{q}_i^\top \mathbf{W}_{k,R} \mathbf{R}_{i-j} + \mathbf{u}^\top \mathbf{k}_j + \mathbf{v}^\top \mathbf{W}_{k,R} \mathbf{R}_{i-j}\]</div>
<p>where:
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{R}_{i-j}\)</span>\)</span>: Relative positional encoding for distance <span class="arithmatex">\(<span class="arithmatex">\(i-j\)</span>\)</span>
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{W}_{k,R}\)</span>\)</span>: Learnable transformation for relative positions
- <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{u}, \mathbf{v}\)</span>\)</span>: Learnable global bias vectors</p>
<p>This formulation has four terms:
1. <strong>Content-based addressing</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{q}_i^\top \mathbf{k}_j\)</span>\)</span>
2. <strong>Content-dependent positional bias</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{q}_i^\top \mathbf{W}_{k,R} \mathbf{R}_{i-j}\)</span>\)</span>
3. <strong>Global content bias</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{u}^\top \mathbf{k}_j\)</span>\)</span>
4. <strong>Global positional bias</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{v}^\top \mathbf{W}_{k,R} \mathbf{R}_{i-j}\)</span>\)</span></p>
<p><strong>Implementation Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RelativeMultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span> <span class="o">=</span> <span class="n">n_head</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_head</span>

        <span class="c1"># Linear projections for Q, K, V</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kv_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Relative position encoding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">r_net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_head</span> <span class="o">*</span> <span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Global bias vectors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_head</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_head</span><span class="p">,</span> <span class="n">d_head</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">d_head</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mems</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># w: [seq_len, batch_size, d_model] - current segment</span>
        <span class="c1"># r: [seq_len, d_model] - relative position encodings</span>
        <span class="c1"># mems: [mem_len, batch_size, d_model] - cached from previous segment</span>

        <span class="n">qlen</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">w</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">mems</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Concatenate memory with current input</span>
            <span class="n">cat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">mems</span><span class="p">,</span> <span class="n">w</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">klen</span> <span class="o">=</span> <span class="n">cat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cat</span> <span class="o">=</span> <span class="n">w</span>
            <span class="n">klen</span> <span class="o">=</span> <span class="n">qlen</span>

        <span class="c1"># Compute Q, K, V</span>
        <span class="n">w_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_net</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>  <span class="c1"># [qlen, bsz, n_head * d_head]</span>
        <span class="n">r_head_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">r_net</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>  <span class="c1"># [qlen, n_head * d_head]</span>

        <span class="n">kv_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kv_net</span><span class="p">(</span><span class="n">cat</span><span class="p">)</span>  <span class="c1"># [klen, bsz, 2 * n_head * d_head]</span>
        <span class="n">k_head_h</span><span class="p">,</span> <span class="n">v_head_h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="n">kv_heads</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Reshape for multi-head attention</span>
        <span class="n">w_head_q</span> <span class="o">=</span> <span class="n">w_heads</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">qlen</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">k_head_h</span> <span class="o">=</span> <span class="n">k_head_h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">klen</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">v_head_h</span> <span class="o">=</span> <span class="n">v_head_h</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">klen</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">r_head_k</span> <span class="o">=</span> <span class="n">r_head_k</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">qlen</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_head</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Compute attention scores with relative positions</span>
        <span class="c1"># Term 1: content-based addressing</span>
        <span class="n">AC</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ibnd,jbnd-&gt;ijbn&#39;</span><span class="p">,</span> <span class="n">w_head_q</span><span class="p">,</span> <span class="n">k_head_h</span><span class="p">)</span>

        <span class="c1"># Term 2: content-dependent positional bias</span>
        <span class="n">BD</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ibnd,jnd-&gt;ijbn&#39;</span><span class="p">,</span> <span class="n">w_head_q</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">u</span><span class="p">,</span> <span class="n">r_head_k</span><span class="p">)</span>

        <span class="c1"># Combine terms</span>
        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">AC</span> <span class="o">+</span> <span class="n">BD</span>
        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">attn_score</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

        <span class="c1"># Apply attention mask if provided</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_score</span> <span class="o">=</span> <span class="n">attn_score</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>

        <span class="c1"># Softmax and dropout</span>
        <span class="n">attn_prob</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_score</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_prob</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_prob</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">attn_vec</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ijbn,jbnd-&gt;ibnd&#39;</span><span class="p">,</span> <span class="n">attn_prob</span><span class="p">,</span> <span class="n">v_head_h</span><span class="p">)</span>
        <span class="n">attn_vec</span> <span class="o">=</span> <span class="n">attn_vec</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">qlen</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">attn_vec</span>
</code></pre></div>
<p><strong>Key Benefits:</strong></p>
<ol>
<li><strong>Infinite Context</strong>: Theoretical ability to capture dependencies of arbitrary length</li>
<li><strong>Better Extrapolation</strong>: Relative positions generalize to unseen sequence lengths</li>
<li><strong>Improved Perplexity</strong>: Significant improvements on language modeling tasks</li>
<li><strong>Efficient Caching</strong>: Memory states can be reused across segments</li>
</ol>
<p><strong>Limitations:</strong></p>
<ol>
<li><strong>Training Complexity</strong>: Requires careful handling of segment boundaries</li>
<li><strong>Memory Overhead</strong>: Must store and manage cached states</li>
<li><strong>Implementation Complexity</strong>: More complex than standard attention</li>
</ol>
<p><strong>Popularity:</strong> Medium-high; influential in design but less directly used today.</p>
<p><strong>Models/Frameworks:</strong> Transformer-XL, XLNet, influenced GPT-3's context handling and modern long-context models.</p>
<h3 id="reformer">Reformer</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2001.04451">Reformer: The Efficient Transformer</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/google/trax/tree/master/trax/models/reformer">google/trax</a>
- 🤗 <strong>HuggingFace</strong>: <a href="https://huggingface.co/docs/transformers/model_doc/reformer">Reformer Documentation</a></p>
<p><strong>Motivation:</strong> Dramatically reduce memory and computational complexity to enable processing of very long sequences (up to 1M tokens).</p>
<p><strong>Core Innovations:</strong></p>
<ol>
<li><strong>Locality-Sensitive Hashing (LSH) Attention</strong></li>
<li><strong>Reversible Residual Layers</strong></li>
<li><strong>Chunked Feed-Forward Layers</strong></li>
</ol>
<p><strong>LSH Attention Mathematical Foundation:</strong></p>
<p>Instead of computing attention between all <span class="arithmatex">\(<span class="arithmatex">\(n^2\)</span>\)</span> token pairs, LSH attention groups similar tokens using hash functions and computes attention only within groups.</p>
<p><strong>Hash Function:</strong>
For a query vector <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{q}\)</span>\)</span>, the LSH function maps it to a bucket:</p>
<div class="arithmatex">\[h(\mathbf{q}) = \arg\max_i (\mathbf{q}^\top \mathbf{r}_i)\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{r}_i\)</span>\)</span> are random vectors drawn from a spherical Gaussian distribution.</p>
<p><strong>Multi-Round Hashing:</strong>
To improve recall, multiple hash functions are used:</p>
<div class="arithmatex">\[\mathcal{H} = \{h_1, h_2, \ldots, h_R\}\]</div>
<p>Tokens are considered similar if they hash to the same bucket in any round.</p>
<p><strong>Attention Computation:</strong>
For each token <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span>, attention is computed only with tokens in the same hash bucket:</p>
<div class="arithmatex">\[\text{Attention}_i = \text{softmax}\left(\frac{\mathbf{q}_i \mathbf{K}_{\mathcal{B}(i)}^\top}{\sqrt{d}}\right) \mathbf{V}_{\mathcal{B}(i)}\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(\mathcal{B}(i)\)</span>\)</span> is the set of tokens in the same bucket as token <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span>.</p>
<p><strong>Complexity Analysis:</strong>
- <strong>Standard Attention</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span>
- <strong>LSH Attention</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n \log n \cdot d)\)</span>\)</span> on average</p>
<p><strong>Reversible Layers:</strong></p>
<p>Inspired by RevNets, Reformer uses reversible residual connections to eliminate the need to store activations during backpropagation.</p>
<p><strong>Forward Pass:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}_1 = \mathbf{x}_1 + F(\mathbf{x}_2)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{y}_2 = \mathbf{x}_2 + G(\mathbf{y}_1)\)</span>\)</span></p>
<p><strong>Backward Pass (Reconstruction):</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}_2 = \mathbf{y}_2 - G(\mathbf{y}_1)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{x}_1 = \mathbf{y}_1 - F(\mathbf{x}_2)\)</span>\)</span></p>
<p><strong>Memory Reduction:</strong>
- <strong>Standard</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(L \cdot n \cdot d)\)</span>\)</span> for <span class="arithmatex">\(<span class="arithmatex">\(L\)</span>\)</span> layers
- <strong>Reversible</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot d)\)</span>\)</span> (constant in depth)</p>
<p><strong>Implementation Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LSHAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_hashes</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">bucket_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span> <span class="o">=</span> <span class="n">n_hashes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span> <span class="o">=</span> <span class="n">bucket_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>

        <span class="c1"># Projections (note: in LSH attention, Q and K are the same)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_qk</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">hash_vectors</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectors</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply LSH to group similar vectors&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Generate random projection vectors</span>
        <span class="n">random_rotations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_hashes</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">vectors</span><span class="o">.</span><span class="n">device</span>
        <span class="p">)</span>

        <span class="c1"># Reshape vectors for hashing</span>
        <span class="n">vectors</span> <span class="o">=</span> <span class="n">vectors</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply rotations and compute hash codes</span>
        <span class="n">rotated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;...ij,hjk-&gt;...hik&#39;</span><span class="p">,</span> <span class="n">vectors</span><span class="p">,</span> <span class="n">random_rotations</span><span class="p">)</span>
        <span class="n">hash_codes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rotated</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">hash_codes</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V (Q and K are the same in LSH attention)</span>
        <span class="n">qk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_qk</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Reshape for multi-head attention</span>
        <span class="n">qk</span> <span class="o">=</span> <span class="n">qk</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply LSH to group similar vectors</span>
        <span class="n">hash_codes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash_vectors</span><span class="p">(</span><span class="n">qk</span><span class="p">)</span>

        <span class="c1"># Sort by hash codes to group similar vectors</span>
        <span class="n">sorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">hash_codes</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Gather vectors according to sorted indices</span>
        <span class="n">qk_sorted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">qk</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">v_sorted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sorted_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Compute attention within buckets</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span><span class="p">):</span>
            <span class="n">end_idx</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bucket_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

            <span class="n">qk_chunk</span> <span class="o">=</span> <span class="n">qk_sorted</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>
            <span class="n">v_chunk</span> <span class="o">=</span> <span class="n">v_sorted</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">end_idx</span><span class="p">]</span>

            <span class="c1"># Standard attention within the chunk</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">qk_chunk</span><span class="p">,</span> <span class="n">qk_chunk</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">chunk_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v_chunk</span><span class="p">)</span>

            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">chunk_output</span><span class="p">)</span>

        <span class="c1"># Concatenate outputs and unsort</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Unsort to original order</span>
        <span class="n">unsorted_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">sorted_indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">unsorted_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_out</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ReversibleBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">f_block</span><span class="p">,</span> <span class="n">g_block</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">f</span> <span class="o">=</span> <span class="n">f_block</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="n">g_block</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">y1</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="n">y2</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward_pass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="n">dy1</span><span class="p">,</span> <span class="n">dy2</span><span class="p">):</span>
        <span class="c1"># Reconstruct x2 and x1</span>
        <span class="n">x2</span> <span class="o">=</span> <span class="n">y2</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">y1</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>

        <span class="c1"># Compute gradients</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">x1</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
            <span class="n">x2</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

            <span class="n">y1_recompute</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">f</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
            <span class="n">y2_recompute</span> <span class="o">=</span> <span class="n">x2</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span><span class="p">(</span><span class="n">y1_recompute</span><span class="p">)</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">([</span><span class="n">y1_recompute</span><span class="p">,</span> <span class="n">y2_recompute</span><span class="p">],</span> <span class="p">[</span><span class="n">dy1</span><span class="p">,</span> <span class="n">dy2</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">x1</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">x2</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div>
<p><strong>Performance Characteristics:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Standard Transformer</th>
<th>Reformer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Complexity</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(L \cdot n \cdot d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot d)\)</span>\)</span></td>
</tr>
<tr>
<td>Attention Complexity</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2 \cdot d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \log n \cdot d)\)</span>\)</span></td>
</tr>
<tr>
<td>Max Sequence Length</td>
<td>~2K tokens</td>
<td>~1M tokens</td>
</tr>
<tr>
<td>Training Speed</td>
<td>Baseline</td>
<td>0.8× (due to hashing overhead)</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Medium; more influential for ideas than direct implementation.</p>
<p><strong>Models/Frameworks:</strong> Research models, some specialized long-document applications.</p>
<h3 id="linformer">Linformer</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2006.04768">Linformer: Self-Attention with Linear Complexity</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/tatp22/linformer-pytorch">tatp22/linformer-pytorch</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2103.03404">Linear Attention Analysis</a></p>
<p><strong>Motivation:</strong> Achieve linear complexity in sequence length while maintaining the expressiveness of full attention.</p>
<p><strong>Core Insight:</strong> The attention matrix <span class="arithmatex">\(<span class="arithmatex">\(A \in \mathbb{R}^{n \times n}\)</span>\)</span> is often low-rank, especially for long sequences where many tokens have similar attention patterns.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Standard Attention:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(Q, K, V \in \mathbb{R}^{n \times d}\)</span>\)</span>.</p>
<p><strong>Linformer Attention:</strong>
Introduce projection matrices <span class="arithmatex">\(<span class="arithmatex">\(E, F \in \mathbb{R}^{k \times n}\)</span>\)</span> where <span class="arithmatex">\(<span class="arithmatex">\(k \ll n\)</span>\)</span>:</p>
<div class="arithmatex">\[\text{Linformer}(Q, K, V) = \text{softmax}\left(\frac{Q(EK)^T}{\sqrt{d_k}}\right)(FV)\]</div>
<p><strong>Complexity Analysis:</strong>
- <strong>Standard</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span> time, <span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span> space
- <strong>Linformer</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(nkd)\)</span>\)</span> time, <span class="arithmatex">\(<span class="arithmatex">\(O(nk)\)</span>\)</span> space</p>
<p><strong>Theoretical Justification:</strong></p>
<p>The attention matrix can be approximated using its SVD decomposition:
<span class="arithmatex">\(<span class="arithmatex">\(A = U\Sigma V^T \approx U_k\Sigma_k V_k^T\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(U_k, V_k\)</span>\)</span> contain the top <span class="arithmatex">\(<span class="arithmatex">\(k\)</span>\)</span> singular vectors. Linformer learns projections that approximate this low-rank structure.</p>
<p><strong>Projection Matrix Design:</strong></p>
<p>Linformer explores several projection strategies:</p>
<ol>
<li><strong>Linear Projection</strong>: <span class="arithmatex">\(<span class="arithmatex">\(E, F\)</span>\)</span> are learned parameters</li>
<li><strong>Convolution</strong>: Use 1D convolutions for local structure</li>
<li><strong>Mean/Max Pooling</strong>: Simple downsampling operations</li>
</ol>
<p><strong>Implementation with Multiple Projection Strategies:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LinformerAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">projection_type</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># Projected dimension</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection_type</span> <span class="o">=</span> <span class="n">projection_type</span>

        <span class="c1"># Standard Q, K, V projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Projection matrices for K and V</span>
        <span class="k">if</span> <span class="n">projection_type</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">E</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">seq_len</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">F</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">seq_len</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">projection_type</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
            <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">E_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">F_conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_projection</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">proj_type</span><span class="o">=</span><span class="s1">&#39;E&#39;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply projection to reduce sequence length dimension&quot;&quot;&quot;</span>
        <span class="c1"># x: [batch_size, seq_len, d_model]</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_type</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
            <span class="n">proj_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">E</span> <span class="k">if</span> <span class="n">proj_type</span> <span class="o">==</span> <span class="s1">&#39;E&#39;</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">F</span>
            <span class="c1"># Project: [k, seq_len] @ [batch_size, seq_len, d_model] -&gt; [batch_size, k, d_model]</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;ks,bsd-&gt;bkd&#39;</span><span class="p">,</span> <span class="n">proj_matrix</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_type</span> <span class="o">==</span> <span class="s1">&#39;conv&#39;</span><span class="p">:</span>
            <span class="n">conv_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">E_conv</span> <span class="k">if</span> <span class="n">proj_type</span> <span class="o">==</span> <span class="s1">&#39;E&#39;</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">F_conv</span>
            <span class="c1"># Reshape for conv1d: [batch_size * d_model, 1, seq_len]</span>
            <span class="n">x_reshaped</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
            <span class="c1"># Apply convolution</span>
            <span class="n">x_conv</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="n">x_reshaped</span><span class="p">)</span>  <span class="c1"># [batch_size * d_model, 1, k]</span>
            <span class="c1"># Reshape back: [batch_size, d_model, k] -&gt; [batch_size, k, d_model]</span>
            <span class="k">return</span> <span class="n">x_conv</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_type</span> <span class="o">==</span> <span class="s1">&#39;mean_pool&#39;</span><span class="p">:</span>
            <span class="c1"># Simple mean pooling</span>
            <span class="n">pool_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span>
            <span class="n">x_pooled</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">(</span>
                <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">pool_size</span><span class="p">,</span> 
                <span class="n">stride</span><span class="o">=</span><span class="n">pool_size</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">x_pooled</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Standard projections</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_len, d_model]</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_len, d_model]</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># [batch_size, seq_len, d_model]</span>

        <span class="c1"># Apply low-rank projections to K and V</span>
        <span class="n">K_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_projection</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="s1">&#39;E&#39;</span><span class="p">)</span>  <span class="c1"># [batch_size, k, d_model]</span>
        <span class="n">V_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_projection</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="s1">&#39;F&#39;</span><span class="p">)</span>  <span class="c1"># [batch_size, k, d_model]</span>

        <span class="c1"># Reshape for multi-head attention</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K_proj</span> <span class="o">=</span> <span class="n">K_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V_proj</span> <span class="o">=</span> <span class="n">V_proj</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_proj</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="c1"># scores: [batch_size, n_heads, seq_len, k]</span>

        <span class="c1"># Apply mask if provided (need to project mask as well)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># Project mask to match K_proj dimensions</span>
            <span class="n">mask_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_projection</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="s1">&#39;E&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">mask_proj</span> <span class="o">=</span> <span class="n">mask_proj</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask_proj</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">V_proj</span><span class="p">)</span>
        <span class="c1"># output: [batch_size, n_heads, seq_len, d_head]</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># Theoretical analysis of approximation quality</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LinformerAnalysis</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">attention_rank_analysis</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Analyze the rank structure of attention matrices&quot;&quot;&quot;</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">attention_matrix</span><span class="p">)</span>

        <span class="c1"># Compute cumulative explained variance</span>
        <span class="n">total_variance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">S</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">cumulative_variance</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">S</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">total_variance</span>

        <span class="c1"># Find rank for 90% variance explained</span>
        <span class="n">rank_90</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">((</span><span class="n">cumulative_variance</span> <span class="o">&gt;=</span> <span class="mf">0.9</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;singular_values&#39;</span><span class="p">:</span> <span class="n">S</span><span class="p">,</span>
            <span class="s1">&#39;rank_90_percent&#39;</span><span class="p">:</span> <span class="n">rank_90</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s1">&#39;effective_rank&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">S</span> <span class="o">&gt;</span> <span class="mf">0.01</span> <span class="o">*</span> <span class="n">S</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">approximation_error</span><span class="p">(</span><span class="n">original_attn</span><span class="p">,</span> <span class="n">linformer_attn</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute approximation error metrics&quot;&quot;&quot;</span>
        <span class="n">frobenius_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">original_attn</span> <span class="o">-</span> <span class="n">linformer_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
        <span class="n">spectral_error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">original_attn</span> <span class="o">-</span> <span class="n">linformer_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;frobenius_error&#39;</span><span class="p">:</span> <span class="n">frobenius_error</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s1">&#39;spectral_error&#39;</span><span class="p">:</span> <span class="n">spectral_error</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
            <span class="s1">&#39;relative_error&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">frobenius_error</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">original_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="p">}</span>
</code></pre></div>
<p><strong>Empirical Results:</strong></p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Standard Transformer</th>
<th>Linformer (k=256)</th>
<th>Speedup</th>
<th>Memory Reduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>WikiText-103</td>
<td>24.0 PPL</td>
<td>24.2 PPL</td>
<td>2.3×</td>
<td>3.1×</td>
</tr>
<tr>
<td>IMDB</td>
<td>91.2% Acc</td>
<td>90.8% Acc</td>
<td>1.8×</td>
<td>2.7×</td>
</tr>
<tr>
<td>Long Range Arena</td>
<td>53.2% Avg</td>
<td>51.8% Avg</td>
<td>4.2×</td>
<td>5.1×</td>
</tr>
</tbody>
</table>
<p><strong>Limitations:</strong></p>
<ol>
<li><strong>Fixed Sequence Length</strong>: Projection matrices are tied to training sequence length</li>
<li><strong>Information Loss</strong>: Low-rank approximation may lose important attention patterns</li>
<li><strong>Task Dependence</strong>: Optimal <span class="arithmatex">\(<span class="arithmatex">\(k\)</span>\)</span> varies significantly across tasks</li>
</ol>
<p><strong>Popularity:</strong> Medium; influential in research but limited production use.</p>
<p><strong>Models/Frameworks:</strong> Research models, some efficient attention implementations.</p>
<h3 id="performer">Performer</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2009.14794">Rethinking Attention with Performers</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/google-research/google-research/tree/master/performer">google-research/performer</a>
- 📊 <strong>Theory</strong>: <a href="https://papers.nips.cc/paper/2007/hash/013a006f03dbc5392effeb8f18fda755-Abstract.html">Random Features for Large-Scale Kernel Machines</a></p>
<p><strong>Motivation:</strong> Approximate standard attention using kernel methods to achieve linear complexity while maintaining theoretical guarantees.</p>
<p><strong>Core Innovation:</strong> FAVOR+ (Fast Attention Via positive Orthogonal Random features) algorithm that uses random feature approximations of the softmax kernel.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Kernel Perspective of Attention:</strong>
Standard attention can be viewed as:
<span class="arithmatex">\(<span class="arithmatex">\(\text{Attention}(Q, K, V) = D^{-1}AV\)</span>\)</span></p>
<p>where:
- <span class="arithmatex">\(<span class="arithmatex">\(A_{ij} = \exp(q_i^T k_j / \sqrt{d})\)</span>\)</span> (unnormalized attention)
- <span class="arithmatex">\(<span class="arithmatex">\(D = \text{diag}(A \mathbf{1})\)</span>\)</span> (normalization)</p>
<p><strong>Random Feature Approximation:</strong>
The exponential kernel <span class="arithmatex">\(<span class="arithmatex">\(\exp(x^T y)\)</span>\)</span> can be approximated using random features:</p>
<div class="arithmatex">\[\exp(x^T y) \approx \phi(x)^T \phi(y)\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(\phi: \mathbb{R}^d \rightarrow \mathbb{R}^m\)</span>\)</span> is a random feature map.</p>
<p><strong>FAVOR+ Feature Map:</strong>
For the softmax kernel <span class="arithmatex">\(<span class="arithmatex">\(\exp(q^T k / \sqrt{d})\)</span>\)</span>, FAVOR+ uses:</p>
<div class="arithmatex">\[\phi(x) = \frac{h(x)}{\sqrt{m}} \exp\left(\frac{\|x\|^2}{2\sqrt{d}}\right)\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(h(x) = [\exp(w_1^T x), \exp(w_2^T x), \ldots, \exp(w_m^T x)]\)</span>\)</span> and <span class="arithmatex">\(<span class="arithmatex">\(w_i\)</span>\)</span> are random vectors.</p>
<p><strong>Orthogonal Random Features:</strong>
To reduce variance, FAVOR+ uses structured orthogonal random matrices:</p>
<div class="arithmatex">\[W = \frac{1}{\sqrt{d}} \begin{bmatrix} G_1 H_1 D_1 \\ G_2 H_2 D_2 \\ \vdots \\ G_{m/d} H_{m/d} D_{m/d} \end{bmatrix}\]</div>
<p>where:
- <span class="arithmatex">\(<span class="arithmatex">\(G_i\)</span>\)</span>: Random orthogonal matrices
- <span class="arithmatex">\(<span class="arithmatex">\(H_i\)</span>\)</span>: Hadamard matrices
- <span class="arithmatex">\(<span class="arithmatex">\(D_i\)</span>\)</span>: Random diagonal matrices with <span class="arithmatex">\(<span class="arithmatex">\(\pm 1\)</span>\)</span> entries</p>
<p><strong>Linear Attention Computation:</strong>
With feature maps <span class="arithmatex">\(<span class="arithmatex">\(\phi(Q), \phi(K)\)</span>\)</span>, attention becomes:</p>
<div class="arithmatex">\[\text{Output} = \phi(Q) \left(\phi(K)^T V\right)\]</div>
<p>This can be computed in <span class="arithmatex">\(<span class="arithmatex">\(O(nmd)\)</span>\)</span> time instead of <span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span>.</p>
<p><strong>Advanced Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.stats</span><span class="w"> </span><span class="kn">import</span> <span class="n">ortho_group</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PerformerAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> 
                 <span class="n">feature_type</span><span class="o">=</span><span class="s1">&#39;orthogonal&#39;</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_type</span> <span class="o">=</span> <span class="n">feature_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">causal</span> <span class="o">=</span> <span class="n">causal</span>

        <span class="c1"># Standard projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Initialize random features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;projection_matrix&#39;</span><span class="p">,</span> 
                           <span class="bp">self</span><span class="o">.</span><span class="n">create_projection_matrix</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_projection_matrix</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create structured random projection matrix&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_type</span> <span class="o">==</span> <span class="s1">&#39;orthogonal&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_orthogonal_features</span><span class="p">()</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_type</span> <span class="o">==</span> <span class="s1">&#39;gaussian&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown feature type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_orthogonal_features</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create orthogonal random features for reduced variance&quot;&quot;&quot;</span>
        <span class="c1"># Number of orthogonal blocks needed</span>
        <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="n">blocks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
            <span class="c1"># Create random orthogonal matrix</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">ortho_group</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">),</span> 
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
            <span class="p">)</span>

            <span class="c1"># Apply random signs</span>
            <span class="n">signs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="n">block</span> <span class="o">=</span> <span class="n">block</span> <span class="o">*</span> <span class="n">signs</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="n">blocks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block</span><span class="p">)</span>

        <span class="c1"># Concatenate and truncate to desired size</span>
        <span class="n">full_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">blocks</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">full_matrix</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">]</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_feature_map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply FAVOR+ feature map&quot;&quot;&quot;</span>
        <span class="c1"># x: [batch_size, n_heads, seq_len, d_head]</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project using random features</span>
        <span class="c1"># [batch_size, n_heads, seq_len, d_head] @ [d_head, n_features]</span>
        <span class="n">projected</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_matrix</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c1"># Apply exponential and normalization</span>
        <span class="c1"># Compute ||x||^2 for each vector</span>
        <span class="n">x_norm_sq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># FAVOR+ feature map: exp(wx) * exp(||x||^2 / 2)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">projected</span> <span class="o">-</span> <span class="n">x_norm_sq</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Normalize by sqrt(m)</span>
        <span class="n">features</span> <span class="o">=</span> <span class="n">features</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">features</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">linear_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute linear attention using random features&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">causal_linear_attention</span><span class="p">(</span><span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">non_causal_linear_attention</span><span class="p">(</span><span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">non_causal_linear_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Non-causal linear attention&quot;&quot;&quot;</span>
        <span class="c1"># q_features, k_features: [batch_size, n_heads, seq_len, n_features]</span>
        <span class="c1"># v: [batch_size, n_heads, seq_len, d_head]</span>

        <span class="c1"># Compute K^T V: [batch_size, n_heads, n_features, d_head]</span>
        <span class="n">kv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">k_features</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Compute Q (K^T V): [batch_size, n_heads, seq_len, d_head]</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_features</span><span class="p">,</span> <span class="n">kv</span><span class="p">)</span>

        <span class="c1"># Compute normalization: Q K^T 1</span>
        <span class="n">k_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">k_features</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># [batch_size, n_heads, 1, n_features]</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_features</span><span class="p">,</span> <span class="n">k_sum</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># [batch_size, n_heads, seq_len, 1]</span>

        <span class="c1"># Avoid division by zero</span>
        <span class="n">normalizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">normalizer</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">qkv</span> <span class="o">/</span> <span class="n">normalizer</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">causal_linear_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Causal linear attention using cumulative sums&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">q_features</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">d_head</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Initialize running sums</span>
        <span class="n">kv_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">d_head</span><span class="p">,</span> 
            <span class="n">device</span><span class="o">=</span><span class="n">q_features</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q_features</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>
        <span class="n">k_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> 
            <span class="n">device</span><span class="o">=</span><span class="n">q_features</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q_features</span><span class="o">.</span><span class="n">dtype</span>
        <span class="p">)</span>

        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># Current query and key features</span>
            <span class="n">q_i</span> <span class="o">=</span> <span class="n">q_features</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># [batch_size, n_heads, 1, n_features]</span>
            <span class="n">k_i</span> <span class="o">=</span> <span class="n">k_features</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># [batch_size, n_heads, 1, n_features]</span>
            <span class="n">v_i</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># [batch_size, n_heads, 1, d_head]</span>

            <span class="c1"># Update running sums</span>
            <span class="n">kv_state</span> <span class="o">=</span> <span class="n">kv_state</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">k_i</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">v_i</span><span class="p">)</span>
            <span class="n">k_state</span> <span class="o">=</span> <span class="n">k_state</span> <span class="o">+</span> <span class="n">k_i</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Compute output for current position</span>
            <span class="n">output_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_i</span><span class="p">,</span> <span class="n">kv_state</span><span class="p">)</span>
            <span class="n">normalizer_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_i</span><span class="p">,</span> <span class="n">k_state</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">normalizer_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">normalizer_i</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

            <span class="n">output_i</span> <span class="o">=</span> <span class="n">output_i</span> <span class="o">/</span> <span class="n">normalizer_i</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output_i</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply feature maps</span>
        <span class="n">Q_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_feature_map</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
        <span class="n">K_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_feature_map</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>

        <span class="c1"># Compute linear attention</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear_attention</span><span class="p">(</span><span class="n">Q_features</span><span class="p">,</span> <span class="n">K_features</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="c1"># Theoretical analysis tools</span>
<span class="k">class</span><span class="w"> </span><span class="nc">PerformerAnalysis</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">approximation_quality</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">n_features_list</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Analyze approximation quality vs number of features&quot;&quot;&quot;</span>
        <span class="c1"># Compute exact attention</span>
        <span class="n">exact_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

        <span class="n">results</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">n_features</span> <span class="ow">in</span> <span class="n">n_features_list</span><span class="p">:</span>
            <span class="c1"># Create random features</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

            <span class="c1"># Apply feature map</span>
            <span class="n">q_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">q</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">k_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">k</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Approximate attention</span>
            <span class="n">approx_attn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_features</span><span class="p">,</span> <span class="n">k_features</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="c1"># Compute error</span>
            <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">exact_attn</span> <span class="o">-</span> <span class="n">approx_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">exact_attn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s1">&#39;fro&#39;</span><span class="p">)</span>
            <span class="n">results</span><span class="p">[</span><span class="n">n_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">error</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">results</span>
</code></pre></div>
<p><strong>Theoretical Guarantees:</strong></p>
<p>Performer provides unbiased estimation with bounded variance:</p>
<div class="arithmatex">\[\mathbb{E}[\phi(q)^T \phi(k)] = \exp(q^T k)\]</div>
<div class="arithmatex">\[\text{Var}[\phi(q)^T \phi(k)] = O\left(\frac{\exp(\|q\|^2 + \|k\|^2)}{m}\right)\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(m\)</span>\)</span> is the number of random features.</p>
<p><strong>Performance Comparison:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Sequence Length</th>
<th>Memory (GB)</th>
<th>Time (s)</th>
<th>Perplexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard Transformer</td>
<td>1K</td>
<td>2.1</td>
<td>1.0</td>
<td>24.2</td>
</tr>
<tr>
<td>Standard Transformer</td>
<td>4K</td>
<td>8.4</td>
<td>4.2</td>
<td>23.8</td>
</tr>
<tr>
<td>Performer</td>
<td>1K</td>
<td>1.8</td>
<td>0.9</td>
<td>24.4</td>
</tr>
<tr>
<td>Performer</td>
<td>4K</td>
<td>2.3</td>
<td>1.1</td>
<td>24.1</td>
</tr>
<tr>
<td>Performer</td>
<td>16K</td>
<td>4.1</td>
<td>2.8</td>
<td>23.9</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Medium; influential in research and specialized applications.</p>
<p><strong>Models/Frameworks:</strong> Research models, some production systems requiring efficient long-sequence processing.</p>
<h3 id="fnet">FNet</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2105.03824">FNet: Mixing Tokens with Fourier Transforms</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/google-research/google-research/tree/master/f_net">google-research/f_net</a>
- 🤗 <strong>HuggingFace</strong>: <a href="https://huggingface.co/docs/transformers/model_doc/fnet">FNet Documentation</a></p>
<p><strong>Motivation:</strong> Dramatically simplify the Transformer architecture while maintaining reasonable performance by replacing attention with Fourier transforms.</p>
<p><strong>Core Innovation:</strong> Complete replacement of self-attention with 2D Fourier Transform operations.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Standard Self-Attention:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>\)</span></p>
<p><strong>FNet Mixing:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{FNet}(X) = \text{Re}(\text{FFT}(\text{Re}(\text{FFT}(X))))\)</span>\)</span></p>
<p>where FFT is applied along both sequence and hidden dimensions.</p>
<p><strong>Two-Dimensional Fourier Transform:</strong>
For input <span class="arithmatex">\(<span class="arithmatex">\(X \in \mathbb{R}^{n \times d}\)</span>\)</span>:</p>
<ol>
<li>
<p><strong>Sequence Mixing</strong>: Apply FFT along sequence dimension
   <span class="arithmatex">\(<span class="arithmatex">\(X_1 = \text{Re}(\text{FFT}_{\text{seq}}(X))\)</span>\)</span></p>
</li>
<li>
<p><strong>Hidden Mixing</strong>: Apply FFT along hidden dimension
   <span class="arithmatex">\(<span class="arithmatex">\(X_2 = \text{Re}(\text{FFT}_{\text{hidden}}(X_1))\)</span>\)</span></p>
</li>
</ol>
<p><strong>Complexity Analysis:</strong>
- <strong>Self-Attention</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span>
- <strong>FNet</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(nd \log n + nd \log d) = O(nd \log(nd))\)</span>\)</span></p>
<p><strong>Theoretical Properties:</strong></p>
<p><strong>Fourier Transform as Linear Operator:</strong>
The DFT can be written as matrix multiplication:
<span class="arithmatex">\(<span class="arithmatex">\(\text{DFT}(x) = F_n x\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(F_n\)</span>\)</span> is the DFT matrix with entries:
<span class="arithmatex">\(<span class="arithmatex">\([F_n]_{jk} = \frac{1}{\sqrt{n}} e^{-2\pi i jk/n}\)</span>\)</span></p>
<p><strong>Mixing Properties:</strong>
1. <strong>Global Receptive Field</strong>: Every output depends on every input
2. <strong>Translation Invariance</strong>: Circular shifts in input create predictable shifts in output
3. <strong>Frequency Domain Processing</strong>: Natural handling of periodic patterns</p>
<p><strong>Advanced Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FNetLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">use_complex</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_complex</span> <span class="o">=</span> <span class="n">use_complex</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Layer normalization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Feed-forward network</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">fourier_transform_2d</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply 2D Fourier transform mixing&quot;&quot;&quot;</span>
        <span class="c1"># x: [batch_size, seq_len, d_model]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_complex</span><span class="p">:</span>
            <span class="c1"># Use complex FFT for potentially better mixing</span>
            <span class="c1"># Convert to complex</span>
            <span class="n">x_complex</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">complex</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

            <span class="c1"># FFT along sequence dimension</span>
            <span class="n">x_fft_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x_complex</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># FFT along hidden dimension</span>
            <span class="n">x_fft_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x_fft_seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

            <span class="c1"># Take real part</span>
            <span class="k">return</span> <span class="n">x_fft_hidden</span><span class="o">.</span><span class="n">real</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Standard real FFT</span>
            <span class="c1"># FFT along sequence dimension (take real part)</span>
            <span class="n">x_fft_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>

            <span class="c1"># FFT along hidden dimension (take real part)</span>
            <span class="n">x_fft_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x_fft_seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>

            <span class="k">return</span> <span class="n">x_fft_hidden</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Fourier mixing with residual connection</span>
        <span class="n">fourier_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fourier_transform_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">fourier_output</span><span class="p">))</span>

        <span class="c1"># Feed-forward with residual connection</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">ffn_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FNetBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Complete FNet block with optional enhancements&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">use_learnable_fourier</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
                 <span class="n">fourier_type</span><span class="o">=</span><span class="s1">&#39;standard&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fourier_type</span> <span class="o">=</span> <span class="n">fourier_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_learnable_fourier</span> <span class="o">=</span> <span class="n">use_learnable_fourier</span>

        <span class="k">if</span> <span class="n">use_learnable_fourier</span><span class="p">:</span>
            <span class="c1"># Learnable Fourier-like mixing</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">seq_mixing</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">hidden_mixing</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_model</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Enhanced FFN</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">apply_mixing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply various types of mixing&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">fourier_type</span> <span class="o">==</span> <span class="s1">&#39;standard&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_fourier_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fourier_type</span> <span class="o">==</span> <span class="s1">&#39;learnable&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learnable_fourier_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">fourier_type</span> <span class="o">==</span> <span class="s1">&#39;hybrid&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hybrid_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unknown fourier_type: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">fourier_type</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">standard_fourier_mixing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Standard FNet Fourier mixing&quot;&quot;&quot;</span>
        <span class="c1"># Apply 2D FFT</span>
        <span class="n">x_fft_seq</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>
        <span class="n">x_fft_hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">x_fft_seq</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">real</span>
        <span class="k">return</span> <span class="n">x_fft_hidden</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learnable_fourier_mixing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Learnable Fourier-like mixing&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Mix along sequence dimension</span>
        <span class="n">x_seq_mixed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_mixing</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Mix along hidden dimension</span>
        <span class="n">x_hidden_mixed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x_seq_mixed</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_mixing</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x_hidden_mixed</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">hybrid_mixing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Hybrid of Fourier and learnable mixing&quot;&quot;&quot;</span>
        <span class="n">fourier_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_fourier_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">learnable_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">learnable_fourier_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Weighted combination</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span>  <span class="c1"># Weight for Fourier component</span>
        <span class="k">return</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">fourier_output</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">learnable_output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Mixing layer</span>
        <span class="n">mixed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply_mixing</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">mixed</span><span class="p">))</span>

        <span class="c1"># Feed-forward layer</span>
        <span class="n">ffn_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ffn_out</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FNetModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Complete FNet model&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> 
                 <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> 
                 <span class="n">fourier_type</span><span class="o">=</span><span class="s1">&#39;standard&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>

        <span class="c1"># Embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># FNet layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">FNetBlock</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="n">fourier_type</span><span class="o">=</span><span class="n">fourier_type</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># Output layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">=</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Create position indices</span>
         <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">input_ids</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

         <span class="c1"># Embeddings</span>
         <span class="n">token_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embedding</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
         <span class="n">pos_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embedding</span><span class="p">(</span><span class="n">position_ids</span><span class="p">)</span>
         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">token_emb</span> <span class="o">+</span> <span class="n">pos_emb</span><span class="p">)</span>

         <span class="c1"># Apply FNet layers</span>
         <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
             <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="c1"># Final normalization</span>
         <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">final_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

         <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p><strong>Performance Characteristics:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Standard Transformer</th>
<th>FNet</th>
</tr>
</thead>
<tbody>
<tr>
<td>Attention Complexity</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nd \log(nd))\)</span>\)</span></td>
</tr>
<tr>
<td>Training Speed</td>
<td>Baseline</td>
<td>7× faster</td>
</tr>
<tr>
<td>Memory Usage</td>
<td>Baseline</td>
<td>0.5×</td>
</tr>
<tr>
<td>GLUE Performance</td>
<td>100%</td>
<td>92-97%</td>
</tr>
<tr>
<td>Long Sequence Capability</td>
<td>Limited</td>
<td>Better</td>
</tr>
</tbody>
</table>
<p><strong>Key Benefits:</strong></p>
<ol>
<li><strong>Simplicity</strong>: Much simpler than attention mechanisms</li>
<li><strong>Speed</strong>: Significantly faster training and inference</li>
<li><strong>Memory Efficiency</strong>: Lower memory requirements</li>
<li><strong>Global Mixing</strong>: Every token interacts with every other token</li>
</ol>
<p><strong>Limitations:</strong></p>
<ol>
<li><strong>Performance Gap</strong>: Some performance loss compared to attention</li>
<li><strong>Task Dependence</strong>: Works better for some tasks than others</li>
<li><strong>Limited Expressiveness</strong>: Less flexible than learned attention patterns</li>
</ol>
<p><strong>Popularity:</strong> Low-medium; primarily of research interest.</p>
<p><strong>Models/Frameworks:</strong> Research models and specialized applications prioritizing efficiency over maximum performance.</p>
<h3 id="sparse-transformers">Sparse Transformers</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/1904.10509">Generating Long Sequences with Sparse Transformers</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/openai/sparse_attention">openai/sparse_attention</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2003.05997">Sparse Attention Patterns</a></p>
<p><strong>Motivation:</strong> Enable efficient processing of very long sequences by introducing structured sparsity in attention patterns.</p>
<p><strong>Core Innovation:</strong> Replace dense attention with sparse attention patterns where each token attends only to a subset of other tokens.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Standard Dense Attention:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\)</span>\)</span></p>
<p><strong>Sparse Attention:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(A = \text{softmax}\left(\frac{QK^T \odot M}{\sqrt{d}}\right)V\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(M\)</span>\)</span> is a binary mask determining which tokens can attend to which others, and <span class="arithmatex">\(<span class="arithmatex">\(\odot\)</span>\)</span> represents element-wise multiplication.</p>
<p><strong>Common Sparse Patterns:</strong></p>
<ol>
<li>
<p><strong>Strided Pattern</strong>: Each token attends to tokens at fixed intervals
   <span class="arithmatex">\(<span class="arithmatex">\(M_{ij} = \begin{cases}
   1 &amp; \text{if } (i - j) \bmod s = 0 \\
   0 &amp; \text{otherwise}
   \end{cases}\)</span>\)</span></p>
</li>
<li>
<p><strong>Fixed Pattern</strong>: Each token attends to a fixed set of positions
   <span class="arithmatex">\(<span class="arithmatex">\(M_{ij} = \begin{cases}
   1 &amp; \text{if } j \in \{i-w, i-w+1, \ldots, i\} \\
   0 &amp; \text{otherwise}
   \end{cases}\)</span>\)</span></p>
</li>
<li>
<p><strong>Random Pattern</strong>: Each token attends to a random subset of tokens</p>
</li>
</ol>
<p><strong>Factorized Sparse Attention:</strong></p>
<p>Sparse Transformers introduce factorized attention patterns that decompose the attention into multiple sparse matrices:</p>
<div class="arithmatex">\[\text{Attend}(X, S) = \{\text{Attention}(x_i, S_i) : i \in \{1, \ldots, n\}\}\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(S_i \subset \{1, \ldots, n\}\)</span>\)</span> defines which positions token <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span> attends to.</p>
<p><strong>Implementation Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SparseAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">pattern_type</span><span class="o">=</span><span class="s1">&#39;strided&#39;</span><span class="p">,</span> 
                 <span class="n">stride</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">random_ratio</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pattern_type</span> <span class="o">=</span> <span class="n">pattern_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_ratio</span> <span class="o">=</span> <span class="n">random_ratio</span>

        <span class="c1"># Standard projections</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_sparse_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create sparse attention mask based on pattern type&quot;&quot;&quot;</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">pattern_type</span> <span class="o">==</span> <span class="s1">&#39;strided&#39;</span><span class="p">:</span>
            <span class="c1"># Strided pattern: attend to every stride-th token</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">):</span>
                    <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pattern_type</span> <span class="o">==</span> <span class="s1">&#39;fixed&#39;</span><span class="p">:</span>
            <span class="c1"># Fixed local window pattern</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">)</span>
                <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pattern_type</span> <span class="o">==</span> <span class="s1">&#39;factorized&#39;</span><span class="p">:</span>
            <span class="c1"># Factorized pattern combining strided and fixed</span>
            <span class="c1"># Local attention</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
                <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># Strided attention</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">):</span>
                    <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">pattern_type</span> <span class="o">==</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span>
            <span class="c1"># Random sparse pattern</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="c1"># Always attend to self and previous tokens in window</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">)</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

                <span class="c1"># Random additional connections</span>
                <span class="n">num_random</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_ratio</span> <span class="o">*</span> <span class="n">seq_len</span><span class="p">)</span>
                <span class="n">random_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[:</span><span class="n">num_random</span><span class="p">]</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">random_indices</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sparse_attention_computation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute attention with sparse mask&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Compute attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply sparse mask</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">Q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">V</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Create sparse attention mask</span>
        <span class="n">sparse_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_sparse_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Combine with input mask if provided</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sparse_mask</span> <span class="o">=</span> <span class="n">sparse_mask</span> <span class="o">&amp;</span> <span class="n">mask</span>

        <span class="c1"># Compute sparse attention</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sparse_attention_computation</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">sparse_mask</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">FactorizedSparseAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Advanced factorized sparse attention with multiple patterns&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>

        <span class="c1"># Separate attention heads for different patterns</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_attn</span> <span class="o">=</span> <span class="n">SparseAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;fixed&#39;</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">strided_attn</span> <span class="o">=</span> <span class="n">SparseAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;strided&#39;</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">block_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Apply different attention patterns</span>
        <span class="n">local_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">strided_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">strided_attn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Combine outputs</span>
        <span class="n">combined_output</span> <span class="o">=</span> <span class="p">(</span><span class="n">local_output</span> <span class="o">+</span> <span class="n">strided_output</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">combined_output</span><span class="p">)</span>
</code></pre></div>
<p><strong>Complexity Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Pattern Type</th>
<th>Complexity</th>
<th>Memory</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dense</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span></td>
<td>Standard attention</td>
</tr>
<tr>
<td>Strided</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot s \cdot d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot s)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(s = n/\text{stride}\)</span>\)</span></td>
</tr>
<tr>
<td>Fixed Window</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot w \cdot d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot w)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(w = \text{window size}\)</span>\)</span></td>
</tr>
<tr>
<td>Factorized</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot \sqrt{n} \cdot d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \cdot \sqrt{n})\)</span>\)</span></td>
<td>Combination of patterns</td>
</tr>
</tbody>
</table>
<p><strong>Performance Trade-offs:</strong></p>
<table>
<thead>
<tr>
<th>Sequence Length</th>
<th>Dense Attention</th>
<th>Sparse Attention</th>
<th>Speedup</th>
<th>Quality Loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>1K</td>
<td>1.0×</td>
<td>1.2×</td>
<td>1.2×</td>
<td>&lt;1%</td>
</tr>
<tr>
<td>4K</td>
<td>1.0×</td>
<td>3.1×</td>
<td>3.1×</td>
<td>2-3%</td>
</tr>
<tr>
<td>16K</td>
<td>1.0×</td>
<td>8.7×</td>
<td>8.7×</td>
<td>3-5%</td>
</tr>
<tr>
<td>64K</td>
<td>OOM</td>
<td>1.0×</td>
<td>∞</td>
<td>5-8%</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Medium-high; concepts widely adopted in various forms.</p>
<p><strong>Models/Frameworks:</strong> Influenced Longformer, BigBird, and aspects of GPT-3 and beyond.</p>
<h2 id="attention-mechanism-optimizations">Attention Mechanism Optimizations</h2>
<h3 id="flashattention">FlashAttention</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>
- 📄 <strong>FlashAttention-2</strong>: <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism</a>
- 💻 <strong>Official Implementation</strong>: <a href="https://github.com/Dao-AILab/flash-attention">Dao-AILab/flash-attention</a>
- 💻 <strong>Triton Implementation</strong>: <a href="https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py">FlashAttention in Triton</a>
- 💻 <strong>PyTorch Integration</strong>: <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">torch.nn.functional.scaled_dot_product_attention</a>
- 📊 <strong>Benchmarks</strong>: <a href="https://github.com/Dao-AILab/flash-attention/tree/main/benchmarks">FlashAttention Performance Analysis</a></p>
<p><img alt="FlashAttention Memory Hierarchy" src="https://github.com/Dao-AILab/flash-attention/raw/main/assets/flashattn_banner.jpg" />
<em>Figure: FlashAttention's IO-aware algorithm design optimizing GPU memory hierarchy (SRAM vs HBM)</em></p>
<p><strong>Research Context and Motivation:</strong></p>
<p>FlashAttention addresses a fundamental bottleneck in Transformer scaling: the quadratic memory complexity of attention mechanisms. While previous work focused on approximating attention (Linformer, Performer), FlashAttention maintains exact computation while achieving superior efficiency through hardware-aware optimization.</p>
<p><strong>The Memory Wall Problem:</strong></p>
<p>Modern GPUs have a complex memory hierarchy:
- <strong>SRAM (On-chip)</strong>: ~20MB, 19TB/s bandwidth
- <strong>HBM (High Bandwidth Memory)</strong>: ~40GB, 1.5TB/s bandwidth<br />
- <strong>DRAM</strong>: ~1TB, 0.1TB/s bandwidth</p>
<p>Standard attention implementations are <strong>memory-bound</strong>, not compute-bound, spending most time moving data between memory levels rather than performing computations.</p>
<p><strong>Core Innovation: IO-Aware Algorithm</strong></p>
<p>FlashAttention reorganizes attention computation to minimize expensive HBM ↔ SRAM transfers:</p>
<ol>
<li><strong>Tiling Strategy</strong>: Divide Q, K, V into blocks that fit in SRAM</li>
<li><strong>Online Softmax</strong>: Compute softmax incrementally without materializing full attention matrix</li>
<li><strong>Recomputation</strong>: Trade computation for memory by recomputing attention during backward pass</li>
</ol>
<p><img alt="FlashAttention Algorithm" src="https://production-media.paperswithcode.com/methods/Screen_Shot_2022-05-30_at_4.47.36_PM_Bd8VXsG.png" />
<em>Figure: FlashAttention's block-wise computation strategy avoiding quadratic memory usage</em></p>
<p><strong>Mathematical Foundation:</strong></p>
<p>The key insight is <strong>online softmax computation</strong>. Instead of computing:
<span class="arithmatex">\(<span class="arithmatex">\(\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V\)</span>\)</span></p>
<p>FlashAttention computes attention incrementally using the <strong>safe softmax</strong> recurrence:</p>
<div class="arithmatex">\[m^{(j)} = \max(m^{(j-1)}, \text{rowmax}(S^{(j)}))$$
$$\ell^{(j)} = e^{m^{(j-1)} - m^{(j)}} \ell^{(j-1)} + \text{rowsum}(e^{S^{(j)} - m^{(j)}})$$
$$O^{(j)} = \text{diag}(\ell^{(j)})^{-1} \left(\text{diag}(\ell^{(j-1)}) e^{m^{(j-1)} - m^{(j)}} O^{(j-1)} + e^{S^{(j)} - m^{(j)}} V^{(j)}\right)\]</div>
<p>where <span class="arithmatex">\(j\)</span> indexes blocks of K and V, enabling <strong>exact attention</strong> computation in <span class="arithmatex">\(O(N)\)</span> memory.</p>
<p><strong>FlashAttention-2 Improvements:</strong></p>
<p>The second iteration introduces several key optimizations:</p>
<ol>
<li><strong>Better Work Partitioning</strong>: Reduces non-matmul FLOPs by 2× through improved parallelization</li>
<li><strong>Sequence Length Parallelism</strong>: Distributes computation across sequence dimension</li>
<li><strong>Optimized Attention Masking</strong>: More efficient handling of causal and padding masks</li>
<li><strong>Reduced Communication</strong>: Minimizes synchronization overhead in multi-GPU settings</li>
</ol>
<p><strong>Research Impact and Applications:</strong></p>
<ul>
<li><strong>Long Context Models</strong>: Enables training on sequences up to 2M tokens (e.g., Longformer, BigBird successors)</li>
<li><strong>Multimodal Models</strong>: Critical for vision-language models processing high-resolution images</li>
<li><strong>Code Generation</strong>: Powers long-context code models like CodeT5+, StarCoder</li>
<li><strong>Scientific Computing</strong>: Enables protein folding models (AlphaFold variants) and molecular dynamics</li>
</ul>
<p><strong>Hardware Considerations:</strong></p>
<table>
<thead>
<tr>
<th>GPU Architecture</th>
<th>Memory Bandwidth</th>
<th>SRAM Size</th>
<th>FlashAttention Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>V100</td>
<td>900 GB/s</td>
<td>6MB</td>
<td>2.0-2.5×</td>
</tr>
<tr>
<td>A100</td>
<td>1.6 TB/s</td>
<td>20MB</td>
<td>2.5-3.5×</td>
</tr>
<tr>
<td>H100</td>
<td>3.0 TB/s</td>
<td>50MB</td>
<td>4.0-6.0×</td>
</tr>
</tbody>
</table>
<p><strong>Implementation Variants:</strong></p>
<ul>
<li><strong><a href="https://github.com/facebookresearch/xformers">xFormers</a></strong>: Memory-efficient attention with FlashAttention backend</li>
<li><strong><a href="https://github.com/openai/triton/blob/main/python/tutorials/06-fused-attention.py">Triton FlashAttention</a></strong>: Educational implementation in Triton</li>
<li><strong><a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html">PyTorch SDPA</a></strong>: Native PyTorch integration with automatic backend selection</li>
<li><strong><a href="https://github.com/google/flax/tree/main/flax/linen">JAX FlashAttention</a></strong>: JAX/Flax implementation for TPU optimization</li>
</ul>
<p><strong>Key Implementation Insights:</strong></p>
<p><strong>Block Size Optimization:</strong>
Optimal block sizes depend on hardware characteristics:
- <strong>A100</strong>: Br=128, Bc=64 for balanced compute/memory
- <strong>H100</strong>: Br=256, Bc=128 for higher parallelism
- <strong>V100</strong>: Br=64, Bc=32 for memory constraints</p>
<p><strong>Critical Implementation Steps:</strong></p>
<ol>
<li><strong>Memory Layout Optimization</strong>: <a href="https://github.com/Dao-AILab/flash-attention/blob/main/csrc/flash_attn/flash_api.cpp">CUDA Kernel Implementation</a></li>
<li>Coalesced memory access patterns</li>
<li>Shared memory bank conflict avoidance</li>
<li>
<p>Warp-level primitives for reduction operations</p>
</li>
<li>
<p><strong>Numerical Stability</strong>: <a href="https://github.com/Dao-AILab/flash-attention/blob/main/csrc/flash_attn/src/flash_fwd_kernel.h">Safe Softmax Implementation</a></p>
</li>
<li>Online computation of max and sum statistics</li>
<li>Avoiding overflow in exponential operations</li>
<li>
<p>Maintaining precision across block boundaries</p>
</li>
<li>
<p><strong>Backward Pass Optimization</strong>: <a href="https://github.com/Dao-AILab/flash-attention/blob/main/csrc/flash_attn/src/flash_bwd_kernel.h">Gradient Computation</a></p>
</li>
<li>Recomputation strategy for memory efficiency</li>
<li>Fused gradient operations</li>
<li>Optimized attention mask handling</li>
</ol>
<p><strong>Simplified Usage Example:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Using PyTorch&#39;s native SDPA (automatically selects FlashAttention)</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="c1"># Automatic backend selection (FlashAttention, Memory-Efficient, Math)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">scaled_dot_product_attention</span><span class="p">(</span>
    <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> 
    <span class="n">attn_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> 
    <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># For autoregressive models</span>
<span class="p">)</span>

<span class="c1"># Direct FlashAttention usage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">flash_attn</span><span class="w"> </span><span class="kn">import</span> <span class="n">flash_attn_func</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">flash_attn_func</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</code></pre></div>
<p><strong>Advanced Research Directions:</strong></p>
<p><strong>1. FlashAttention Variants and Extensions:</strong>
- <strong><a href="https://arxiv.org/abs/2407.08608">FlashAttention-3</a></strong>: Asynchronous processing and improved load balancing
- <strong><a href="https://arxiv.org/abs/2309.06180">PagedAttention</a></strong>: Virtual memory management for attention computation
- <strong><a href="https://arxiv.org/abs/2310.01889">Ring Attention</a></strong>: Distributed attention across multiple devices
- <strong><a href="https://arxiv.org/abs/2311.01906">Striped Attention</a></strong>: Optimized for extremely long sequences</p>
<p><strong>2. Theoretical Analysis:</strong>
- <strong>IO Complexity</strong>: Proven optimal for the red-blue pebble game model
- <strong>Approximation Quality</strong>: Maintains exact computation unlike other efficiency methods
- <strong>Scaling Laws</strong>: Memory usage scales as O(N) vs O(N²) for standard attention</p>
<p><strong>3. Integration with Modern Architectures:</strong>
- <strong>Mixture of Experts</strong>: <a href="https://github.com/Dao-AILab/flash-attention/issues/123">FlashAttention + MoE</a> for sparse expert routing
- <strong>Multimodal Models</strong>: Critical for vision-language models processing high-resolution images
- <strong>Long Context</strong>: Enables 1M+ token context windows in models like Claude-3, GPT-4 Turbo</p>
<p><strong>4. Hardware Co-design:</strong>
- <strong>Custom ASIC</strong>: Specialized chips designed around FlashAttention principles
- <strong>Memory Hierarchy</strong>: Optimizations for emerging memory technologies (HBM3, CXL)
- <strong>Quantization</strong>: Integration with INT8/FP8 quantization schemes</p>
<p><strong>Performance Improvements:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Standard Attention</th>
<th>FlashAttention</th>
<th>FlashAttention-2</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory Usage</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(N^2)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(N)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(N)\)</span>\)</span></td>
</tr>
<tr>
<td>Speed (A100)</td>
<td>1.0×</td>
<td>2.4×</td>
<td>3.1×</td>
</tr>
<tr>
<td>Speed (H100)</td>
<td>1.0×</td>
<td>3.2×</td>
<td>4.8×</td>
</tr>
<tr>
<td>Sequence Length</td>
<td>Limited</td>
<td>8× longer</td>
<td>16× longer</td>
</tr>
</tbody>
</table>
<p><strong>Key Benefits:</strong></p>
<ol>
<li><strong>Memory Efficiency</strong>: Reduces memory from <span class="arithmatex">\(<span class="arithmatex">\(O(N^2)\)</span>\)</span> to <span class="arithmatex">\(<span class="arithmatex">\(O(N)\)</span>\)</span></li>
<li><strong>Speed</strong>: 2-5× faster due to better memory access patterns</li>
<li><strong>Exact Computation</strong>: Unlike approximation methods, computes exact attention</li>
<li><strong>Hardware Optimization</strong>: Designed for modern GPU architectures</li>
</ol>
<p><strong>Popularity:</strong> Very high; widely adopted in modern LLM implementations.</p>
<p><strong>Models/Frameworks:</strong> Llama 3, DeepSeek, Qwen-2, and most state-of-the-art LLM inference systems.</p>
<h3 id="multi-query-attention-mqa">Multi-Query Attention (MQA)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/1911.02150">Fast Transformer Decoding: One Write-Head is All You Need</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/huggingface/transformers">huggingface/transformers</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2305.13245">Multi-Query Attention Analysis</a></p>
<p><strong>Motivation:</strong> Reduce memory usage and computational cost during autoregressive inference.</p>
<p><strong>Problem:</strong> Standard multi-head attention requires storing separate key and value projections for each attention head, leading to large KV cache requirements.</p>
<p><strong>Solution:</strong> Use a single key and value head shared across all query heads, significantly reducing memory requirements.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Standard Multi-Head Attention (MHA):</strong>
<span class="arithmatex">\(<span class="arithmatex">\(Q_i = XW_i^Q, \quad K_i = XW_i^K, \quad V_i = XW_i^V\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(O_i = \text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right)V_i\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(i \in \{1, 2, \ldots, h\}\)</span>\)</span> represents the head index.</p>
<p><strong>Multi-Query Attention (MQA):</strong>
<span class="arithmatex">\(<span class="arithmatex">\(Q_i = XW_i^Q, \quad K = XW^K, \quad V = XW^V\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(O_i = \text{Attention}(Q_i, K, V) = \text{softmax}\left(\frac{Q_i K^T}{\sqrt{d_k}}\right)V\)</span>\)</span></p>
<p><strong>Memory Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>MHA</th>
<th>MQA</th>
<th>Reduction</th>
</tr>
</thead>
<tbody>
<tr>
<td>Query Projections</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times d \times d_k\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times d \times d_k\)</span>\)</span></td>
<td>1×</td>
</tr>
<tr>
<td>Key Projections</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times d \times d_k\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(1 \times d \times d_k\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span>×</td>
</tr>
<tr>
<td>Value Projections</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times d \times d_v\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(1 \times d \times d_v\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span>×</td>
</tr>
<tr>
<td>KV Cache</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times n \times (d_k + d_v)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(1 \times n \times (d_k + d_v)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span>×</td>
</tr>
</tbody>
</table>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="c1"># Multiple query heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Single key and value heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project queries (multiple heads)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch_size, n_heads, seq_len, d_head]</span>

        <span class="c1"># Project keys and values (single head each)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Handle past key-value cache for autoregressive generation</span>
        <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Expand k and v to match query heads</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply causal mask for autoregressive models</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
                <span class="n">diagonal</span><span class="o">=</span><span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Prepare cache for next iteration</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="c1"># Store only the single k, v heads</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">v</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MQATransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiQueryAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Pre-norm attention</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">present_kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">)</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span>

        <span class="c1"># Pre-norm FFN</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ffn_output</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">present_kv</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p><strong>Performance Benefits:</strong></p>
<table>
<thead>
<tr>
<th>Model Size</th>
<th>MHA KV Cache</th>
<th>MQA KV Cache</th>
<th>Memory Reduction</th>
<th>Inference Speedup</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B (32 heads)</td>
<td>4.2 GB</td>
<td>131 MB</td>
<td>32×</td>
<td>1.8×</td>
</tr>
<tr>
<td>13B (40 heads)</td>
<td>8.1 GB</td>
<td>203 MB</td>
<td>40×</td>
<td>2.1×</td>
</tr>
<tr>
<td>70B (64 heads)</td>
<td>32.4 GB</td>
<td>506 MB</td>
<td>64×</td>
<td>2.7×</td>
</tr>
</tbody>
</table>
<p><strong>Quality Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Task</th>
<th>MHA</th>
<th>MQA</th>
<th>Performance Drop</th>
</tr>
</thead>
<tbody>
<tr>
<td>Language Modeling</td>
<td>100%</td>
<td>97-99%</td>
<td>1-3%</td>
</tr>
<tr>
<td>Question Answering</td>
<td>100%</td>
<td>96-98%</td>
<td>2-4%</td>
</tr>
<tr>
<td>Code Generation</td>
<td>100%</td>
<td>95-97%</td>
<td>3-5%</td>
</tr>
<tr>
<td>Reasoning Tasks</td>
<td>100%</td>
<td>94-96%</td>
<td>4-6%</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> High; widely adopted in modern LLMs.</p>
<p><strong>Models/Frameworks:</strong> PaLM, Falcon, and many other recent models.</p>
<h3 id="grouped-query-attention-gqa">Grouped-Query Attention (GQA)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2305.13245">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/huggingface/transformers">huggingface/transformers</a>
- 📊 <strong>Comparison</strong>: <a href="https://arxiv.org/abs/2307.09288">MHA vs MQA vs GQA Analysis</a></p>
<p><strong>Motivation:</strong> Balance the efficiency benefits of MQA with the performance benefits of multi-head attention.</p>
<p><strong>Problem:</strong> MQA reduces memory usage but can impact model quality, while MHA provides better quality but higher memory usage.</p>
<p><strong>Solution:</strong> Group query heads to share key and value projections, providing a middle ground between MQA and MHA.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Grouped-Query Attention (GQA):</strong>
Divide <span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span> query heads into <span class="arithmatex">\(<span class="arithmatex">\(g\)</span>\)</span> groups, where each group shares a single key-value head:</p>
<div class="arithmatex">\[Q_i = XW_i^Q, \quad K_{G(i)} = XW_{G(i)}^K, \quad V_{G(i)} = XW_{G(i)}^V\]</div>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(G(i)\)</span>\)</span> maps query head <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span> to its group.</p>
<p><strong>Group Assignment:</strong>
For <span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span> heads and <span class="arithmatex">\(<span class="arithmatex">\(g\)</span>\)</span> groups:
<span class="arithmatex">\(<span class="arithmatex">\(G(i) = \lfloor i \cdot g / h \rfloor\)</span>\)</span></p>
<p><strong>Memory Comparison:</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Query Heads</th>
<th>KV Heads</th>
<th>KV Cache Size</th>
<th>Quality</th>
</tr>
</thead>
<tbody>
<tr>
<td>MHA</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h \times n \times d\)</span>\)</span></td>
<td>100%</td>
</tr>
<tr>
<td>GQA</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(g\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(g \times n \times d\)</span>\)</span></td>
<td>98-99%</td>
</tr>
<tr>
<td>MQA</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(1\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(1 \times n \times d\)</span>\)</span></td>
<td>95-97%</td>
</tr>
</tbody>
</table>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GroupedQueryAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_kv_groups</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_groups</span> <span class="o">=</span> <span class="n">n_kv_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">heads_per_group</span> <span class="o">=</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="n">n_kv_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

        <span class="k">assert</span> <span class="n">n_heads</span> <span class="o">%</span> <span class="n">n_kv_groups</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;n_heads must be divisible by n_kv_groups&quot;</span>

        <span class="c1"># Query projections (one per head)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Key and value projections (one per group)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_kv_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_kv_groups</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project queries</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch_size, n_heads, seq_len, d_head]</span>

        <span class="c1"># Project keys and values</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_kv_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Handle past key-value cache</span>
        <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch_size, n_kv_groups, seq_len_k, d_head]</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch_size, n_kv_groups, seq_len_k, d_head]</span>

        <span class="c1"># Expand keys and values to match query groups</span>
        <span class="n">k_expanded</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">heads_per_group</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">v_expanded</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">repeat_interleave</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">heads_per_group</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Compute attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_expanded</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply causal mask</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="ow">or</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">k_expanded</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
                <span class="n">diagonal</span><span class="o">=</span><span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax and dropout</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout_layer</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v_expanded</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Prepare cache for next iteration</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span><span class="w"> </span><span class="nc">GQATransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_kv_groups</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">GroupedQueryAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">n_kv_groups</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>  <span class="c1"># Using RMSNorm as in modern models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># SwiGLU FFN as used in modern models</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">SwiGLUFFN</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Pre-norm attention</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">present_kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span><span class="p">)</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span>

        <span class="c1"># Pre-norm FFN</span>
        <span class="n">ffn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">ffn_output</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">present_kv</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SwiGLUFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SwiGLU Feed-Forward Network as used in modern models&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Gate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Down projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Up projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># SwiGLU: Swish(W1(x)) * W3(x)</span>
        <span class="n">gate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Swish activation</span>
        <span class="n">up</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="n">gate</span> <span class="o">*</span> <span class="n">up</span>
        <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">w2</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
</code></pre></div>
<p><strong>Configuration Examples:</strong></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Total Heads</th>
<th>KV Groups</th>
<th>Heads per Group</th>
<th>Memory Reduction</th>
<th>Quality Retention</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama-7B</td>
<td>32</td>
<td>8</td>
<td>4</td>
<td>4×</td>
<td>99.2%</td>
</tr>
<tr>
<td>Llama-13B</td>
<td>40</td>
<td>8</td>
<td>5</td>
<td>5×</td>
<td>99.1%</td>
</tr>
<tr>
<td>Llama-70B</td>
<td>64</td>
<td>8</td>
<td>8</td>
<td>8×</td>
<td>98.9%</td>
</tr>
<tr>
<td>Custom</td>
<td>48</td>
<td>12</td>
<td>4</td>
<td>4×</td>
<td>99.3%</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Very high; rapidly adopted in recent models.</p>
<p><strong>Models/Frameworks:</strong> Llama 3, Gemma, Claude, and many other recent models.</p>
<h3 id="multi-level-attention-mla">Multi-Level Attention (MLA)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2405.04434">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/deepseek-ai/DeepSeek-V2">deepseek-ai/DeepSeek-V2</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2405.04434">Multi-Level Attention Analysis</a></p>
<p><strong>Motivation:</strong> Further reduce KV cache memory usage while maintaining model quality through hierarchical attention compression.</p>
<p><strong>Problem:</strong> Even GQA still requires significant memory for KV cache in very large models and long sequences.</p>
<p><strong>Solution:</strong> Introduce multiple levels of key-value compression with different granularities.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Multi-Level Key-Value Compression:</strong></p>
<p>MLA introduces a hierarchical compression scheme:</p>
<ol>
<li><strong>Level 1 (Fine-grained)</strong>: Local attention within windows</li>
<li><strong>Level 2 (Medium-grained)</strong>: Compressed representations for medium-range dependencies  </li>
<li><strong>Level 3 (Coarse-grained)</strong>: Highly compressed global context</li>
</ol>
<p><strong>Compression Functions:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(K_1 = \text{LocalCompress}(K), \quad V_1 = \text{LocalCompress}(V)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(K_2 = \text{MediumCompress}(K_1), \quad V_2 = \text{MediumCompress}(V_1)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(K_3 = \text{GlobalCompress}(K_2), \quad V_3 = \text{GlobalCompress}(V_2)\)</span>\)</span></p>
<p><strong>Attention Computation:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(O = \text{Attention}(Q, [K_1; K_2; K_3], [V_1; V_2; V_3])\)</span>\)</span></p>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MultiLevelAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">window_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">64</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">],</span> 
                 <span class="n">compression_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_sizes</span> <span class="o">=</span> <span class="n">window_sizes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compression_ratios</span> <span class="o">=</span> <span class="n">compression_ratios</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_levels</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">window_sizes</span><span class="p">)</span>

        <span class="c1"># Query projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Key and value projections for each level</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_projs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
            <span class="k">for</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="n">compression_ratios</span>
        <span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_projs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">ratio</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> 
            <span class="k">for</span> <span class="n">ratio</span> <span class="ow">in</span> <span class="n">compression_ratios</span>
        <span class="p">])</span>

        <span class="c1"># Compression layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compressors</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span><span class="n">d_model</span> <span class="o">//</span> <span class="n">compression_ratios</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> 
                     <span class="n">d_model</span> <span class="o">//</span> <span class="n">compression_ratios</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> 
                     <span class="n">kernel_size</span><span class="o">=</span><span class="n">compression_ratios</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> 
                     <span class="n">stride</span><span class="o">=</span><span class="n">compression_ratios</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_levels</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">compress_kv</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">level</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compress key-value pairs for a specific level&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">compression_ratios</span><span class="p">[</span><span class="n">level</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span>

        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Reshape for convolution</span>
        <span class="n">k_conv</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch, d_k, seq_len]</span>
        <span class="n">v_conv</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch, d_v, seq_len]</span>

        <span class="c1"># Apply compression</span>
        <span class="n">k_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compressors</span><span class="p">[</span><span class="n">level</span><span class="p">](</span><span class="n">k_conv</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compressors</span><span class="p">[</span><span class="n">level</span><span class="p">](</span><span class="n">v_conv</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">k_compressed</span><span class="p">,</span> <span class="n">v_compressed</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_level_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">level</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create attention mask for specific level&quot;&quot;&quot;</span>
        <span class="n">window_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_sizes</span><span class="p">[</span><span class="n">level</span><span class="p">]</span>
        <span class="n">compression_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compression_ratios</span><span class="p">[</span><span class="n">level</span><span class="p">]</span>

        <span class="c1"># Compressed sequence length</span>
        <span class="n">compressed_len</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">compression_ratio</span>

        <span class="k">if</span> <span class="n">level</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># Local attention</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
                <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># Global attention to compressed representations</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">compressed_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project queries</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># [batch_size, n_heads, seq_len, d_head]</span>

        <span class="c1"># Process each level</span>
        <span class="n">all_k</span><span class="p">,</span> <span class="n">all_v</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">level</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_levels</span><span class="p">):</span>
            <span class="c1"># Project keys and values for this level</span>
            <span class="n">k_level</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_projs</span><span class="p">[</span><span class="n">level</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
            <span class="n">v_level</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_projs</span><span class="p">[</span><span class="n">level</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>

            <span class="c1"># Compress if needed</span>
            <span class="n">k_compressed</span><span class="p">,</span> <span class="n">v_compressed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compress_kv</span><span class="p">(</span><span class="n">k_level</span><span class="p">,</span> <span class="n">v_level</span><span class="p">,</span> <span class="n">level</span><span class="p">)</span>

            <span class="c1"># Handle past cache</span>
            <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">level</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">past_kv</span><span class="p">):</span>
                <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span><span class="p">[</span><span class="n">level</span><span class="p">]</span>
                <span class="n">k_compressed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k_compressed</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">v_compressed</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v_compressed</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">all_k</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">k_compressed</span><span class="p">)</span>
            <span class="n">all_v</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">v_compressed</span><span class="p">)</span>

        <span class="c1"># Concatenate all levels</span>
        <span class="n">k_concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">v_concat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">all_v</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Reshape for attention</span>
        <span class="n">k_concat</span> <span class="o">=</span> <span class="n">k_concat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v_concat</span> <span class="o">=</span> <span class="n">v_concat</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k_concat</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply attention</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v_concat</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="c1"># Prepare cache</span>
        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">[(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">all_k</span><span class="p">,</span> <span class="n">all_v</span><span class="p">)]</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p><strong>Memory Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Level</th>
<th>Window Size</th>
<th>Compression</th>
<th>Memory Usage</th>
<th>Coverage</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 (Local)</td>
<td>64</td>
<td>1×</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(w \cdot d)\)</span>\)</span></td>
<td>Local patterns</td>
</tr>
<tr>
<td>2 (Medium)</td>
<td>256</td>
<td>4×</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n/4 \cdot d/4)\)</span>\)</span></td>
<td>Medium-range</td>
</tr>
<tr>
<td>3 (Global)</td>
<td>1024</td>
<td>16×</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n/16 \cdot d/16)\)</span>\)</span></td>
<td>Global context</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td>-</td>
<td>-</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(w \cdot d + n \cdot d/16)\)</span>\)</span></td>
<td>Full coverage</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Medium; primarily used in DeepSeek models.</p>
<p><strong>Models/Frameworks:</strong> DeepSeek-V2, specialized efficient architectures.</p>
<h3 id="sliding-window-attention">Sliding Window Attention</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2004.05150">Longformer: The Long-Document Transformer</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/allenai/longformer">allenai/longformer</a>
- 📊 <strong>Mistral Implementation</strong>: <a href="https://arxiv.org/abs/2310.06825">Mistral 7B</a></p>
<p><strong>Motivation:</strong> Enable efficient processing of long sequences by limiting attention to local windows while maintaining global connectivity.</p>
<p><strong>Problem:</strong> Full attention scales quadratically with sequence length, making long sequences computationally prohibitive.</p>
<p><strong>Solution:</strong> Each token attends only to tokens within a fixed-size sliding window, reducing complexity to linear.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Sliding Window Attention:</strong>
For a window size <span class="arithmatex">\(<span class="arithmatex">\(w\)</span>\)</span>, token at position <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span> attends to positions <span class="arithmatex">\(<span class="arithmatex">\([i-w/2, i+w/2]\)</span>\)</span>:</p>
<div class="arithmatex">\[\text{SWA}(Q, K, V)_i = \text{Attention}(Q_i, K_{i-w/2:i+w/2}, V_{i-w/2:i+w/2})\]</div>
<p><strong>Attention Mask:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(M_{ij} = \begin{cases}
1 &amp; \text{if } |i - j| \leq w/2 \\
0 &amp; \text{otherwise}
\end{cases}\)</span>\)</span></p>
<p><strong>Global Attention (Optional):</strong>
Some tokens (e.g., [CLS], special tokens) can attend globally:
<span class="arithmatex">\(<span class="arithmatex">\(\text{GlobalSWA}(Q, K, V)_i = \begin{cases}
\text{Attention}(Q_i, K, V) &amp; \text{if } i \in \text{global\_tokens} \\
\text{SWA}(Q, K, V)_i &amp; \text{otherwise}
\end{cases}\)</span>\)</span></p>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SlidingWindowAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
                 <span class="n">global_attention_indices</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">global_attention_indices</span> <span class="o">=</span> <span class="n">global_attention_indices</span> <span class="ow">or</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_sliding_window_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create sliding window attention mask&quot;&quot;&quot;</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="c1"># Local window</span>
            <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="c1"># Global attention for special tokens</span>
            <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">global_attention_indices</span><span class="p">:</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># This token attends globally</span>
                <span class="n">mask</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># All tokens attend to this token</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">efficient_sliding_window_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Efficient implementation using sparse operations&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># For very long sequences, we can implement block-wise computation</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="mi">4096</span><span class="p">:</span>  <span class="c1"># Use block-wise computation for very long sequences</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">block_wise_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

        <span class="c1"># Standard computation for shorter sequences</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">block_wise_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Block-wise computation for very long sequences&quot;&quot;&quot;</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_head</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">block_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">start</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
            <span class="n">end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">start</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

            <span class="c1"># Extract blocks</span>
            <span class="n">q_block</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="p">:]</span>

            <span class="c1"># Determine attention range for this block</span>
            <span class="n">attn_start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">start</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">attn_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">end</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>

            <span class="n">k_block</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">attn_start</span><span class="p">:</span><span class="n">attn_end</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">v_block</span> <span class="o">=</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">attn_start</span><span class="p">:</span><span class="n">attn_end</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">mask_block</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="n">attn_start</span><span class="p">:</span><span class="n">attn_end</span><span class="p">]</span>

            <span class="c1"># Compute attention for this block</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_block</span><span class="p">,</span> <span class="n">k_block</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_head</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask_block</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

            <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

            <span class="n">block_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v_block</span><span class="p">)</span>
            <span class="n">output</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">block_output</span>

        <span class="k">return</span> <span class="n">output</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Create sliding window mask</span>
        <span class="n">sliding_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">create_sliding_window_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Combine with input attention mask if provided</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sliding_mask</span> <span class="o">=</span> <span class="n">sliding_mask</span> <span class="o">&amp;</span> <span class="n">attention_mask</span>

        <span class="c1"># Compute attention</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">efficient_sliding_window_attention</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">sliding_mask</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MistralSlidingWindowAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mistral-style sliding window attention with optimizations&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="c1"># Rotary position embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">RotaryEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply rotary position embedding</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>

        <span class="c1"># Handle past key-value cache</span>
        <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Sliding window attention</span>
        <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">seq_len_k</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">:</span>
            <span class="c1"># Full attention for short sequences</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Sliding window for long sequences</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> 
                               <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
                <span class="n">start</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">i</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">)</span>
                <span class="n">end</span> <span class="o">=</span> <span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span>

                <span class="n">q_i</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
                <span class="n">k_window</span> <span class="o">=</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">,</span> <span class="p">:]</span>

                <span class="n">scores_i</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q_i</span><span class="p">,</span> <span class="n">k_window</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>
                <span class="n">scores</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i</span><span class="p">,</span> <span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">=</span> <span class="n">scores_i</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Apply causal mask</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">q</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
            <span class="n">diagonal</span><span class="o">=</span><span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p><strong>Complexity Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Attention Type</th>
<th>Time Complexity</th>
<th>Space Complexity</th>
<th>Max Sequence Length</th>
</tr>
</thead>
<tbody>
<tr>
<td>Full Attention</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span></td>
<td>~2K (limited by memory)</td>
</tr>
<tr>
<td>Sliding Window</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nwd)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nw)\)</span>\)</span></td>
<td>~32K+ (limited by compute)</td>
</tr>
<tr>
<td>Block-wise SW</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nwd)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(w^2)\)</span>\)</span></td>
<td>~128K+ (very efficient)</td>
</tr>
</tbody>
</table>
<p><strong>Performance Characteristics:</strong></p>
<table>
<thead>
<tr>
<th>Window Size</th>
<th>Memory Usage</th>
<th>Quality (vs Full)</th>
<th>Speed (vs Full)</th>
</tr>
</thead>
<tbody>
<tr>
<td>256</td>
<td>0.1×</td>
<td>94-96%</td>
<td>8×</td>
</tr>
<tr>
<td>512</td>
<td>0.2×</td>
<td>96-98%</td>
<td>6×</td>
</tr>
<tr>
<td>1024</td>
<td>0.4×</td>
<td>98-99%</td>
<td>4×</td>
</tr>
<tr>
<td>2048</td>
<td>0.8×</td>
<td>99-99.5%</td>
<td>2×</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> High; widely adopted for long-context models.</p>
<p><strong>Models/Frameworks:</strong> Longformer, BigBird, Mistral, and many long-context models.</p>
<h2 id="positional-encoding-innovations">Positional Encoding Innovations</h2>
<h3 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/huggingface/transformers">huggingface/transformers</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2104.09864">Understanding RoPE</a></p>
<p><strong>Motivation:</strong> Provide better relative position encoding that naturally handles variable sequence lengths and maintains rotational invariance.</p>
<p><strong>Problem:</strong> Absolute positional encodings don't capture relative relationships well, and learned position embeddings don't generalize to longer sequences.</p>
<p><strong>Solution:</strong> Apply rotary transformations to query and key vectors that encode relative positions through rotation angles.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Rotary Transformation:</strong>
For a 2D vector <span class="arithmatex">\(<span class="arithmatex">\((x_1, x_2)\)</span>\)</span>, rotation by angle <span class="arithmatex">\(<span class="arithmatex">\(\theta\)</span>\)</span>:
<span class="arithmatex">\(<span class="arithmatex">\(\begin{pmatrix} x_1' \\ x_2' \end{pmatrix} = \begin{pmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}\)</span>\)</span></p>
<p><strong>RoPE for Position <span class="arithmatex">(<span class="arithmatex">\(m\)</span>\)</span>:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(f(\mathbf{q}, m) = \mathbf{R}_\Theta^d(m) \mathbf{q}\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(f(\mathbf{k}, n) = \mathbf{R}_\Theta^d(n) \mathbf{k}\)</span>\)</span></p>
<p>where <span class="arithmatex">\(<span class="arithmatex">\(\mathbf{R}_\Theta^d(m)\)</span>\)</span> is the rotation matrix for position <span class="arithmatex">\(<span class="arithmatex">\(m\)</span>\)</span>:</p>
<div class="arithmatex">\[\mathbf{R}_\Theta^d(m) = \begin{pmatrix}
\cos(m\theta_1) &amp; -\sin(m\theta_1) &amp; 0 &amp; 0 &amp; \cdots \\
\sin(m\theta_1) &amp; \cos(m\theta_1) &amp; 0 &amp; 0 &amp; \cdots \\
0 &amp; 0 &amp; \cos(m\theta_2) &amp; -\sin(m\theta_2) &amp; \cdots \\
0 &amp; 0 &amp; \sin(m\theta_2) &amp; \cos(m\theta_2) &amp; \cdots \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{pmatrix}\]</div>
<p><strong>Frequency Calculation:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\theta_i = 10000^{-2i/d}, \quad i = 0, 1, \ldots, d/2-1\)</span>\)</span></p>
<p><strong>Relative Position Property:</strong>
The inner product after RoPE naturally encodes relative position:
<span class="arithmatex">\(<span class="arithmatex">\(\langle f(\mathbf{q}, m), f(\mathbf{k}, n) \rangle = \text{Re}[\langle \mathbf{q}, \mathbf{k} \rangle e^{i(m-n)\theta}]\)</span>\)</span></p>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RotaryEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">base</span>

        <span class="c1"># Compute frequency for each dimension pair</span>
        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">base</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;inv_freq&quot;</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Build here to make `torch.jit.trace` work.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_cos_sin_cache</span><span class="p">(</span>
            <span class="n">seq_len</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_cos_sin_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,j-&gt;ij&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">)</span>
        <span class="c1"># Different from paper, but it uses a different permutation in order to obtain the same calculation</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;cos_cached&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;sin_cached&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># x: [bs, num_attention_heads, seq_len, head_size]</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_cos_sin_cache</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cos_cached</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sin_cached</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">rotate_half</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Rotates half the hidden dims of the input.&quot;&quot;&quot;</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Apply rotary position embedding to query and key tensors.&quot;&quot;&quot;</span>
    <span class="c1"># The first two dimensions of cos and sin are always 1, so we can `squeeze` them.</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [seq_len, dim]</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># [seq_len, dim]</span>
    <span class="n">cos</span> <span class="o">=</span> <span class="n">cos</span><span class="p">[</span><span class="n">position_ids</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [bs, 1, seq_len, dim]</span>
    <span class="n">sin</span> <span class="o">=</span> <span class="n">sin</span><span class="p">[</span><span class="n">position_ids</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [bs, 1, seq_len, dim]</span>

    <span class="n">q_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="n">k_embed</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotate_half</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_embed</span><span class="p">,</span> <span class="n">k_embed</span>

<span class="k">class</span><span class="w"> </span><span class="nc">RoPEAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span> <span class="o">=</span> <span class="n">RotaryEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Get rotary embeddings</span>
        <span class="k">if</span> <span class="n">position_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">position_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotary_emb</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>

        <span class="c1"># Apply rotary position embedding</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">apply_rotary_pos_emb</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">position_ids</span><span class="p">)</span>

        <span class="c1"># Handle past key-value cache</span>
        <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Apply causal mask</span>
        <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
            <span class="n">diagonal</span><span class="o">=</span><span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax and dropout</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span><span class="w"> </span><span class="nc">LlamaRotaryEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Llama-style RoPE with scaling for longer sequences&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">max_position_embeddings</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">base</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scaling_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">scaling_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dim</span> <span class="o">=</span> <span class="n">dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_position_embeddings</span> <span class="o">=</span> <span class="n">max_position_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base</span> <span class="o">=</span> <span class="n">base</span>

        <span class="n">inv_freq</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">base</span> <span class="o">**</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">/</span> <span class="n">dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;inv_freq&quot;</span><span class="p">,</span> <span class="n">inv_freq</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Build here to make `torch.jit.trace` work.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_cos_sin_cache</span><span class="p">(</span>
            <span class="n">seq_len</span><span class="o">=</span><span class="n">max_position_embeddings</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_set_cos_sin_cache</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaling_factor</span>  <span class="c1"># Apply scaling</span>

        <span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,j-&gt;ij&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">inv_freq</span><span class="p">)</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">freqs</span><span class="p">,</span> <span class="n">freqs</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;cos_cached&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">cos</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;sin_cached&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">.</span><span class="n">sin</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="p">),</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_cached</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_set_cos_sin_cache</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cos_cached</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sin_cached</span><span class="p">[:</span><span class="n">seq_len</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>
</code></pre></div>
<p><strong>Key Properties:</strong></p>
<ol>
<li><strong>Relative Position Encoding</strong>: Naturally encodes relative distances</li>
<li><strong>Length Generalization</strong>: Works for sequences longer than training</li>
<li><strong>Efficiency</strong>: No additional parameters beyond base frequencies</li>
<li><strong>Rotational Invariance</strong>: Maintains geometric properties</li>
</ol>
<p><strong>Scaling Techniques:</strong></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Formula</th>
<th>Use Case</th>
</tr>
</thead>
<tbody>
<tr>
<td>Linear Scaling</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(t' = t / s\)</span>\)</span></td>
<td>Moderate extensions</td>
</tr>
<tr>
<td>NTK Scaling</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\theta_i' = \theta_i \cdot s^{-2i/d}\)</span>\)</span></td>
<td>Better long-range</td>
</tr>
<tr>
<td>Dynamic Scaling</td>
<td>Adaptive <span class="arithmatex">\(<span class="arithmatex">\(s\)</span>\)</span></td>
<td>Variable lengths</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Very high; standard in modern LLMs.</p>
<p><strong>Models/Frameworks:</strong> Llama, GPT-NeoX, PaLM, and most recent models.</p>
<h3 id="alibi-attention-with-linear-biases">ALiBi (Attention with Linear Biases)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2108.12409">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/ofirpress/attention_with_linear_biases">ofirpress/attention_with_linear_biases</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2108.12409">ALiBi vs RoPE Comparison</a></p>
<p><strong>Motivation:</strong> Enable length extrapolation without position embeddings by adding linear biases to attention scores.</p>
<p><strong>Problem:</strong> Models trained on short sequences often fail on longer sequences due to position encoding limitations.</p>
<p><strong>Solution:</strong> Add linearly decreasing biases to attention scores based on key-query distance, eliminating the need for position embeddings.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>ALiBi Bias Calculation:</strong>
For head <span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span> with slope <span class="arithmatex">\(<span class="arithmatex">\(m_h\)</span>\)</span>, the bias for query position <span class="arithmatex">\(<span class="arithmatex">\(i\)</span>\)</span> attending to key position <span class="arithmatex">\(<span class="arithmatex">\(j\)</span>\)</span> is:
<span class="arithmatex">\(<span class="arithmatex">\(\text{bias}_{h,i,j} = m_h \cdot (j - i)\)</span>\)</span></p>
<p><strong>Modified Attention Scores:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{score}_{h,i,j} = \frac{q_i^T k_j}{\sqrt{d_k}} + m_h \cdot (j - i)\)</span>\)</span></p>
<p><strong>Slope Assignment:</strong>
For <span class="arithmatex">\(<span class="arithmatex">\(n\)</span>\)</span> heads, slopes are assigned as:
<span class="arithmatex">\(<span class="arithmatex">\(m_h = \frac{1}{2^{\frac{8h}{n}}}, \quad h = 1, 2, \ldots, n\)</span>\)</span></p>
<p><strong>Causal Mask Integration:</strong>
For causal attention, biases are only applied to valid positions:
<span class="arithmatex">\(<span class="arithmatex">\(\text{ALiBi\_score}_{h,i,j} = \begin{cases}
\frac{q_i^T k_j}{\sqrt{d_k}} + m_h \cdot (j - i) &amp; \text{if } j \leq i \\
-\infty &amp; \text{if } j &gt; i
\end{cases}\)</span>\)</span></p>
<p><strong>Implementation:</strong></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ALiBiAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

        <span class="c1"># Pre-compute ALiBi slopes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;slopes&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_alibi_slopes</span><span class="p">(</span><span class="n">n_heads</span><span class="p">))</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_alibi_slopes</span><span class="p">(</span><span class="n">n_heads</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate ALiBi slopes for each attention head&quot;&quot;&quot;</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">get_slopes_power_of_2</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">**-</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">-</span><span class="mi">3</span><span class="p">)))</span>
            <span class="n">ratio</span> <span class="o">=</span> <span class="n">start</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">start</span><span class="o">*</span><span class="n">ratio</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)]</span>

        <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">n_heads</span><span class="p">)</span><span class="o">.</span><span class="n">is_integer</span><span class="p">():</span>
            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">get_slopes_power_of_2</span><span class="p">(</span><span class="n">n_heads</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Handle non-power-of-2 cases</span>
            <span class="n">closest_power_of_2</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">n_heads</span><span class="p">))</span>
            <span class="n">slopes</span> <span class="o">=</span> <span class="n">get_slopes_power_of_2</span><span class="p">(</span><span class="n">closest_power_of_2</span><span class="p">)</span>

            <span class="c1"># Add extra slopes for remaining heads</span>
            <span class="n">extra_slopes</span> <span class="o">=</span> <span class="n">get_slopes_power_of_2</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">closest_power_of_2</span><span class="p">)</span>
            <span class="n">slopes</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">extra_slopes</span><span class="p">[</span><span class="n">closest_power_of_2</span><span class="p">:</span><span class="n">n_heads</span><span class="p">])</span>

            <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">slopes</span><span class="p">[:</span><span class="n">n_heads</span><span class="p">])</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_alibi_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate ALiBi bias matrix&quot;&quot;&quot;</span>
        <span class="c1"># Create position matrix</span>
        <span class="n">context_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
        <span class="n">memory_position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Calculate relative positions (j - i)</span>
        <span class="n">relative_position</span> <span class="o">=</span> <span class="n">memory_position</span> <span class="o">-</span> <span class="n">context_position</span>

        <span class="c1"># Apply slopes to get bias for each head</span>
        <span class="n">bias</span> <span class="o">=</span> <span class="n">relative_position</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">slopes</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>

        <span class="k">return</span> <span class="n">bias</span>  <span class="c1"># [n_heads, seq_len, seq_len]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Project to Q, K, V</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Handle past key-value cache</span>
        <span class="k">if</span> <span class="n">past_kv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">past_k</span><span class="p">,</span> <span class="n">past_v</span> <span class="o">=</span> <span class="n">past_kv</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_k</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">past_v</span><span class="p">,</span> <span class="n">v</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">seq_len_k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Compute attention scores</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_head</span><span class="p">)</span>

        <span class="c1"># Add ALiBi bias</span>
        <span class="n">alibi_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_alibi_bias</span><span class="p">(</span><span class="n">seq_len_k</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Handle different sequence lengths for q and k</span>
        <span class="k">if</span> <span class="n">seq_len</span> <span class="o">!=</span> <span class="n">seq_len_k</span><span class="p">:</span>
            <span class="c1"># For generation with past_kv, adjust bias</span>
            <span class="n">alibi_bias</span> <span class="o">=</span> <span class="n">alibi_bias</span><span class="p">[:,</span> <span class="o">-</span><span class="n">seq_len</span><span class="p">:,</span> <span class="p">:]</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">alibi_bias</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># Add batch dimension</span>

        <span class="c1"># Apply attention mask if provided</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply causal mask for autoregressive models</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len_k</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">),</span>
            <span class="n">diagonal</span><span class="o">=</span><span class="n">seq_len_k</span> <span class="o">-</span> <span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">causal_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>

        <span class="c1"># Apply softmax and dropout</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">)</span>

        <span class="c1"># Apply attention to values</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

        <span class="c1"># Reshape and project</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">present_kv</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">present_kv</span>

        <span class="k">return</span> <span class="n">output</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ALiBiTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Complete transformer block with ALiBi attention&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">ALiBiAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Self-attention with residual connection</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> 
            <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">present_kv</span> <span class="o">=</span> <span class="n">attn_output</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">attn_output</span>

        <span class="c1"># Feed-forward with residual connection</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">present_kv</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<p><strong>Length Extrapolation Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Training Length</th>
<th>Test Length</th>
<th>ALiBi Performance</th>
<th>Standard Attention</th>
</tr>
</thead>
<tbody>
<tr>
<td>1K</td>
<td>2K</td>
<td>95%</td>
<td>60%</td>
</tr>
<tr>
<td>1K</td>
<td>4K</td>
<td>90%</td>
<td>30%</td>
</tr>
<tr>
<td>1K</td>
<td>8K</td>
<td>85%</td>
<td>15%</td>
</tr>
<tr>
<td>2K</td>
<td>16K</td>
<td>80%</td>
<td>5%</td>
</tr>
</tbody>
</table>
<p><strong>Slope Distribution:</strong></p>
<table>
<thead>
<tr>
<th>Head Index</th>
<th>Slope (8 heads)</th>
<th>Slope (16 heads)</th>
<th>Attention Range</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>1/2</td>
<td>1/2</td>
<td>Short-range</td>
</tr>
<tr>
<td>2</td>
<td>1/4</td>
<td>1/4</td>
<td>Medium-range</td>
</tr>
<tr>
<td>4</td>
<td>1/16</td>
<td>1/16</td>
<td>Long-range</td>
</tr>
<tr>
<td>8</td>
<td>1/256</td>
<td>1/256</td>
<td>Very long-range</td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Medium; used in specific models focused on length extrapolation.</p>
<p><strong>Models/Frameworks:</strong> BLOOM, some research models, specialized long-context architectures.</p>
<h2 id="training-and-optimization-innovations">Training and Optimization Innovations</h2>
<h3 id="mixture-of-experts-moe">Mixture of Experts (MoE)</h3>
<p><strong>Reference Links:</strong>
- 📄 <strong>Switch Transformer</strong>: <a href="https://arxiv.org/abs/2101.03961">Scaling to Trillion Parameter Models</a>
- 📄 <strong>GLaM</strong>: <a href="https://arxiv.org/abs/2112.06905">Efficient Scaling of Language Models with Mixture-of-Experts</a>
- 📄 <strong>PaLM</strong>: <a href="https://arxiv.org/abs/2204.02311">Scaling Language Modeling with Pathways</a>
- 📄 <strong>Mixtral 8x7B</strong>: <a href="https://arxiv.org/abs/2401.04088">Mixtral of Experts</a>
- 💻 <strong>FairScale MoE</strong>: <a href="https://github.com/facebookresearch/fairscale">Facebook's MoE Implementation</a>
- 💻 <strong>DeepSpeed MoE</strong>: <a href="https://github.com/microsoft/DeepSpeed">Microsoft's MoE Framework</a>
- 💻 <strong>Megablocks</strong>: <a href="https://github.com/stanford-futuredata/megablocks">Efficient MoE Training</a>
- 🤗 <strong>HuggingFace MoE</strong>: <a href="https://huggingface.co/docs/transformers/model_doc/switch_transformer">Transformers MoE Models</a></p>
<p><img alt="MoE Architecture" src="https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-11_at_10.39.58_PM_V9dKaAg.png" />
<em>Figure: Mixture of Experts architecture showing sparse expert routing and load balancing</em></p>
<p><strong>Research Context and Evolution:</strong></p>
<p>Mixture of Experts represents a paradigm shift from dense to sparse computation, enabling unprecedented model scaling. The concept, originally from ensemble learning, has been revolutionized for modern deep learning through innovations in routing algorithms and distributed training.</p>
<p><strong>The Scaling Challenge:</strong></p>
<p>Traditional dense models face fundamental limitations:
- <strong>Quadratic scaling</strong>: Both parameters and computation grow together
- <strong>Memory bottlenecks</strong>: All parameters must be loaded for every forward pass
- <strong>Diminishing returns</strong>: Adding parameters beyond a point yields minimal improvements</p>
<p><strong>MoE Solution: Sparse Activation</strong></p>
<p>MoE decouples model capacity from computational cost:
- <strong>Sparse routing</strong>: Only a subset of experts process each token
- <strong>Conditional computation</strong>: Different inputs activate different parameters
- <strong>Scalable architecture</strong>: Can add experts without proportional compute increase</p>
<p><img alt="MoE vs Dense Comparison" src="https://huggingface.co/blog/assets/76_moe/01_moe_vs_dense.png" />
<em>Figure: MoE vs Dense model comparison showing parameter efficiency and computational patterns</em></p>
<p><strong>Mathematical Foundation and Routing Algorithms:</strong></p>
<p><strong>1. Standard MoE Routing:</strong>
For input token <span class="arithmatex">\(x\)</span>, the gating function computes expert probabilities:
<span class="arithmatex">\(<span class="arithmatex">\(G(x) = \text{Softmax}(x \cdot W_g + \text{noise})\)</span>\)</span></p>
<p>Top-K expert selection:
<span class="arithmatex">\(<span class="arithmatex">\(\text{MoE}(x) = \sum_{i \in \text{TopK}(G(x))} \frac{G(x)_i}{\sum_{j \in \text{TopK}} G(x)_j} \cdot E_i(x)\)</span>\)</span></p>
<p><strong>2. Switch Transformer (Top-1 Routing):</strong>
Simplified routing to single expert with auxiliary loss:
<span class="arithmatex">\(<span class="arithmatex">\(\text{Switch}(x) = G(x)_{\text{argmax}} \cdot E_{\text{argmax}}(x)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\mathcal{L}_{\text{aux}} = \alpha \sum_{i=1}^{E} f_i \cdot P_i\)</span>\)</span></p>
<p>where <span class="arithmatex">\(f_i\)</span> is the fraction of tokens routed to expert <span class="arithmatex">\(i\)</span>, and <span class="arithmatex">\(P_i\)</span> is the average gate probability.</p>
<p><strong>3. GLaM Expert Parallelism:</strong>
Distributed expert computation with capacity constraints:
<span class="arithmatex">\(<span class="arithmatex">\(\text{Capacity}_i = \frac{\text{tokens\_per\_batch}}{\text{num\_experts}} \times \text{capacity\_factor}\)</span>\)</span></p>
<p><strong>4. Advanced Routing Strategies:</strong></p>
<ul>
<li><strong>Hash Routing</strong>: Deterministic expert assignment based on token hash</li>
<li><strong>Learned Routing</strong>: Trainable routing policies with reinforcement learning</li>
<li><strong>Dynamic Routing</strong>: Adaptive expert selection based on input complexity</li>
<li><strong>Hierarchical MoE</strong>: Multi-level expert organization for better specialization</li>
</ul>
<p><strong>Key Research Innovations:</strong></p>
<p><strong>Expert Specialization Patterns:</strong>
- <strong>Syntactic Experts</strong>: Grammar, punctuation, structural patterns
- <strong>Semantic Experts</strong>: Meaning, context, world knowledge
- <strong>Domain Experts</strong>: Technical, scientific, creative content
- <strong>Language Experts</strong>: Multilingual models with language-specific experts</p>
<p><strong>Training Stability Improvements:</strong>
- <strong>Auxiliary Loss Weighting</strong>: Balancing expert utilization vs. performance
- <strong>Expert Dropout</strong>: Preventing over-reliance on specific experts
- <strong>Gradient Clipping</strong>: Stabilizing training with sparse gradients
- <strong>Expert Initialization</strong>: Specialized initialization strategies for experts</p>
<p><strong>Implementation Frameworks and Usage:</strong></p>
<p><strong>1. HuggingFace Transformers Integration:</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># Using Switch Transformer from HuggingFace</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SwitchTransformersForConditionalGeneration</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SwitchTransformersForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/switch-base-8&quot;</span>
<span class="p">)</span>

<span class="c1"># Mixtral 8x7B usage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">MixtralForCausalLM</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MixtralForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>2. DeepSpeed MoE Framework:</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># DeepSpeed MoE configuration</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">deepspeed.moe</span><span class="w"> </span><span class="kn">import</span> <span class="n">MoE</span>

<span class="n">moe_layer</span> <span class="o">=</span> <span class="n">MoE</span><span class="p">(</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
    <span class="n">expert</span><span class="o">=</span><span class="n">expert_layer</span><span class="p">,</span>
    <span class="n">num_experts</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># top-k routing</span>
    <span class="n">capacity_factor</span><span class="o">=</span><span class="mf">1.25</span><span class="p">,</span>
    <span class="n">eval_capacity_factor</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
    <span class="n">min_capacity</span><span class="o">=</span><span class="mi">4</span>
<span class="p">)</span>
</code></pre></div></p>
<p><strong>3. FairScale Implementation:</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># FairScale MoE usage</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fairscale.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">MOELayer</span>

<span class="n">moe</span> <span class="o">=</span> <span class="n">MOELayer</span><span class="p">(</span>
    <span class="n">gate</span><span class="o">=</span><span class="n">Top2Gate</span><span class="p">(</span><span class="n">model_dim</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">),</span>
    <span class="n">experts</span><span class="o">=</span><span class="n">experts</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="n">expert_group</span>
<span class="p">)</span>
</code></pre></div></p>
<p><strong>Critical Implementation Considerations:</strong></p>
<p><strong>1. Memory Management</strong>: <a href="https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/moe">DeepSpeed ZeRO Integration</a>
   - Expert parameter sharding across devices
   - Dynamic expert loading/unloading
   - Gradient accumulation strategies</p>
<p><strong>2. Communication Optimization</strong>: <a href="https://github.com/stanford-futuredata/megablocks">All-to-All Communication</a>
   - Efficient token routing across devices
   - Minimizing communication overhead
   - Asynchronous expert computation</p>
<p><strong>3. Load Balancing Strategies</strong>: <a href="https://arxiv.org/abs/2101.03961">Auxiliary Loss Design</a>
   - Preventing expert collapse
   - Encouraging expert diversity
   - Adaptive capacity management</p>
<p><strong>Advanced Research Directions:</strong></p>
<p><strong>1. Hierarchical MoE Architectures</strong>: <a href="https://arxiv.org/abs/2202.08906">ST-MoE</a>
   - Multi-level expert routing
   - Coarse-to-fine specialization
   - Reduced communication overhead</p>
<p><strong>2. Dynamic Expert Allocation</strong>: <a href="https://arxiv.org/abs/2205.14755">DynaMoE</a>
   - Runtime expert creation/deletion
   - Adaptive capacity management
   - Task-specific expert specialization</p>
<p><strong>3. Expert Compression Techniques</strong>: <a href="https://arxiv.org/abs/2204.07179">MoE Pruning</a>
   - Expert importance scoring
   - Structured pruning strategies
   - Knowledge distillation from experts</p>
<p><strong>Performance Analysis and Trade-offs:</strong></p>
<p><strong>Training Efficiency:</strong>
<div class="highlight"><pre><span></span><code>Metric                  Dense    MoE (8x)   MoE (64x)
Training Speed          1.0×     0.8×       0.6×
Memory per Device       1.0×     0.5×       0.25×
Communication Overhead  Low      Medium     High
Load Balancing Issues   None     Moderate   Significant
</code></pre></div></p>
<p><strong>Inference Characteristics:</strong>
<div class="highlight"><pre><span></span><code>Sequence Length    Dense Latency    MoE Latency    Speedup
512               100ms            80ms           1.25×
2048              400ms            200ms          2.0×
8192              1600ms           600ms          2.67×
</code></pre></div></p>
<p><strong>Expert Utilization Insights:</strong>
- <strong>Syntactic Experts</strong>: Handle grammar, punctuation (high frequency)
- <strong>Semantic Experts</strong>: Process meaning, context (medium frequency)<br />
- <strong>Domain Experts</strong>: Specialized knowledge areas (low frequency)
- <strong>Multilingual Experts</strong>: Language-specific patterns</p>
<p><strong>Production Deployment Considerations:</strong></p>
<p><strong>1. Serving Infrastructure</strong>: <a href="https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/inference">Model Parallelism</a>
   - Expert placement strategies
   - Load balancing across devices
   - Fault tolerance mechanisms</p>
<p><strong>2. Caching Strategies</strong>: <a href="https://arxiv.org/abs/2203.16758">Expert Caching</a>
   - Frequently used expert caching
   - Dynamic expert loading
   - Memory-efficient serving</p>
<p><strong>3. Quantization and Optimization</strong>: <a href="https://arxiv.org/abs/2208.07339">INT8 MoE</a>
   - Expert-specific quantization
   - Mixed precision strategies
   - Hardware-aware optimization
<div class="highlight"><pre><span></span><code>**Scaling Analysis:**

| Model Type | Parameters | Active Parameters | FLOPs Ratio | Memory Ratio |
|------------|------------|-------------------|-------------|---------------|
| Dense | 175B | 175B | 1.0× | 1.0× |
| MoE (8 experts, top-2) | 1.6T | 350B | 2.0× | 0.125× |
| Switch (64 experts) | 1.6T | 175B | 1.0× | 0.0625× |

**Expert Utilization Patterns:**

| Expert Type | Specialization | Usage Pattern |
|-------------|----------------|---------------|
| Syntactic | Grammar, structure | High frequency |
| Semantic | Meaning, context | Medium frequency |
| Domain-specific | Technical terms | Low frequency |
| Rare patterns | Edge cases | Very low frequency |

**Popularity:** High; widely adopted in large-scale models.

**Models/Frameworks:** Switch Transformer, GLaM, PaLM-2, GPT-4 (rumored), many Google models.

### Normalization Innovations

#### RMSNorm (Root Mean Square Normalization)

**Reference Links:**
- 📄 **Paper**: [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)
- 💻 **Code**: [huggingface/transformers](https://github.com/huggingface/transformers)
- 📊 **Analysis**: [RMSNorm vs LayerNorm](https://arxiv.org/abs/1910.07467)

**Motivation:** Simplify layer normalization by removing mean centering while maintaining training stability.

**Problem:** LayerNorm requires computing both mean and variance, adding computational overhead.

**Solution:** Normalize using only the root mean square, eliminating mean computation.

**Mathematical Foundation:**

**Standard LayerNorm:**
$$\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \odot \gamma + \beta$$

where:
- $$\mu = \frac{1}{d}\sum_{i=1}^d x_i$$
- $$\sigma^2 = \frac{1}{d}\sum_{i=1}^d (x_i - \mu)^2$$

**RMSNorm:**
$$\text{RMSNorm}(x) = \frac{x}{\text{RMS}(x)} \odot \gamma$$

where:
$$\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + \epsilon}$$

**Key Differences:**
1. **No mean centering**: $$\mu = 0$$
2. **No bias term**: $$\beta = 0$$
3. **Simplified variance**: $$\sigma^2 = \frac{1}{d}\sum_{i=1}^d x_i^2$$

**Implementation:**

**Implementation Frameworks:**

🔗 **HuggingFace Transformers RMSNorm**: [LlamaRMSNorm](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L76)
🔗 **T5 LayerNorm**: [T5LayerNorm](https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L239)
🔗 **Apex FusedLayerNorm**: [NVIDIA Apex](https://github.com/NVIDIA/apex/tree/master/apex/normalization)
🔗 **FlashAttention RMSNorm**: [Triton Implementation](https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/ops/rms_norm.py)

**Visual Architecture Comparison:**
</code></pre></div>
┌─────────────────────────────────────────────────────────────────┐
│                    LayerNorm vs RMSNorm                        │
├─────────────────────────────────────────────────────────────────┤
│  LayerNorm:                                                     │
│  Input → [Compute μ] → [Compute σ²] → [(x-μ)/σ] → [γ·x + β]    │
│           ↓             ↓              ↓           ↓            │
│         Mean         Variance      Normalize    Scale &amp; Shift   │
│                                                                 │
│  RMSNorm:                                                       │
│  Input → [Compute RMS] → [x/RMS] → [γ·x]                       │
│           ↓              ↓         ↓                            │
│      Root Mean Square  Normalize  Scale Only                   │
│                                                                 │
│  Computational Savings: 50% fewer operations                   │
└─────────────────────────────────────────────────────────────────┘
<div class="highlight"><pre><span></span><code>**Research Context and Evolution:**

RMSNorm emerged from the observation that the mean-centering step in LayerNorm might be unnecessary for many tasks. The key insight is that the scaling factor (variance normalization) provides most of the benefits, while the shifting factor (mean centering) adds computational overhead without proportional benefits.

**Advanced RMSNorm Variants:**

🔗 **Adaptive RMSNorm**: [Learnable scaling factors](https://arxiv.org/abs/2307.14995)
🔗 **Fused RMSNorm**: [CUDA kernel optimizations](https://github.com/NVIDIA/apex/tree/master/apex/normalization)
🔗 **Quantized RMSNorm**: [INT8 implementations](https://arxiv.org/abs/2208.07339)

**Simple Usage Example:**

```python
# HuggingFace Transformers
from transformers.models.llama.modeling_llama import LlamaRMSNorm

# Initialize RMSNorm layer
rms_norm = LlamaRMSNorm(hidden_size=4096, eps=1e-6)

# Apply normalization
normalized_output = rms_norm(hidden_states)
</code></pre></div></p>
<p><strong>Performance Comparison:</strong></p>
<table>
<thead>
<tr>
<th>Normalization</th>
<th>Computation</th>
<th>Memory</th>
<th>Training Speed</th>
<th>Stability</th>
</tr>
</thead>
<tbody>
<tr>
<td>LayerNorm</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(2d)\)</span>\)</span></td>
<td>High</td>
<td>1.0×</td>
<td>High</td>
</tr>
<tr>
<td>RMSNorm</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(d)\)</span>\)</span></td>
<td>Medium</td>
<td>1.1-1.2×</td>
<td>High</td>
</tr>
<tr>
<td>BatchNorm</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(2d)\)</span>\)</span></td>
<td>High</td>
<td>0.9×</td>
<td>Medium</td>
</tr>
<tr>
<td>GroupNorm</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(2d)\)</span>\)</span></td>
<td>High</td>
<td>0.95×</td>
<td>Medium</td>
</tr>
</tbody>
</table>
<p><strong>Computational Savings:</strong></p>
<table>
<thead>
<tr>
<th>Operation</th>
<th>LayerNorm</th>
<th>RMSNorm</th>
<th>Savings</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean computation</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\sum x_i / d\)</span>\)</span></td>
<td>-</td>
<td>50%</td>
</tr>
<tr>
<td>Variance computation</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\sum (x_i - \mu)^2 / d\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(\sum x_i^2 / d\)</span>\)</span></td>
<td>25%</td>
</tr>
<tr>
<td>Bias addition</td>
<td><span class="arithmatex">\(<span class="arithmatex">\(+ \beta\)</span>\)</span></td>
<td>-</td>
<td>100%</td>
</tr>
<tr>
<td><strong>Total FLOPs</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(4d\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(2d\)</span>\)</span></td>
<td><strong>50%</strong></td>
</tr>
</tbody>
</table>
<p><strong>Popularity:</strong> Very high; standard in modern LLMs.</p>
<p><strong>Models/Frameworks:</strong> Llama, PaLM, T5, Chinchilla, and most recent large models.</p>
<h4 id="pre-norm-vs-post-norm">Pre-Norm vs Post-Norm</h4>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2002.04745">On Layer Normalization in the Transformer Architecture</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2002.04745">Pre-norm vs Post-norm</a></p>
<p><strong>Motivation:</strong> Improve training stability and convergence by changing the position of normalization layers.</p>
<p><strong>Post-Norm (Original Transformer):</strong>
<div class="highlight"><pre><span></span><code>Output = LayerNorm(x + Sublayer(x))
</code></pre></div></p>
<p><strong>Pre-Norm (Modern Approach):</strong>
<div class="highlight"><pre><span></span><code>Output = x + Sublayer(LayerNorm(x))
</code></pre></div></p>
<p><strong>Mathematical Comparison:</strong></p>
<p><strong>Post-Norm Block:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(y = \text{LayerNorm}(x + \text{Attention}(x))\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(z = \text{LayerNorm}(y + \text{FFN}(y))\)</span>\)</span></p>
<p><strong>Pre-Norm Block:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(y = x + \text{Attention}(\text{LayerNorm}(x))\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(z = y + \text{FFN}(\text{LayerNorm}(y))\)</span>\)</span></p>
<p><strong>Training Characteristics:</strong></p>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Post-Norm</th>
<th>Pre-Norm</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Gradient Flow</strong></td>
<td>Can suffer from vanishing gradients</td>
<td>Better gradient flow</td>
</tr>
<tr>
<td><strong>Training Stability</strong></td>
<td>Requires careful initialization</td>
<td>More stable</td>
</tr>
<tr>
<td><strong>Learning Rate</strong></td>
<td>Needs lower LR for deep models</td>
<td>Can use higher LR</td>
</tr>
<tr>
<td><strong>Convergence</strong></td>
<td>Slower for deep models</td>
<td>Faster convergence</td>
</tr>
<tr>
<td><strong>Final Performance</strong></td>
<td>Slightly better (sometimes)</td>
<td>Competitive</td>
</tr>
</tbody>
</table>
<p><strong>Implementation Frameworks:</strong></p>
<p>🔗 <strong>HuggingFace Pre-Norm</strong>: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L393">GPT-2 Block</a>
🔗 <strong>Llama Pre-Norm</strong>: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L693">LlamaDecoderLayer</a>
🔗 <strong>T5 Pre-Norm</strong>: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/t5/modeling_t5.py#L688">T5Block</a>
🔗 <strong>BERT Post-Norm</strong>: <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py#L421">BertLayer</a></p>
<p><strong>Visual Architecture Comparison:</strong></p>
<div class="highlight"><pre><span></span><code>┌─────────────────────────────────────────────────────────────────┐
│                 Post-Norm vs Pre-Norm Architecture             │
├─────────────────────────────────────────────────────────────────┤
│  Post-Norm (Original Transformer):                             │
│  Input → Attention → Add → LayerNorm → FFN → Add → LayerNorm   │
│    ↓        ↓         ↓       ↓        ↓     ↓       ↓         │
│    x    Attn(x)    x+Attn   LN(x+A)   FFN   x+FFN   LN(x+F)   │
│                                                                 │
│  Pre-Norm (Modern Approach):                                   │
│  Input → LayerNorm → Attention → Add → LayerNorm → FFN → Add   │
│    ↓        ↓           ↓        ↓       ↓        ↓     ↓       │
│    x      LN(x)     Attn(LN)  x+Attn   LN(x)    FFN  x+FFN    │
│                                                                 │
│  Key Difference: Normalization applied BEFORE vs AFTER         │
└─────────────────────────────────────────────────────────────────┘
</code></pre></div>
<p><strong>Research Insights:</strong></p>
<p>The shift from post-norm to pre-norm represents one of the most significant architectural improvements in modern transformers. Research shows that pre-norm provides:</p>
<ol>
<li><strong>Better Gradient Flow</strong>: Direct residual connections preserve gradients</li>
<li><strong>Training Stability</strong>: Reduces gradient explosion in deep networks</li>
<li><strong>Faster Convergence</strong>: Enables higher learning rates</li>
<li><strong>Scalability</strong>: Essential for training very deep models (&gt;24 layers)</li>
</ol>
<p><strong>Critical Implementation Considerations:</strong></p>
<p>🔗 <strong>Gradient Analysis</strong>: <a href="https://arxiv.org/abs/2002.04745">Understanding Pre-norm Benefits</a>
🔗 <strong>Initialization Strategies</strong>: <a href="https://arxiv.org/abs/2002.04745">Proper Weight Initialization</a>
🔗 <strong>Learning Rate Scheduling</strong>: <a href="https://arxiv.org/abs/2006.04768">Adaptive LR for Pre-norm</a></p>
<p><strong>Simple Usage Examples:</strong></p>
<div class="highlight"><pre><span></span><code><span class="c1"># Pre-Norm (Modern - Recommended)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaConfig</span><span class="p">,</span> <span class="n">LlamaModel</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LlamaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># Uses pre-norm by default</span>

<span class="c1"># Post-Norm (Legacy)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertConfig</span><span class="p">,</span> <span class="n">BertModel</span>

<span class="n">config</span> <span class="o">=</span> <span class="n">BertConfig</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>  <span class="c1"># Uses post-norm</span>
</code></pre></div>
<p><strong>Gradient Analysis:</strong></p>
<p><strong>Post-Norm Gradient:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial L}{\partial x} = \frac{\partial L}{\partial \text{LN}(x + f(x))} \cdot \frac{\partial \text{LN}(x + f(x))}{\partial x}\)</span>\)</span></p>
<p><strong>Pre-Norm Gradient:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial L}{\partial x} = \frac{\partial L}{\partial (x + f(\text{LN}(x)))} \cdot (1 + \frac{\partial f(\text{LN}(x))}{\partial x})\)</span>\)</span></p>
<p>The pre-norm formulation provides a more direct gradient path through the identity connection.</p>
<p><strong>Popularity:</strong> Pre-norm is now standard; post-norm mainly historical.</p>
<p><strong>Models/Frameworks:</strong> Pre-norm: Llama, GPT-3, T5, PaLM; Post-norm: Original Transformer, BERT.</p>
<h2 id="performance-analysis-and-comparisons">Performance Analysis and Comparisons</h2>
<h3 id="computational-complexity-comparison">Computational Complexity Comparison</h3>
<table>
<thead>
<tr>
<th>Architecture</th>
<th>Time Complexity</th>
<th>Space Complexity</th>
<th>Memory Efficiency</th>
<th>Training Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Standard Attention</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2 d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span></td>
<td>Low</td>
<td>1.0×</td>
</tr>
<tr>
<td><strong>Linformer</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nkd)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nk)\)</span>\)</span></td>
<td>High</td>
<td>1.5-2.0×</td>
</tr>
<tr>
<td><strong>Performer</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nd \log d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(nd)\)</span>\)</span></td>
<td>High</td>
<td>1.2-1.8×</td>
</tr>
<tr>
<td><strong>FlashAttention</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2 d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n)\)</span>\)</span></td>
<td>Very High</td>
<td>2.0-4.0×</td>
</tr>
<tr>
<td><strong>Sparse Attention</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \sqrt{n} d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n \sqrt{n})\)</span>\)</span></td>
<td>Medium</td>
<td>1.3-2.5×</td>
</tr>
<tr>
<td><strong>MQA</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2 d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span></td>
<td>Medium</td>
<td>1.1-1.3×</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2 d)\)</span>\)</span></td>
<td><span class="arithmatex">\(<span class="arithmatex">\(O(n^2)\)</span>\)</span></td>
<td>Medium</td>
<td>1.05-1.2×</td>
</tr>
</tbody>
</table>
<h3 id="memory-usage-analysis">Memory Usage Analysis</h3>
<p><strong>Standard Multi-Head Attention:</strong>
- <strong>Attention Matrix</strong>: <span class="arithmatex">\(<span class="arithmatex">\(n^2 \times h\)</span>\)</span> (where <span class="arithmatex">\(<span class="arithmatex">\(h\)</span>\)</span> = number of heads)
- <strong>Key/Value Cache</strong>: <span class="arithmatex">\(<span class="arithmatex">\(2 \times n \times d \times h\)</span>\)</span>
- <strong>Total Memory</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n^2 h + ndhd)\)</span>\)</span></p>
<p><strong>Multi-Query Attention:</strong>
- <strong>Attention Matrix</strong>: <span class="arithmatex">\(<span class="arithmatex">\(n^2 \times h\)</span>\)</span>
- <strong>Key/Value Cache</strong>: <span class="arithmatex">\(<span class="arithmatex">\(2 \times n \times d\)</span>\)</span> (shared across heads)
- <strong>Total Memory</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(n^2 h + nd)\)</span>\)</span>
- <strong>Memory Reduction</strong>: <span class="arithmatex">\(<span class="arithmatex">\(\frac{h-1}{h} \times 100\%\)</span>\)</span> for KV cache</p>
<p><strong>FlashAttention:</strong>
- <strong>Attention Matrix</strong>: Not materialized
- <strong>Key/Value Cache</strong>: <span class="arithmatex">\(<span class="arithmatex">\(2 \times n \times d \times h\)</span>\)</span>
- <strong>Working Memory</strong>: <span class="arithmatex">\(<span class="arithmatex">\(O(\sqrt{n} \times d \times h)\)</span>\)</span>
- <strong>Memory Reduction</strong>: Up to 10-20× for attention computation</p>
<h3 id="scaling-behavior">Scaling Behavior</h3>
<table>
<thead>
<tr>
<th>Sequence Length</th>
<th>Standard Attention</th>
<th>Linformer</th>
<th>Performer</th>
<th>FlashAttention</th>
</tr>
</thead>
<tbody>
<tr>
<td>512</td>
<td>1.0×</td>
<td>0.8×</td>
<td>0.9×</td>
<td>0.7×</td>
</tr>
<tr>
<td>1K</td>
<td>1.0×</td>
<td>0.6×</td>
<td>0.7×</td>
<td>0.5×</td>
</tr>
<tr>
<td>2K</td>
<td>1.0×</td>
<td>0.4×</td>
<td>0.5×</td>
<td>0.3×</td>
</tr>
<tr>
<td>4K</td>
<td>1.0×</td>
<td>0.3×</td>
<td>0.4×</td>
<td>0.2×</td>
</tr>
<tr>
<td>8K</td>
<td>1.0×</td>
<td>0.2×</td>
<td>0.3×</td>
<td>0.15×</td>
</tr>
<tr>
<td>16K</td>
<td>OOM</td>
<td>0.15×</td>
<td>0.2×</td>
<td>0.1×</td>
</tr>
</tbody>
</table>
<h3 id="quality-vs-efficiency-trade-offs">Quality vs Efficiency Trade-offs</h3>
<table>
<thead>
<tr>
<th>Method</th>
<th>Perplexity (↓)</th>
<th>BLEU Score (↑)</th>
<th>Training Time (↓)</th>
<th>Memory Usage (↓)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Standard</strong></td>
<td>15.2</td>
<td>34.5</td>
<td>1.0×</td>
<td>1.0×</td>
</tr>
<tr>
<td><strong>Linformer</strong></td>
<td>15.8</td>
<td>33.9</td>
<td>0.6×</td>
<td>0.4×</td>
</tr>
<tr>
<td><strong>Performer</strong></td>
<td>15.6</td>
<td>34.1</td>
<td>0.7×</td>
<td>0.5×</td>
</tr>
<tr>
<td><strong>FlashAttention</strong></td>
<td>15.2</td>
<td>34.5</td>
<td>0.4×</td>
<td>0.2×</td>
</tr>
<tr>
<td><strong>Sparse (Local)</strong></td>
<td>15.4</td>
<td>34.2</td>
<td>0.5×</td>
<td>0.3×</td>
</tr>
<tr>
<td><strong>MQA</strong></td>
<td>15.3</td>
<td>34.3</td>
<td>0.8×</td>
<td>0.6×</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td>15.2</td>
<td>34.4</td>
<td>0.9×</td>
<td>0.8×</td>
</tr>
</tbody>
</table>
<h2 id="implementation-guidelines-and-best-practices">Implementation Guidelines and Best Practices</h2>
<h3 id="choosing-the-right-architecture">Choosing the Right Architecture</h3>
<p><strong>For Long Sequences (&gt;4K tokens):</strong>
1. <strong>FlashAttention</strong>: Best overall choice for most cases
2. <strong>Linformer</strong>: When approximation is acceptable
3. <strong>Sparse Attention</strong>: For very long sequences with local patterns
4. <strong>ALiBi</strong>: For length extrapolation requirements</p>
<p><strong>For Memory-Constrained Environments:</strong>
1. <strong>Multi-Query Attention (MQA)</strong>: Significant memory savings
2. <strong>Grouped-Query Attention (GQA)</strong>: Balanced trade-off
3. <strong>FlashAttention</strong>: Reduces peak memory usage</p>
<p><strong>For High-Throughput Inference:</strong>
1. <strong>MQA/GQA</strong>: Faster autoregressive generation
2. <strong>FlashAttention</strong>: Optimized CUDA kernels
3. <strong>Sparse Attention</strong>: Reduced computation</p>
<h3 id="implementation-checklist">Implementation Checklist</h3>
<p><strong>Memory Optimization:</strong>
- [ ] Use gradient checkpointing for training
- [ ] Implement attention with memory-efficient backends
- [ ] Use mixed precision (FP16/BF16)
- [ ] Optimize KV cache management</p>
<p><strong>Performance Optimization:</strong>
- [ ] Fuse attention operations when possible
- [ ] Use optimized CUDA kernels (FlashAttention, xFormers)
- [ ] Implement efficient position encoding
- [ ] Optimize feed-forward networks</p>
<p><strong>Numerical Stability:</strong>
- [ ] Use stable softmax implementation
- [ ] Handle attention mask correctly
- [ ] Implement proper gradient clipping
- [ ] Use appropriate epsilon values for normalization</p>
<h3 id="common-implementation-patterns">Common Implementation Patterns</h3>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">OptimizedTransformerBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Production-ready transformer block with best practices&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># Choose attention mechanism based on config</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_type</span> <span class="o">==</span> <span class="s2">&quot;flash&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">FlashAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_type</span> <span class="o">==</span> <span class="s2">&quot;mqa&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiQueryAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">config</span><span class="o">.</span><span class="n">attention_type</span> <span class="o">==</span> <span class="s2">&quot;gqa&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">GroupedQueryAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">StandardAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Use RMSNorm for better efficiency</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span> <span class="o">=</span> <span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">norm_eps</span><span class="p">)</span>

        <span class="c1"># Optimized feed-forward with SwiGLU activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">SwiGLUMLP</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

        <span class="c1"># Optional: Mixture of Experts</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">use_moe</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">MixtureOfExperts</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">position_ids</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                <span class="n">past_kv</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="c1"># Pre-norm architecture</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Attention with optional caching</span>
        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> 
            <span class="n">position_ids</span><span class="o">=</span><span class="n">position_ids</span><span class="p">,</span>
            <span class="n">past_kv</span><span class="o">=</span><span class="n">past_kv</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="n">use_cache</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="n">attn_output</span><span class="p">,</span> <span class="n">present_kv</span> <span class="o">=</span> <span class="n">attn_output</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="n">attn_output</span>

        <span class="c1"># Feed-forward</span>
        <span class="n">residual</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ln2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">residual</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">use_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">present_kv</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">SwiGLUMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;SwiGLU activation for better performance&quot;&quot;&quot;</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">gate</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gate_proj</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">up</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="n">gate</span> <span class="o">*</span> <span class="n">up</span><span class="p">)</span>
</code></pre></div>
<h3 id="debugging-and-profiling">Debugging and Profiling</h3>
<p><strong>Memory Profiling:</strong>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.profiler</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">profile</span><span class="p">(</span>
    <span class="n">activities</span><span class="o">=</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CPU</span><span class="p">,</span> 
                <span class="n">torch</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">ProfilerActivity</span><span class="o">.</span><span class="n">CUDA</span><span class="p">],</span>
    <span class="n">record_shapes</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">profile_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">with_stack</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">prof</span><span class="p">:</span>
    <span class="c1"># Your model forward pass</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

<span class="c1"># Analyze memory usage</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prof</span><span class="o">.</span><span class="n">key_averages</span><span class="p">()</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">sort_by</span><span class="o">=</span><span class="s2">&quot;cuda_memory_usage&quot;</span><span class="p">,</span> <span class="n">row_limit</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></p>
<p><strong>Attention Pattern Visualization:</strong>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">visualize_attention_patterns</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">,</span> <span class="n">layer_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Visualize attention patterns for debugging&quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">output_attentions</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">attentions</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="n">head_idx</span><span class="p">]</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

    <span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Attention Pattern - Layer </span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s1">, Head </span><span class="si">{</span><span class="n">head_idx</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Key Position&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Query Position&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></p>
<h2 id="future-directions-and-research-trends">Future Directions and Research Trends</h2>
<h3 id="emerging-architectures">Emerging Architectures</h3>
<h4 id="mamba-and-state-space-models">Mamba and State Space Models</h4>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-Time Sequence Modeling with Selective State Spaces</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/state-spaces/mamba">state-spaces/mamba</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2111.00396">Structured State Space Models</a>
- 🔬 <strong>Implementation</strong>: <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/mamba">HuggingFace Mamba</a></p>
<p><img alt="Mamba Architecture" src="https://raw.githubusercontent.com/state-spaces/mamba/main/assets/selection_mechanism.png" />
<em>Figure: Mamba's selective state space mechanism with input-dependent parameters</em></p>
<p><strong>Research Context and Motivation:</strong></p>
<p>State Space Models (SSMs) represent a fundamental shift from attention-based architectures to recurrent models with linear complexity. The evolution progresses through:</p>
<ol>
<li><strong>Classical State Spaces</strong>: Linear time-invariant systems</li>
<li><strong>Structured SSMs (S4)</strong>: Diagonal plus low-rank parameterization</li>
<li><strong>Selective SSMs (Mamba)</strong>: Input-dependent state transitions</li>
</ol>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Classical State Space Model:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(h'(t) = Ah(t) + Bx(t)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(y(t) = Ch(t) + Dx(t)\)</span>\)</span></p>
<p><strong>Discretized SSM:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(h_k = \bar{A}h_{k-1} + \bar{B}x_k\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(y_k = Ch_k\)</span>\)</span></p>
<p>where <span class="arithmatex">\(\bar{A} = \exp(\Delta A)\)</span> and <span class="arithmatex">\(\bar{B} = (\Delta A)^{-1}(\exp(\Delta A) - I) \cdot \Delta B\)</span></p>
<p><strong>Mamba's Selective Mechanism:</strong></p>
<p>The key innovation is making parameters <span class="arithmatex">\(B\)</span>, <span class="arithmatex">\(C\)</span>, and <span class="arithmatex">\(\Delta\)</span> functions of the input:</p>
<div class="arithmatex">\[B_k = s_B(x_k), \quad C_k = s_C(x_k), \quad \Delta_k = \tau_{\Delta}(\text{Parameter} + s_{\Delta}(x_k))\]</div>
<p><strong>Selective Scan Algorithm:</strong>
<div class="highlight"><pre><span></span><code><span class="c1"># Simplified Mamba selective scan</span>
<span class="k">def</span><span class="w"> </span><span class="nf">selective_scan</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    u: input sequence [batch, length, dim]</span>
<span class="sd">    delta: step sizes [batch, length, dim] </span>
<span class="sd">    A, B, C: state space parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">u</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Discretize A and B</span>
    <span class="n">deltaA</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">delta</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">A</span><span class="p">)</span>  <span class="c1"># [batch, length, dim, state_size]</span>
    <span class="n">deltaB</span> <span class="o">=</span> <span class="n">delta</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">B</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [batch, length, dim, state_size]</span>

    <span class="c1"># Selective scan (parallel implementation)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">u</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">length</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">deltaA</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">deltaB</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">u</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">C</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">D</span> <span class="o">*</span> <span class="n">u</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Hardware-Efficient Implementation:</strong></p>
<p><strong>1. Parallel Scan Algorithm</strong>: <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py">Efficient Parallel Scan</a>
   - Associative scan for parallelization
   - CUDA kernel optimization
   - Memory-efficient computation</p>
<p><strong>2. Selective State Space Kernel</strong>: <a href="https://github.com/state-spaces/mamba/tree/main/csrc/selective_scan">CUDA Implementation</a>
   - Fused operations for efficiency
   - Optimized memory access patterns
   - Hardware-aware design</p>
<p><strong>Performance Characteristics:</strong></p>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Sequence Length</th>
<th>Memory Usage</th>
<th>Training Speed</th>
<th>Inference Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>Transformer</td>
<td>2K</td>
<td>1.0×</td>
<td>1.0×</td>
<td>1.0×</td>
</tr>
<tr>
<td>Mamba</td>
<td>2K</td>
<td>0.8×</td>
<td>1.2×</td>
<td>1.5×</td>
</tr>
<tr>
<td>Transformer</td>
<td>16K</td>
<td>8.0×</td>
<td>0.3×</td>
<td>0.2×</td>
</tr>
<tr>
<td>Mamba</td>
<td>16K</td>
<td>1.2×</td>
<td>1.1×</td>
<td>1.8×</td>
</tr>
<tr>
<td>Transformer</td>
<td>64K</td>
<td>OOM</td>
<td>OOM</td>
<td>OOM</td>
</tr>
<tr>
<td>Mamba</td>
<td>64K</td>
<td>2.1×</td>
<td>0.9×</td>
<td>2.2×</td>
</tr>
</tbody>
</table>
<p><strong>Research Applications and Results:</strong></p>
<p><strong>1. Language Modeling</strong>: <a href="https://arxiv.org/abs/2312.00752">Mamba Performance</a>
   - Competitive with Transformers on standard benchmarks
   - Superior scaling to long sequences
   - Better inference efficiency</p>
<p><strong>2. DNA Sequence Modeling</strong>: <a href="https://arxiv.org/abs/2306.15794">HyenaDNA</a>
   - Million-token sequences
   - Genomic pattern recognition
   - Long-range dependency modeling</p>
<p><strong>3. Audio Processing</strong>: <a href="https://arxiv.org/abs/2403.01456">Audio Mamba</a>
   - Speech recognition and generation
   - Music modeling
   - Real-time audio processing</p>
<h4 id="retnet-retentive-networks">RetNet (Retentive Networks)</h4>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2307.08621">Retentive Network: A Successor to Transformer for Large Language Models</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/microsoft/torchscale/tree/main/torchscale/architecture/retnet">microsoft/torchscale</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2307.08621">RetNet vs Transformer Comparison</a></p>
<p><img alt="RetNet Architecture" src="https://github.com/microsoft/torchscale/raw/main/docs/retnet.png" />
<em>Figure: RetNet architecture showing retention mechanism and multi-scale modeling</em></p>
<p><strong>Core Innovation: Retention Mechanism</strong></p>
<p>RetNet replaces attention with a retention mechanism that provides:
1. <strong>Training Parallelism</strong>: Like Transformers
2. <strong>Inference Efficiency</strong>: Like RNNs
3. <strong>Strong Performance</strong>: Competitive with Transformers</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Retention Mechanism:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{Retention}(X) = (QK^T \odot D) V\)</span>\)</span></p>
<p>where <span class="arithmatex">\(D\)</span> is a causal decay matrix:
<span class="arithmatex">\(<span class="arithmatex">\(D_{nm} = \begin{cases}
\gamma^{n-m} &amp; \text{if } n \geq m \\
0 &amp; \text{if } n &lt; m
\end{cases}\)</span>\)</span></p>
<p><strong>Multi-Scale Retention:</strong>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MultiScaleRetention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">num_heads</span>

        <span class="c1"># Different decay rates for different heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gammas</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">incremental_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>

        <span class="c1"># Compute retention for each head</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">):</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">gammas</span><span class="p">[</span><span class="n">h</span><span class="p">])</span>

            <span class="c1"># Create decay matrix</span>
            <span class="n">decay_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>
            <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">decay_matrix</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">**</span> <span class="p">(</span><span class="n">positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">positions</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">decay_matrix</span> <span class="o">=</span> <span class="n">decay_matrix</span> <span class="o">*</span> <span class="n">decay_mask</span>

            <span class="c1"># Apply retention</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">h</span><span class="p">],</span> <span class="n">k</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">*</span> <span class="n">decay_matrix</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">h</span><span class="p">])</span>
            <span class="n">outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Training vs Inference Modes:</strong></p>
<p><strong>1. Parallel Training</strong>: <a href="https://github.com/microsoft/torchscale/blob/main/torchscale/architecture/retnet.py">Parallel Implementation</a>
   - Matrix operations like Transformers
   - Efficient gradient computation
   - Stable training dynamics</p>
<p><strong>2. Recurrent Inference</strong>: <a href="https://github.com/microsoft/torchscale/blob/main/torchscale/architecture/retnet.py">Recurrent Implementation</a>
   - Constant memory usage
   - Linear time complexity
   - Real-time generation</p>
<p><strong>Performance Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Transformer</th>
<th>RetNet</th>
<th>Improvement</th>
</tr>
</thead>
<tbody>
<tr>
<td>Training Speed</td>
<td>1.0×</td>
<td>1.0×</td>
<td>Comparable</td>
</tr>
<tr>
<td>Inference Memory</td>
<td>O(n)</td>
<td>O(1)</td>
<td>Linear → Constant</td>
</tr>
<tr>
<td>Inference Speed</td>
<td>1.0×</td>
<td>1.3-2.1×</td>
<td>30-110% faster</td>
</tr>
<tr>
<td>Perplexity</td>
<td>Baseline</td>
<td>-0.5 to +0.2</td>
<td>Competitive</td>
</tr>
</tbody>
</table>
<h4 id="mixture-of-depths-mod">Mixture of Depths (MoD)</h4>
<p><strong>Reference Links:</strong>
- 📄 <strong>Paper</strong>: <a href="https://arxiv.org/abs/2404.02258">Mixture of Depths: Dynamically allocating compute in transformer-based language models</a>
- 💻 <strong>Code</strong>: <a href="https://github.com/google-research/mixture-of-depths">google-research/mixture-of-depths</a>
- 📊 <strong>Analysis</strong>: <a href="https://arxiv.org/abs/2404.02258">Dynamic Computation Allocation</a></p>
<p><strong>Core Innovation: Dynamic Layer Computation</strong></p>
<p>MoD allows tokens to "skip" certain layers based on learned routing decisions, optimizing compute allocation.</p>
<p><strong>Mathematical Foundation:</strong></p>
<p><strong>Router Function:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(r_l(x) = \sigma(W_r^{(l)} x + b_r^{(l)})\)</span>\)</span></p>
<p><strong>Capacity-Constrained Routing:</strong>
<span class="arithmatex">\(<span class="arithmatex">\(\text{top-k}(r_l(X), k = \lfloor \alpha \cdot n \rfloor)\)</span>\)</span></p>
<p>where <span class="arithmatex">\(\alpha\)</span> is the capacity factor (e.g., 0.5 for 50% of tokens).</p>
<p><strong>Implementation Example:</strong>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MixtureOfDepthsLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">capacity_factor</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capacity_factor</span> <span class="o">=</span> <span class="n">capacity_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">router</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layer</span> <span class="o">=</span> <span class="n">TransformerLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Compute routing scores</span>
        <span class="n">router_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">router</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [B, T]</span>

        <span class="c1"># Select top-k tokens for processing</span>
        <span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capacity_factor</span> <span class="o">*</span> <span class="n">T</span><span class="p">)</span>
        <span class="n">top_k_scores</span><span class="p">,</span> <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">router_scores</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Process selected tokens</span>
        <span class="n">selected_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">top_k_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>
        <span class="n">processed_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_layer</span><span class="p">(</span><span class="n">selected_tokens</span><span class="p">)</span>

        <span class="c1"># Scatter back to original positions</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">output</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">top_k_indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">processed_tokens</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div></p>
<p><strong>Efficiency Analysis:</strong></p>
<table>
<thead>
<tr>
<th>Capacity Factor</th>
<th>FLOPs Reduction</th>
<th>Performance Retention</th>
<th>Memory Savings</th>
</tr>
</thead>
<tbody>
<tr>
<td>100% (baseline)</td>
<td>0%</td>
<td>100%</td>
<td>0%</td>
</tr>
<tr>
<td>75%</td>
<td>25%</td>
<td>98-99%</td>
<td>15-20%</td>
</tr>
<tr>
<td>50%</td>
<td>50%</td>
<td>95-97%</td>
<td>30-35%</td>
</tr>
<tr>
<td>25%</td>
<td>75%</td>
<td>85-90%</td>
<td>50-55%</td>
</tr>
</tbody>
</table>
<p><strong>Advanced Research Directions:</strong></p>
<p><strong>1. Hybrid Architectures</strong>: <a href="https://arxiv.org/abs/2403.19888">Mamba-Transformer Hybrids</a>
   - Combining attention and state space models
   - Layer-wise architecture search
   - Task-specific optimization</p>
<p><strong>2. Hardware Co-design</strong>: <a href="https://arxiv.org/abs/2312.00752">Efficient SSM Hardware</a>
   - Custom ASIC designs
   - Memory hierarchy optimization
   - Parallel processing units</p>
<p><strong>3. Theoretical Analysis</strong>: <a href="https://arxiv.org/abs/2405.21060">SSM Theory</a>
   - Expressivity comparisons
   - Approximation capabilities
   - Scaling law analysis</p>
<h3 id="research-frontiers">Research Frontiers</h3>
<p><strong>Efficiency Improvements:</strong>
- Hardware-aware architecture design
- Dynamic sparsity patterns
- Adaptive computation time
- Neural architecture search for transformers</p>
<p><strong>Scaling Laws:</strong>
- Understanding optimal model configurations
- Compute-optimal training strategies
- Data efficiency improvements
- Transfer learning optimization</p>
<p><strong>Long Context Modeling:</strong>
- Infinite attention mechanisms
- Hierarchical attention patterns
- Memory-augmented transformers
- Retrieval-augmented architectures</p>
<h2 id="comprehensive-references-and-resources">Comprehensive References and Resources</h2>
<h3 id="foundational-papers">Foundational Papers</h3>
<p><strong>Original Transformer:</strong>
- 📄 <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> - Vaswani et al., 2017</p>
<p><strong>Efficiency Improvements:</strong>
- 📄 <a href="https://arxiv.org/abs/1901.02860">Transformer-XL</a> - Dai et al., 2019
- 📄 <a href="https://arxiv.org/abs/2001.04451">Reformer</a> - Kitaev et al., 2020
- 📄 <a href="https://arxiv.org/abs/2006.04768">Linformer</a> - Wang et al., 2020
- 📄 <a href="https://arxiv.org/abs/2009.14794">Performer</a> - Choromanski et al., 2020
- 📄 <a href="https://arxiv.org/abs/2205.14135">FlashAttention</a> - Dao et al., 2022
- 📄 <a href="https://arxiv.org/abs/2307.08691">FlashAttention-2</a> - Dao, 2023</p>
<p><strong>Position Encoding:</strong>
- 📄 <a href="https://arxiv.org/abs/2104.09864">RoPE</a> - Su et al., 2021
- 📄 <a href="https://arxiv.org/abs/2108.12409">ALiBi</a> - Press et al., 2021</p>
<p><strong>Attention Variants:</strong>
- 📄 <a href="https://arxiv.org/abs/1911.02150">Multi-Query Attention</a> - Shazeer, 2019
- 📄 <a href="https://arxiv.org/abs/2305.13245">Grouped-Query Attention</a> - Ainslie et al., 2023</p>
<p><strong>Training Innovations:</strong>
- 📄 <a href="https://arxiv.org/abs/2101.03961">Switch Transformer</a> - Fedus et al., 2021
- 📄 <a href="https://arxiv.org/abs/2112.06905">GLaM</a> - Du et al., 2021
- 📄 <a href="https://arxiv.org/abs/1910.07467">RMSNorm</a> - Zhang &amp; Sennrich, 2019</p>
<h3 id="implementation-resources">Implementation Resources</h3>
<p><strong>Official Implementations:</strong>
- 💻 <a href="https://github.com/huggingface/transformers">Hugging Face Transformers</a>
- 💻 <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention</a>
- 💻 <a href="https://github.com/facebookresearch/xformers">xFormers</a>
- 💻 <a href="https://github.com/openai/triton">Triton</a></p>
<p><strong>Educational Resources:</strong>
- 📚 <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>
- 📚 <a href="https://transformer-circuits.pub/">Transformer Circuits Thread</a>
- 📚 <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">Attention Mechanisms Guide</a></p>
<p><strong>Benchmarking and Evaluation:</strong>
- 🔧 <a href="https://github.com/google-research/long-range-arena">Long Range Arena</a>
- 🔧 <a href="https://gluebenchmark.com/">GLUE Benchmark</a>
- 🔧 <a href="https://super.gluebenchmark.com/">SuperGLUE</a></p>
<h3 id="model-implementations">Model Implementations</h3>
<p><strong>Popular Models Using Advanced Techniques:</strong>
- <strong>Llama 2/3</strong>: RoPE, RMSNorm, SwiGLU, GQA
- <strong>GPT-4</strong>: Rumored to use MoE, advanced attention
- <strong>PaLM</strong>: RMSNorm, parallel layers, SwiGLU
- <strong>BLOOM</strong>: ALiBi, sparse attention patterns
- <strong>T5</strong>: Relative position encoding, pre-norm
- <strong>Switch Transformer</strong>: Mixture of Experts</p>
<h3 id="performance-optimization-tools">Performance Optimization Tools</h3>
<p><strong>CUDA Kernels:</strong>
- <a href="https://github.com/Dao-AILab/flash-attention">FlashAttention CUDA</a>
- <a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a>
- <a href="https://github.com/microsoft/DeepSpeed">DeepSpeed</a></p>
<p><strong>Memory Optimization:</strong>
- <a href="https://pytorch.org/docs/stable/checkpoint.html">Gradient Checkpointing</a>
- <a href="https://www.deepspeed.ai/tutorials/zero/">ZeRO Optimizer</a>
- <a href="https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html">Model Parallelism</a></p>
<p><strong>Profiling and Debugging:</strong>
- <a href="https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html">PyTorch Profiler</a>
- <a href="https://developer.nvidia.com/nsight-systems">NVIDIA Nsight</a>
- <a href="https://wandb.ai/">Weights &amp; Biases</a></p>
<hr />
<h2 id="conclusion">Conclusion</h2>
<p>This comprehensive guide covers the major architectural innovations in Transformer models, from efficiency improvements to training optimizations. The field continues to evolve rapidly, with new techniques emerging regularly. When implementing these techniques:</p>
<ol>
<li><strong>Start with proven methods</strong>: FlashAttention, RMSNorm, and pre-norm are safe choices</li>
<li><strong>Profile your specific use case</strong>: Different techniques excel in different scenarios</li>
<li><strong>Consider the trade-offs</strong>: Efficiency gains often come with implementation complexity</li>
<li><strong>Stay updated</strong>: The field moves quickly, and new optimizations appear frequently</li>
</ol>
<p>For production systems, prioritize techniques with strong empirical validation and robust implementations. For research, explore the cutting-edge methods that push the boundaries of what's possible with Transformer architectures.</p>
<p>The future of Transformer architectures lies in finding the optimal balance between computational efficiency, model quality, and implementation simplicity. As hardware continues to evolve and new mathematical insights emerge, we can expect even more innovative approaches to sequence modeling and attention mechanisms.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>