
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
        <link rel="next" href="../self-supervised/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>Deep Learning Foundations - Multimodal Memory LLM and AI Agent</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4af4bdda.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-learning-from-perceptrons-to-modern-architectures" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-header__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Multimodal Memory LLM and AI Agent
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep Learning Foundations
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-nav__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Multimodal Memory LLM and AI Agent
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core Modules
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Core Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Deep Learning Foundations
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Deep Learning Foundations
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What is Deep Learning?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#history-of-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      History of Neural Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="History of Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-perceptron-era-1940s-1960s" class="md-nav__link">
    <span class="md-ellipsis">
      The Perceptron Era (1940s-1960s)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Perceptron Era (1940s-1960s)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mcculloch-pitts-neuron-1943" class="md-nav__link">
    <span class="md-ellipsis">
      McCulloch-Pitts Neuron (1943)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rosenblatts-perceptron-1957" class="md-nav__link">
    <span class="md-ellipsis">
      Rosenblatt's Perceptron (1957)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-first-ai-winter-1969-1980s" class="md-nav__link">
    <span class="md-ellipsis">
      The First AI Winter (1969-1980s)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-multi-layer-perceptron-renaissance-1980s" class="md-nav__link">
    <span class="md-ellipsis">
      The Multi-Layer Perceptron Renaissance (1980s)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Multi-Layer Perceptron Renaissance (1980s)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#backpropagation-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Backpropagation Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-second-ai-winter-1990s" class="md-nav__link">
    <span class="md-ellipsis">
      The Second AI Winter (1990s)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-deep-learning-revolution" class="md-nav__link">
    <span class="md-ellipsis">
      The Deep Learning Revolution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Deep Learning Revolution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-perfect-storm-2000s-2010s" class="md-nav__link">
    <span class="md-ellipsis">
      The Perfect Storm (2000s-2010s)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#imagenet-and-the-visual-recognition-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet and the Visual Recognition Challenge
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alexnet-the-breakthrough-2012" class="md-nav__link">
    <span class="md-ellipsis">
      AlexNet: The Breakthrough (2012)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlexNet: The Breakthrough (2012)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-details" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Key Innovations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Neural Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mathematical Foundation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convolution-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Convolution Operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-channel-convolution" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Channel Convolution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#output-size-calculation" class="md-nav__link">
    <span class="md-ellipsis">
      Output Size Calculation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pooling-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Pooling Operations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pooling Operations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#max-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Max Pooling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#average-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Average Pooling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-average-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Global Average Pooling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Activation Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relu-and-variants" class="md-nav__link">
    <span class="md-ellipsis">
      ReLU and Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#major-cnn-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Major CNN Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Major CNN Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vggnet-depth-matters-2014" class="md-nav__link">
    <span class="md-ellipsis">
      VGGNet: Depth Matters (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VGGNet: Depth Matters (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-innovations_1" class="md-nav__link">
    <span class="md-ellipsis">
      Key Innovations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-33-filters" class="md-nav__link">
    <span class="md-ellipsis">
      Why 3Ã—3 Filters?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vgg-16-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      VGG-16 Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet-the-residual-revolution-2015" class="md-nav__link">
    <span class="md-ellipsis">
      ResNet: The Residual Revolution (2015)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ResNet: The Residual Revolution (2015)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-degradation-problem" class="md-nav__link">
    <span class="md-ellipsis">
      The Degradation Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-block" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Block
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-flow-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Flow Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet-50-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      ResNet-50 Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#impact-and-variants" class="md-nav__link">
    <span class="md-ellipsis">
      Impact and Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#googlenetinception-efficient-architecture-design-2014" class="md-nav__link">
    <span class="md-ellipsis">
      GoogLeNet/Inception: Efficient Architecture Design (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GoogLeNet/Inception: Efficient Architecture Design (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inception-module" class="md-nav__link">
    <span class="md-ellipsis">
      Inception Module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#auxiliary-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      Auxiliary Classifiers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent-variants" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent Variants
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Descent Variants">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stochastic-gradient-descent-sgd" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Gradient Descent (SGD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-learning-rate-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Adaptive Learning Rate Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Scheduling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning Rate Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Step Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Exponential Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-annealing" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Annealing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#warm-up-and-restart" class="md-nav__link">
    <span class="md-ellipsis">
      Warm-up and Restart
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Weight Initialization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Weight Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#xavierglorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Xavier/Glorot Initialization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#he-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      He Initialization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regularization Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1-and-l2-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      L1 and L2 Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="L1 and L2 Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l2-regularization-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      L2 Regularization (Weight Decay)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1-regularization-lasso" class="md-nav__link">
    <span class="md-ellipsis">
      L1 Regularization (Lasso)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Dropout
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inverted-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Inverted Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropconnect" class="md-nav__link">
    <span class="md-ellipsis">
      DropConnect
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Normalization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch Normalization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variants" class="md-nav__link">
    <span class="md-ellipsis">
      Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-augmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Data Augmentation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data Augmentation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#traditional-augmentations" class="md-nav__link">
    <span class="md-ellipsis">
      Traditional Augmentations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-augmentations" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Augmentations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-training-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Training Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Training Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Transfer Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transfer Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning Strategies
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-gpu-training" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-GPU Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Data Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-training" class="md-nav__link">
    <span class="md-ellipsis">
      Distributed Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#modern-architectures-and-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Architectures and Trends
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Modern Architectures and Trends">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vision-transformers-vits" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformers (ViTs)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision Transformers (ViTs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation_2" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficientnet-scaling-cnns-efficiently-2019" class="md-nav__link">
    <span class="md-ellipsis">
      EfficientNet: Scaling CNNs Efficiently (2019)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EfficientNet: Scaling CNNs Efficiently (2019)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compound-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Compound Scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mobile-inverted-bottleneck-mbconv" class="md-nav__link">
    <span class="md-ellipsis">
      Mobile Inverted Bottleneck (MBConv)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeeze-and-excitation-se" class="md-nav__link">
    <span class="md-ellipsis">
      Squeeze-and-Excitation (SE)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-architecture-search-nas" class="md-nav__link">
    <span class="md-ellipsis">
      Neural Architecture Search (NAS)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Architecture Search (NAS)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automl-and-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      AutoML and Architecture Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#darts-differentiable-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      DARTS (Differentiable Architecture Search)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#semi-supervised-and-self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised and Self-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised and Self-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#semi-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      Problem Formulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consistency-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Consistency Regularization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pseudo-labeling" class="md-nav__link">
    <span class="md-ellipsis">
      Pseudo-Labeling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked-languageimage-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      Masked Language/Image Modeling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Guide
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Guide">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setting-up-a-deep-learning-project" class="md-nav__link">
    <span class="md-ellipsis">
      Setting Up a Deep Learning Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Setting Up a Deep Learning Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Project Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-management" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-loop-template" class="md-nav__link">
    <span class="md-ellipsis">
      Training Loop Template
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Debugging and Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Debugging and Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-issues-and-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issues and Solutions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-and-resources" class="md-nav__link">
    <span class="md-ellipsis">
      References and Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="References and Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#foundational-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Foundational Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Foundational Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-foundations" class="md-nav__link">
    <span class="md-ellipsis">
      Historical Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modern-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Deep Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization-and-training" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization and Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning_1" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#frameworks-and-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Frameworks and Libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tools-and-utilities" class="md-nav__link">
    <span class="md-ellipsis">
      Tools and Utilities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#books-and-courses" class="md-nav__link">
    <span class="md-ellipsis">
      Books and Courses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Books and Courses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#books" class="md-nav__link">
    <span class="md-ellipsis">
      Books
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#online-courses" class="md-nav__link">
    <span class="md-ellipsis">
      Online Courses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Takeaways">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-perspective" class="md-nav__link">
    <span class="md-ellipsis">
      Historical Perspective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architectural-principles" class="md-nav__link">
    <span class="md-ellipsis">
      Architectural Principles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Training Best Practices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modern-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Trends
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#future-directions" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../self-supervised/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../embeddings/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multimodal Embeddings
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Frameworks and Architectures
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../memory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tool Calling and Agent Capabilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi_modal_LM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Modal Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Advanced Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_advanced/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced Transformer Techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_architecture_evolution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT Architecture Evolution
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notebooks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Notebooks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../notebooks/memory_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Example
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      Table of Contents
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-is-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      What is Deep Learning?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#history-of-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      History of Neural Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="History of Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-perceptron-era-1940s-1960s" class="md-nav__link">
    <span class="md-ellipsis">
      The Perceptron Era (1940s-1960s)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Perceptron Era (1940s-1960s)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mcculloch-pitts-neuron-1943" class="md-nav__link">
    <span class="md-ellipsis">
      McCulloch-Pitts Neuron (1943)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rosenblatts-perceptron-1957" class="md-nav__link">
    <span class="md-ellipsis">
      Rosenblatt's Perceptron (1957)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-first-ai-winter-1969-1980s" class="md-nav__link">
    <span class="md-ellipsis">
      The First AI Winter (1969-1980s)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-multi-layer-perceptron-renaissance-1980s" class="md-nav__link">
    <span class="md-ellipsis">
      The Multi-Layer Perceptron Renaissance (1980s)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Multi-Layer Perceptron Renaissance (1980s)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#backpropagation-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Backpropagation Algorithm
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-second-ai-winter-1990s" class="md-nav__link">
    <span class="md-ellipsis">
      The Second AI Winter (1990s)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-deep-learning-revolution" class="md-nav__link">
    <span class="md-ellipsis">
      The Deep Learning Revolution
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Deep Learning Revolution">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-perfect-storm-2000s-2010s" class="md-nav__link">
    <span class="md-ellipsis">
      The Perfect Storm (2000s-2010s)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#imagenet-and-the-visual-recognition-challenge" class="md-nav__link">
    <span class="md-ellipsis">
      ImageNet and the Visual Recognition Challenge
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#alexnet-the-breakthrough-2012" class="md-nav__link">
    <span class="md-ellipsis">
      AlexNet: The Breakthrough (2012)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AlexNet: The Breakthrough (2012)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-details" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      Key Innovations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-neural-networks" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Neural Networks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Mathematical Foundation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convolution-operation" class="md-nav__link">
    <span class="md-ellipsis">
      Convolution Operation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-channel-convolution" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Channel Convolution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#output-size-calculation" class="md-nav__link">
    <span class="md-ellipsis">
      Output Size Calculation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pooling-operations" class="md-nav__link">
    <span class="md-ellipsis">
      Pooling Operations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pooling Operations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#max-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Max Pooling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#average-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Average Pooling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#global-average-pooling" class="md-nav__link">
    <span class="md-ellipsis">
      Global Average Pooling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#activation-functions" class="md-nav__link">
    <span class="md-ellipsis">
      Activation Functions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Activation Functions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#relu-and-variants" class="md-nav__link">
    <span class="md-ellipsis">
      ReLU and Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#major-cnn-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Major CNN Architectures
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Major CNN Architectures">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vggnet-depth-matters-2014" class="md-nav__link">
    <span class="md-ellipsis">
      VGGNet: Depth Matters (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="VGGNet: Depth Matters (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#key-innovations_1" class="md-nav__link">
    <span class="md-ellipsis">
      Key Innovations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-33-filters" class="md-nav__link">
    <span class="md-ellipsis">
      Why 3Ã—3 Filters?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vgg-16-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      VGG-16 Architecture
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet-the-residual-revolution-2015" class="md-nav__link">
    <span class="md-ellipsis">
      ResNet: The Residual Revolution (2015)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ResNet: The Residual Revolution (2015)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-degradation-problem" class="md-nav__link">
    <span class="md-ellipsis">
      The Degradation Problem
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#residual-block" class="md-nav__link">
    <span class="md-ellipsis">
      Residual Block
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation_1" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-flow-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Flow Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#resnet-50-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      ResNet-50 Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#impact-and-variants" class="md-nav__link">
    <span class="md-ellipsis">
      Impact and Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#googlenetinception-efficient-architecture-design-2014" class="md-nav__link">
    <span class="md-ellipsis">
      GoogLeNet/Inception: Efficient Architecture Design (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GoogLeNet/Inception: Efficient Architecture Design (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#inception-module" class="md-nav__link">
    <span class="md-ellipsis">
      Inception Module
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#auxiliary-classifiers" class="md-nav__link">
    <span class="md-ellipsis">
      Auxiliary Classifiers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Optimization Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-descent-variants" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Descent Variants
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Gradient Descent Variants">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#stochastic-gradient-descent-sgd" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Gradient Descent (SGD)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-learning-rate-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Adaptive Learning Rate Methods
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#learning-rate-scheduling" class="md-nav__link">
    <span class="md-ellipsis">
      Learning Rate Scheduling
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Learning Rate Scheduling">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Step Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-decay" class="md-nav__link">
    <span class="md-ellipsis">
      Exponential Decay
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cosine-annealing" class="md-nav__link">
    <span class="md-ellipsis">
      Cosine Annealing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#warm-up-and-restart" class="md-nav__link">
    <span class="md-ellipsis">
      Warm-up and Restart
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weight-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Weight Initialization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Weight Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#xavierglorot-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Xavier/Glorot Initialization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#he-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      He Initialization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regularization-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Regularization Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l1-and-l2-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      L1 and L2 Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="L1 and L2 Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#l2-regularization-weight-decay" class="md-nav__link">
    <span class="md-ellipsis">
      L2 Regularization (Weight Decay)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#l1-regularization-lasso" class="md-nav__link">
    <span class="md-ellipsis">
      L1 Regularization (Lasso)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#elastic-net" class="md-nav__link">
    <span class="md-ellipsis">
      Elastic Net
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Dropout
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Dropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#standard-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Standard Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inverted-dropout" class="md-nav__link">
    <span class="md-ellipsis">
      Inverted Dropout
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#dropconnect" class="md-nav__link">
    <span class="md-ellipsis">
      DropConnect
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization" class="md-nav__link">
    <span class="md-ellipsis">
      Batch Normalization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Batch Normalization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benefits" class="md-nav__link">
    <span class="md-ellipsis">
      Benefits
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#variants" class="md-nav__link">
    <span class="md-ellipsis">
      Variants
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-augmentation" class="md-nav__link">
    <span class="md-ellipsis">
      Data Augmentation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Data Augmentation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#traditional-augmentations" class="md-nav__link">
    <span class="md-ellipsis">
      Traditional Augmentations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-augmentations" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Augmentations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#advanced-training-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Training Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Advanced Training Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Transfer Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Transfer Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fine-tuning-strategies" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning Strategies
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-gpu-training" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-GPU Training
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multi-GPU Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallelism" class="md-nav__link">
    <span class="md-ellipsis">
      Data Parallelism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#distributed-training" class="md-nav__link">
    <span class="md-ellipsis">
      Distributed Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixed-precision-training" class="md-nav__link">
    <span class="md-ellipsis">
      Mixed Precision Training
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#modern-architectures-and-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Architectures and Trends
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Modern Architectures and Trends">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vision-transformers-vits" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformers (ViTs)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision Transformers (ViTs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematical-formulation_2" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Formulation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficientnet-scaling-cnns-efficiently-2019" class="md-nav__link">
    <span class="md-ellipsis">
      EfficientNet: Scaling CNNs Efficiently (2019)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="EfficientNet: Scaling CNNs Efficiently (2019)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#compound-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      Compound Scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mobile-inverted-bottleneck-mbconv" class="md-nav__link">
    <span class="md-ellipsis">
      Mobile Inverted Bottleneck (MBConv)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#squeeze-and-excitation-se" class="md-nav__link">
    <span class="md-ellipsis">
      Squeeze-and-Excitation (SE)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#neural-architecture-search-nas" class="md-nav__link">
    <span class="md-ellipsis">
      Neural Architecture Search (NAS)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Neural Architecture Search (NAS)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#automl-and-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      AutoML and Architecture Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#darts-differentiable-architecture-search" class="md-nav__link">
    <span class="md-ellipsis">
      DARTS (Differentiable Architecture Search)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#semi-supervised-and-self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised and Self-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised and Self-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#semi-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Semi-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Semi-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#problem-formulation" class="md-nav__link">
    <span class="md-ellipsis">
      Problem Formulation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#consistency-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Consistency Regularization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pseudo-labeling" class="md-nav__link">
    <span class="md-ellipsis">
      Pseudo-Labeling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Self-Supervised Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#contrastive-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Contrastive Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#masked-languageimage-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      Masked Language/Image Modeling
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#implementation-guide" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Guide
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Guide">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#setting-up-a-deep-learning-project" class="md-nav__link">
    <span class="md-ellipsis">
      Setting Up a Deep Learning Project
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Setting Up a Deep Learning Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#project-structure" class="md-nav__link">
    <span class="md-ellipsis">
      Project Structure
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#configuration-management" class="md-nav__link">
    <span class="md-ellipsis">
      Configuration Management
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-loop-template" class="md-nav__link">
    <span class="md-ellipsis">
      Training Loop Template
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#debugging-and-monitoring" class="md-nav__link">
    <span class="md-ellipsis">
      Debugging and Monitoring
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Debugging and Monitoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#common-issues-and-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      Common Issues and Solutions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references-and-resources" class="md-nav__link">
    <span class="md-ellipsis">
      References and Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="References and Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#foundational-papers" class="md-nav__link">
    <span class="md-ellipsis">
      Foundational Papers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Foundational Papers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-foundations" class="md-nav__link">
    <span class="md-ellipsis">
      Historical Foundations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modern-deep-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Deep Learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization-and-training" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization and Training
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning_1" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Supervised Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-resources" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Resources
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Implementation Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#frameworks-and-libraries" class="md-nav__link">
    <span class="md-ellipsis">
      Frameworks and Libraries
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datasets" class="md-nav__link">
    <span class="md-ellipsis">
      Datasets
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tools-and-utilities" class="md-nav__link">
    <span class="md-ellipsis">
      Tools and Utilities
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#books-and-courses" class="md-nav__link">
    <span class="md-ellipsis">
      Books and Courses
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Books and Courses">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#books" class="md-nav__link">
    <span class="md-ellipsis">
      Books
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#online-courses" class="md-nav__link">
    <span class="md-ellipsis">
      Online Courses
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      Key Takeaways
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Key Takeaways">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#historical-perspective" class="md-nav__link">
    <span class="md-ellipsis">
      Historical Perspective
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#architectural-principles" class="md-nav__link">
    <span class="md-ellipsis">
      Architectural Principles
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-best-practices" class="md-nav__link">
    <span class="md-ellipsis">
      Training Best Practices
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#modern-trends" class="md-nav__link">
    <span class="md-ellipsis">
      Modern Trends
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#future-directions" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="deep-learning-from-perceptrons-to-modern-architectures">Deep Learning: From Perceptrons to Modern Architectures</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#history-of-neural-networks">History of Neural Networks</a></li>
<li><a href="#the-deep-learning-revolution">The Deep Learning Revolution</a></li>
<li><a href="#convolutional-neural-networks">Convolutional Neural Networks</a></li>
<li><a href="#major-cnn-architectures">Major CNN Architectures</a></li>
<li><a href="#optimization-techniques">Optimization Techniques</a></li>
<li><a href="#regularization-methods">Regularization Methods</a></li>
<li><a href="#advanced-training-techniques">Advanced Training Techniques</a></li>
<li><a href="#modern-architectures-and-trends">Modern Architectures and Trends</a></li>
<li><a href="#semi-supervised-and-self-supervised-learning">Semi-Supervised and Self-Supervised Learning</a></li>
<li><a href="#implementation-guide">Implementation Guide</a></li>
<li><a href="#references-and-resources">References and Resources</a></li>
</ol>
<hr />
<h2 id="introduction">Introduction</h2>
<p>Deep Learning has revolutionized artificial intelligence, enabling breakthroughs in computer vision, natural language processing, speech recognition, and many other domains. This comprehensive tutorial explores the evolution of neural networks from simple perceptrons to sophisticated modern architectures.</p>
<h3 id="what-is-deep-learning">What is Deep Learning?</h3>
<p>Deep Learning is a subset of machine learning that uses artificial neural networks with multiple layers (hence "deep") to model and understand complex patterns in data. The key characteristics include:</p>
<ul>
<li><strong>Hierarchical Feature Learning</strong>: Automatic extraction of features at multiple levels of abstraction</li>
<li><strong>End-to-End Learning</strong>: Direct mapping from raw input to desired output</li>
<li><strong>Scalability</strong>: Performance improves with more data and computational resources</li>
<li><strong>Versatility</strong>: Applicable across diverse domains and tasks</li>
</ul>
<h3 id="mathematical-foundation">Mathematical Foundation</h3>
<p>At its core, deep learning involves learning a function <span class="arithmatex">\(f: \mathcal{X} \rightarrow \mathcal{Y}\)</span> that maps inputs <span class="arithmatex">\(x \in \mathcal{X}\)</span> to outputs <span class="arithmatex">\(y \in \mathcal{Y}\)</span>. This function is approximated by a composition of simpler functions:</p>
<div class="arithmatex">\[f(x) = f^{(L)}(f^{(L-1)}(...f^{(2)}(f^{(1)}(x))))\]</div>
<p>Where each <span class="arithmatex">\(f^{(i)}\)</span> represents a layer in the network, and <span class="arithmatex">\(L\)</span> is the total number of layers.</p>
<hr />
<h2 id="history-of-neural-networks">History of Neural Networks</h2>
<h3 id="the-perceptron-era-1940s-1960s">The Perceptron Era (1940s-1960s)</h3>
<h4 id="mcculloch-pitts-neuron-1943">McCulloch-Pitts Neuron (1943)</h4>
<p><strong>Paper</strong>: <a href="https://link.springer.com/article/10.1007/BF02478259">A Logical Calculus of Ideas Immanent in Nervous Activity</a></p>
<p>The first mathematical model of a neuron, proposed by Warren McCulloch and Walter Pitts:</p>
<div class="arithmatex">\[y = \begin{cases}
1 &amp; \text{if } \sum_{i=1}^n w_i x_i \geq \theta \\
0 &amp; \text{otherwise}
\end{cases}\]</div>
<p>Where:
- <span class="arithmatex">\(x_i\)</span> are binary inputs
- <span class="arithmatex">\(w_i\)</span> are weights
- <span class="arithmatex">\(\theta\)</span> is the threshold
- <span class="arithmatex">\(y\)</span> is the binary output</p>
<h4 id="rosenblatts-perceptron-1957">Rosenblatt's Perceptron (1957)</h4>
<p><strong>Paper</strong>: <a href="https://psycnet.apa.org/record/1959-09865-001">The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain</a></p>
<p>Frank Rosenblatt introduced the first learning algorithm for neural networks:</p>
<div class="arithmatex">\[w_{i}^{(t+1)} = w_{i}^{(t)} + \eta (y - \hat{y}) x_i\]</div>
<p>Where:
- <span class="arithmatex">\(\eta\)</span> is the learning rate
- <span class="arithmatex">\(y\)</span> is the true label
- <span class="arithmatex">\(\hat{y}\)</span> is the predicted output
- <span class="arithmatex">\(t\)</span> denotes the time step</p>
<p><strong>Perceptron Learning Algorithm</strong>:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">perceptron_update</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update perceptron weights using the perceptron learning rule</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">weights</span><span class="p">)):</span>
        <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">error</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">weights</span>
</code></pre></div></p>
<h4 id="the-first-ai-winter-1969-1980s">The First AI Winter (1969-1980s)</h4>
<p><strong>Minsky and Papert's Critique</strong>: <a href="https://mitpress.mit.edu/9780262630221/perceptrons/">Perceptrons: An Introduction to Computational Geometry</a></p>
<p>In 1969, Marvin Minsky and Seymour Papert proved that single-layer perceptrons cannot solve linearly non-separable problems like XOR:</p>
<p><strong>XOR Problem</strong>:
| <span class="arithmatex">\(x_1\)</span> | <span class="arithmatex">\(x_2\)</span> | XOR |
|-------|-------|-----|
| 0     | 0     | 0   |
| 0     | 1     | 1   |
| 1     | 0     | 1   |
| 1     | 1     | 0   |</p>
<p>No single line can separate the positive and negative examples, highlighting the limitations of linear classifiers.</p>
<h3 id="the-multi-layer-perceptron-renaissance-1980s">The Multi-Layer Perceptron Renaissance (1980s)</h3>
<h4 id="backpropagation-algorithm">Backpropagation Algorithm</h4>
<p><strong>Papers</strong>: 
- <a href="https://www.nature.com/articles/323533a0">Learning Representations by Back-Propagating Errors</a> (Rumelhart, Hinton, Williams, 1986)
- <a href="https://web.stanford.edu/class/psych209a/ReadingsByDate/02_06/PDPVolIChapter8.pdf">Learning Internal Representations by Error Propagation</a> (Rumelhart &amp; McClelland, 1986)</p>
<p>The breakthrough that enabled training multi-layer networks by efficiently computing gradients:</p>
<p><strong>Forward Pass</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(a^{(l)} = \sigma(z^{(l)})\)</span>\)</span></p>
<p><strong>Backward Pass</strong> (Chain Rule):
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial W^{(l)}} = \frac{\partial \mathcal{L}}{\partial z^{(l)}} \frac{\partial z^{(l)}}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T\)</span>\)</span></p>
<div class="arithmatex">\[\delta^{(l)} = \frac{\partial \mathcal{L}}{\partial z^{(l)}} = (W^{(l+1)})^T \delta^{(l+1)} \odot \sigma'(z^{(l)})\]</div>
<p>Where:
- <span class="arithmatex">\(\mathcal{L}\)</span> is the loss function
- <span class="arithmatex">\(\sigma\)</span> is the activation function
- <span class="arithmatex">\(\odot\)</span> denotes element-wise multiplication
- <span class="arithmatex">\(\delta^{(l)}\)</span> is the error term for layer <span class="arithmatex">\(l\)</span></p>
<p><strong>Implementation Example</strong>:
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
        <span class="c1"># Initialize weights randomly</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">output_size</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a1</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">a2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">z2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">a2</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Backward propagation</span>
        <span class="n">dz2</span> <span class="o">=</span> <span class="n">output</span> <span class="o">-</span> <span class="n">y</span>
        <span class="n">dW2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz2</span><span class="p">)</span>
        <span class="n">db2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">da1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dz2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">dz1</span> <span class="o">=</span> <span class="n">da1</span> <span class="o">*</span> <span class="n">sigmoid_derivative</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">dW1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dz1</span><span class="p">)</span>
        <span class="n">db1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dz1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dW1</span><span class="p">,</span> <span class="n">db1</span><span class="p">,</span> <span class="n">dW2</span><span class="p">,</span> <span class="n">db2</span>
</code></pre></div></p>
<h3 id="the-second-ai-winter-1990s">The Second AI Winter (1990s)</h3>
<p>Despite the theoretical breakthrough of backpropagation, practical limitations emerged:</p>
<ol>
<li><strong>Vanishing Gradient Problem</strong>: Gradients become exponentially small in deep networks</li>
<li><strong>Limited Computational Resources</strong>: Training deep networks was computationally prohibitive</li>
<li><strong>Lack of Data</strong>: Insufficient large-scale datasets</li>
<li><strong>Competition from SVMs</strong>: Support Vector Machines often outperformed neural networks</li>
</ol>
<hr />
<h2 id="the-deep-learning-revolution">The Deep Learning Revolution</h2>
<h3 id="the-perfect-storm-2000s-2010s">The Perfect Storm (2000s-2010s)</h3>
<p>Several factors converged to enable the deep learning revolution:</p>
<ol>
<li><strong>Big Data</strong>: Internet-scale datasets became available</li>
<li><strong>GPU Computing</strong>: Parallel processing power for matrix operations</li>
<li><strong>Algorithmic Innovations</strong>: Better initialization, activation functions, and optimization</li>
<li><strong>Open Source Frameworks</strong>: TensorFlow, PyTorch, etc.</li>
</ol>
<h3 id="imagenet-and-the-visual-recognition-challenge">ImageNet and the Visual Recognition Challenge</h3>
<p><strong>Dataset</strong>: <a href="http://www.image-net.org/challenges/LSVRC/">ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</a></p>
<p><strong>Paper</strong>: <a href="https://ieeexplore.ieee.org/document/5206848">ImageNet: A Large-Scale Hierarchical Image Database</a></p>
<p>ImageNet became the benchmark that catalyzed the deep learning revolution:</p>
<ul>
<li><strong>Scale</strong>: 14+ million images, 20,000+ categories</li>
<li><strong>Challenge</strong>: Annual competition from 2010-2017</li>
<li><strong>Impact</strong>: Drove innovation in computer vision architectures</li>
</ul>
<p><strong>ILSVRC Results Timeline</strong>:
| Year | Winner | Top-5 Error | Architecture |
|------|--------|-------------|-------------|
| 2010 | NEC | 28.2% | Traditional CV |
| 2011 | XRCE | 25.8% | Traditional CV |
| 2012 | <strong>AlexNet</strong> | <strong>16.4%</strong> | <strong>CNN</strong> |
| 2013 | Clarifai | 11.7% | CNN |
| 2014 | GoogLeNet | 6.7% | Inception |
| 2015 | <strong>ResNet</strong> | <strong>3.6%</strong> | <strong>Residual</strong> |
| 2016 | Trimps-Soushen | 2.99% | Ensemble |
| 2017 | SENet | 2.25% | Attention |</p>
<h3 id="alexnet-the-breakthrough-2012">AlexNet: The Breakthrough (2012)</h3>
<p><strong>Paper</strong>: <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet Classification with Deep Convolutional Neural Networks</a></p>
<p><strong>Authors</strong>: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton</p>
<p><strong>Code</strong>: <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/alexnet.py">AlexNet Implementation</a></p>
<p>AlexNet's revolutionary impact came from several key innovations:</p>
<h4 id="architecture-details">Architecture Details</h4>
<div class="highlight"><pre><span></span><code>Input: 224Ã—224Ã—3 RGB image

Conv1: 96 filters, 11Ã—11, stride 4, ReLU â†’ 55Ã—55Ã—96
MaxPool1: 3Ã—3, stride 2 â†’ 27Ã—27Ã—96

Conv2: 256 filters, 5Ã—5, stride 1, ReLU â†’ 27Ã—27Ã—256
MaxPool2: 3Ã—3, stride 2 â†’ 13Ã—13Ã—256

Conv3: 384 filters, 3Ã—3, stride 1, ReLU â†’ 13Ã—13Ã—384
Conv4: 384 filters, 3Ã—3, stride 1, ReLU â†’ 13Ã—13Ã—384
Conv5: 256 filters, 3Ã—3, stride 1, ReLU â†’ 13Ã—13Ã—256
MaxPool3: 3Ã—3, stride 2 â†’ 6Ã—6Ã—256

FC1: 4096 neurons, ReLU, Dropout(0.5)
FC2: 4096 neurons, ReLU, Dropout(0.5)
FC3: 1000 neurons (classes), Softmax
</code></pre></div>
<h4 id="key-innovations">Key Innovations</h4>
<p><strong>1. ReLU Activation Function</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(\text{ReLU}(x) = \max(0, x)\)</span>\)</span></p>
<p>Advantages over sigmoid/tanh:
- <strong>No saturation</strong> for positive values
- <strong>Sparse activation</strong> (many neurons output 0)
- <strong>Computational efficiency</strong> (simple thresholding)
- <strong>Better gradient flow</strong> (derivative is 1 for positive inputs)</p>
<p><strong>2. Dropout Regularization</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(y = \text{dropout}(x, p) = \begin{cases}
\frac{x}{1-p} &amp; \text{with probability } 1-p \\
0 &amp; \text{with probability } p
\end{cases}\)</span>\)</span></p>
<p>During training, randomly set neurons to 0 with probability <span class="arithmatex">\(p\)</span>, preventing co-adaptation.</p>
<p><strong>3. Data Augmentation</strong>:
- Random crops and horizontal flips
- Color jittering (PCA on RGB values)
- Increased effective dataset size by 2048Ã—</p>
<p><strong>4. GPU Implementation</strong>:
- Utilized two GTX 580 GPUs
- Parallelized convolutions across GPUs
- 5-6 days training time vs. weeks on CPU</p>
<h4 id="mathematical-formulation">Mathematical Formulation</h4>
<p>For a convolutional layer with input <span class="arithmatex">\(X \in \mathbb{R}^{H \times W \times C}\)</span> and filter <span class="arithmatex">\(W \in \mathbb{R}^{K \times K \times C \times F}\)</span>:</p>
<div class="arithmatex">\[Y_{i,j,f} = \sum_{c=1}^{C} \sum_{u=1}^{K} \sum_{v=1}^{K} X_{i \cdot s + u, j \cdot s + v, c} \cdot W_{u,v,c,f} + b_f\]</div>
<p>Where:
- <span class="arithmatex">\(s\)</span> is the stride
- <span class="arithmatex">\(b_f\)</span> is the bias for filter <span class="arithmatex">\(f\)</span>
- <span class="arithmatex">\((i,j)\)</span> are output spatial coordinates</p>
<p><strong>PyTorch Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">AlexNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AlexNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># Conv1</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">96</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Conv2</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">96</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Conv3</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>

            <span class="c1"># Conv4</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">384</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>

            <span class="c1"># Conv5</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">384</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span> <span class="o">*</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Flatten</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></p>
<hr />
<h2 id="convolutional-neural-networks">Convolutional Neural Networks</h2>
<h3 id="mathematical-foundation_1">Mathematical Foundation</h3>
<p>Convolutional Neural Networks (CNNs) are specifically designed for processing grid-like data such as images. They leverage three key principles:</p>
<ol>
<li><strong>Local Connectivity</strong>: Neurons connect only to local regions</li>
<li><strong>Parameter Sharing</strong>: Same weights used across spatial locations</li>
<li><strong>Translation Invariance</strong>: Features detected regardless of position</li>
</ol>
<h4 id="convolution-operation">Convolution Operation</h4>
<p>The discrete convolution operation in 2D:</p>
<div class="arithmatex">\[(I * K)_{i,j} = \sum_{m} \sum_{n} I_{i-m,j-n} K_{m,n}\]</div>
<p>In practice, we use cross-correlation (which is often called convolution in deep learning):</p>
<div class="arithmatex">\[(I * K)_{i,j} = \sum_{m} \sum_{n} I_{i+m,j+n} K_{m,n}\]</div>
<h4 id="multi-channel-convolution">Multi-Channel Convolution</h4>
<p>For input with <span class="arithmatex">\(C\)</span> channels and <span class="arithmatex">\(F\)</span> filters:</p>
<div class="arithmatex">\[Y_{i,j,f} = \sum_{c=1}^{C} \sum_{u=0}^{K-1} \sum_{v=0}^{K-1} X_{i+u,j+v,c} \cdot W_{u,v,c,f} + b_f\]</div>
<h4 id="output-size-calculation">Output Size Calculation</h4>
<p>Given input size <span class="arithmatex">\((H, W)\)</span>, kernel size <span class="arithmatex">\(K\)</span>, padding <span class="arithmatex">\(P\)</span>, and stride <span class="arithmatex">\(S\)</span>:</p>
<div class="arithmatex">\[H_{out} = \left\lfloor \frac{H + 2P - K}{S} \right\rfloor + 1$$
$$W_{out} = \left\lfloor \frac{W + 2P - K}{S} \right\rfloor + 1\]</div>
<h3 id="pooling-operations">Pooling Operations</h3>
<h4 id="max-pooling">Max Pooling</h4>
<div class="arithmatex">\[\text{MaxPool}(X)_{i,j} = \max_{u,v \in \text{pool region}} X_{i \cdot s + u, j \cdot s + v}\]</div>
<h4 id="average-pooling">Average Pooling</h4>
<div class="arithmatex">\[\text{AvgPool}(X)_{i,j} = \frac{1}{K^2} \sum_{u,v \in \text{pool region}} X_{i \cdot s + u, j \cdot s + v}\]</div>
<h4 id="global-average-pooling">Global Average Pooling</h4>
<div class="arithmatex">\[\text{GAP}(X)_c = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_{i,j,c}\]</div>
<h3 id="activation-functions">Activation Functions</h3>
<h4 id="relu-and-variants">ReLU and Variants</h4>
<p><strong>ReLU</strong>: <span class="arithmatex">\(f(x) = \max(0, x)\)</span></p>
<p><strong>Leaky ReLU</strong>: <span class="arithmatex">\(f(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \\ \alpha x &amp; \text{if } x \leq 0 \end{cases}\)</span></p>
<p><strong>ELU</strong>: <span class="arithmatex">\(f(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \\ \alpha(e^x - 1) &amp; \text{if } x \leq 0 \end{cases}\)</span></p>
<p><strong>Swish</strong>: <span class="arithmatex">\(f(x) = x \cdot \sigma(\beta x)\)</span> where <span class="arithmatex">\(\sigma\)</span> is sigmoid</p>
<p><strong>GELU</strong>: <span class="arithmatex">\(f(x) = x \cdot \Phi(x)\)</span> where <span class="arithmatex">\(\Phi\)</span> is the CDF of standard normal distribution</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>

<span class="k">class</span><span class="w"> </span><span class="nc">ActivationFunctions</span><span class="p">:</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">swish</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="major-cnn-architectures">Major CNN Architectures</h2>
<h3 id="vggnet-depth-matters-2014">VGGNet: Depth Matters (2014)</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a></p>
<p><strong>Authors</strong>: Karen Simonyan, Andrew Zisserman (Oxford)</p>
<p><strong>Code</strong>: <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/vgg.py">VGG Implementation</a></p>
<h4 id="key-innovations_1">Key Innovations</h4>
<ol>
<li><strong>Uniform Architecture</strong>: Only 3Ã—3 convolutions and 2Ã—2 max pooling</li>
<li><strong>Increased Depth</strong>: Up to 19 layers (VGG-19)</li>
<li><strong>Small Filters</strong>: 3Ã—3 filters throughout the network</li>
</ol>
<h4 id="why-33-filters">Why 3Ã—3 Filters?</h4>
<p>Two 3Ã—3 convolutions have the same receptive field as one 5Ã—5 convolution but with:
- <strong>Fewer parameters</strong>: <span class="arithmatex">\(2 \times (3^2 \times C^2) = 18C^2\)</span> vs. <span class="arithmatex">\(5^2 \times C^2 = 25C^2\)</span>
- <strong>More non-linearity</strong>: Two ReLU activations instead of one
- <strong>Better feature learning</strong>: More complex decision boundaries</p>
<h4 id="vgg-16-architecture">VGG-16 Architecture</h4>
<div class="highlight"><pre><span></span><code>Input: 224Ã—224Ã—3

Block 1:
Conv3-64, Conv3-64, MaxPool â†’ 112Ã—112Ã—64

Block 2:
Conv3-128, Conv3-128, MaxPool â†’ 56Ã—56Ã—128

Block 3:
Conv3-256, Conv3-256, Conv3-256, MaxPool â†’ 28Ã—28Ã—256

Block 4:
Conv3-512, Conv3-512, Conv3-512, MaxPool â†’ 14Ã—14Ã—512

Block 5:
Conv3-512, Conv3-512, Conv3-512, MaxPool â†’ 7Ã—7Ã—512

Classifier:
FC-4096, FC-4096, FC-1000
</code></pre></div>
<p><strong>PyTorch Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">VGG16</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VGG16</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">features</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="c1"># Block 1</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Block 2</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Block 3</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Block 4</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>

            <span class="c1"># Block 5</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">512</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">*</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">4096</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">features</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></p>
<h3 id="resnet-the-residual-revolution-2015">ResNet: The Residual Revolution (2015)</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a></p>
<p><strong>Authors</strong>: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (Microsoft Research)</p>
<p><strong>Code</strong>: <a href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py">ResNet Implementation</a></p>
<h4 id="the-degradation-problem">The Degradation Problem</h4>
<p>As networks get deeper, accuracy saturates and then degrades rapidly. This is <strong>not</strong> due to overfitting but rather optimization difficulty.</p>
<p><strong>Observation</strong>: A deeper network should perform at least as well as its shallower counterpart by learning identity mappings in the extra layers.</p>
<h4 id="residual-learning">Residual Learning</h4>
<p>Instead of learning the desired mapping <span class="arithmatex">\(\mathcal{H}(x)\)</span>, learn the residual:</p>
<div class="arithmatex">\[\mathcal{F}(x) = \mathcal{H}(x) - x\]</div>
<p>Then the original mapping becomes:</p>
<div class="arithmatex">\[\mathcal{H}(x) = \mathcal{F}(x) + x\]</div>
<p><strong>Hypothesis</strong>: It's easier to optimize <span class="arithmatex">\(\mathcal{F}(x) = 0\)</span> (identity) than to learn <span class="arithmatex">\(\mathcal{H}(x) = x\)</span> directly.</p>
<h4 id="residual-block">Residual Block</h4>
<p><strong>Basic Block</strong> (for ResNet-18, ResNet-34):
<div class="highlight"><pre><span></span><code>x â†’ Conv3Ã—3 â†’ BN â†’ ReLU â†’ Conv3Ã—3 â†’ BN â†’ (+) â†’ ReLU
â†“                                           â†‘
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ identity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></p>
<p><strong>Bottleneck Block</strong> (for ResNet-50, ResNet-101, ResNet-152):
<div class="highlight"><pre><span></span><code>x â†’ Conv1Ã—1 â†’ BN â†’ ReLU â†’ Conv3Ã—3 â†’ BN â†’ ReLU â†’ Conv1Ã—1 â†’ BN â†’ (+) â†’ ReLU
â†“                                                                â†‘
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ identity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></p>
<h4 id="mathematical-formulation_1">Mathematical Formulation</h4>
<p>For a residual block:
<span class="arithmatex">\(<span class="arithmatex">\(y_l = h(x_l) + \mathcal{F}(x_l, W_l)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(x_{l+1} = f(y_l)\)</span>\)</span></p>
<p>Where:
- <span class="arithmatex">\(x_l\)</span> is input to the <span class="arithmatex">\(l\)</span>-th block
- <span class="arithmatex">\(\mathcal{F}\)</span> is the residual function
- <span class="arithmatex">\(h(x_l) = x_l\)</span> is identity mapping
- <span class="arithmatex">\(f\)</span> is ReLU activation</p>
<p>For the entire network:
<span class="arithmatex">\(<span class="arithmatex">\(x_L = x_l + \sum_{i=l}^{L-1} \mathcal{F}(x_i, W_i)\)</span>\)</span></p>
<h4 id="gradient-flow-analysis">Gradient Flow Analysis</h4>
<p>The gradient of the loss with respect to <span class="arithmatex">\(x_l\)</span>:</p>
<div class="arithmatex">\[\frac{\partial \mathcal{L}}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_L} \frac{\partial x_L}{\partial x_l} = \frac{\partial \mathcal{L}}{\partial x_L} \left(1 + \frac{\partial}{\partial x_l} \sum_{i=l}^{L-1} \mathcal{F}(x_i, W_i)\right)\]</div>
<p>The key insight: The gradient has two terms:
1. <span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial x_L}\)</span> - direct path (never vanishes)
2. <span class="arithmatex">\(\frac{\partial \mathcal{L}}{\partial x_L} \frac{\partial}{\partial x_l} \sum_{i=l}^{L-1} \mathcal{F}(x_i, W_i)\)</span> - residual path</p>
<p>This ensures that gradients can flow directly to earlier layers.</p>
<h4 id="resnet-50-architecture">ResNet-50 Architecture</h4>
<div class="highlight"><pre><span></span><code>Input: 224Ã—224Ã—3

Conv1: 7Ã—7, 64, stride 2 â†’ 112Ã—112Ã—64
MaxPool: 3Ã—3, stride 2 â†’ 56Ã—56Ã—64

Conv2_x: [1Ã—1,64; 3Ã—3,64; 1Ã—1,256] Ã— 3 â†’ 56Ã—56Ã—256
Conv3_x: [1Ã—1,128; 3Ã—3,128; 1Ã—1,512] Ã— 4 â†’ 28Ã—28Ã—512
Conv4_x: [1Ã—1,256; 3Ã—3,256; 1Ã—1,1024] Ã— 6 â†’ 14Ã—14Ã—1024
Conv5_x: [1Ã—1,512; 3Ã—3,512; 1Ã—1,2048] Ã— 3 â†’ 7Ã—7Ã—2048

GlobalAvgPool â†’ 1Ã—1Ã—2048
FC: 1000
</code></pre></div>
<p><strong>PyTorch Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">BasicBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BasicBlock</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>  <span class="c1"># Residual connection</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Bottleneck</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">expansion</span> <span class="o">=</span> <span class="mi">4</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Bottleneck</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">inplanes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">planes</span><span class="p">,</span> <span class="n">planes</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">planes</span> <span class="o">*</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="o">=</span> <span class="n">downsample</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">identity</span> <span class="o">=</span> <span class="n">x</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn3</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">identity</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">downsample</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">out</span> <span class="o">+=</span> <span class="n">identity</span>  <span class="c1"># Residual connection</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></p>
<h4 id="impact-and-variants">Impact and Variants</h4>
<p><strong>ResNet Variants</strong>:
- <strong>ResNeXt</strong>: <a href="https://arxiv.org/abs/1611.05431">Aggregated Residual Transformations for Deep Neural Networks</a>
- <strong>Wide ResNet</strong>: <a href="https://arxiv.org/abs/1605.07146">Wide Residual Networks</a>
- <strong>DenseNet</strong>: <a href="https://arxiv.org/abs/1608.06993">Densely Connected Convolutional Networks</a>
- <strong>ResNeSt</strong>: <a href="https://arxiv.org/abs/2004.08955">ResNeSt: Split-Attention Networks</a></p>
<h3 id="googlenetinception-efficient-architecture-design-2014">GoogLeNet/Inception: Efficient Architecture Design (2014)</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1409.4842">Going Deeper with Convolutions</a></p>
<p><strong>Authors</strong>: Christian Szegedy et al. (Google)</p>
<h4 id="inception-module">Inception Module</h4>
<p>The key idea: Use multiple filter sizes in parallel and let the network decide which to use.</p>
<div class="highlight"><pre><span></span><code>Input
â”œâ”€â”€ 1Ã—1 conv
â”œâ”€â”€ 1Ã—1 conv â†’ 3Ã—3 conv
â”œâ”€â”€ 1Ã—1 conv â†’ 5Ã—5 conv
â””â”€â”€ 3Ã—3 maxpool â†’ 1Ã—1 conv
        â†“
    Concatenate
</code></pre></div>
<p><strong>Dimensionality Reduction</strong>: 1Ã—1 convolutions reduce computational cost:
- Without 1Ã—1: <span class="arithmatex">\(5 \times 5 \times 192 \times 32 = 153,600\)</span> operations
- With 1Ã—1: <span class="arithmatex">\(1 \times 1 \times 192 \times 16 + 5 \times 5 \times 16 \times 32 = 15,872\)</span> operations</p>
<h4 id="auxiliary-classifiers">Auxiliary Classifiers</h4>
<p>To combat vanishing gradients, GoogLeNet uses auxiliary classifiers at intermediate layers:</p>
<div class="arithmatex">\[\mathcal{L}_{total} = \mathcal{L}_{main} + 0.3 \times \mathcal{L}_{aux1} + 0.3 \times \mathcal{L}_{aux2}\]</div>
<hr />
<h2 id="optimization-techniques">Optimization Techniques</h2>
<h3 id="gradient-descent-variants">Gradient Descent Variants</h3>
<h4 id="stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</h4>
<p><strong>Vanilla SGD</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \nabla_{\theta} \mathcal{L}(\theta_t; x^{(i)}, y^{(i)})\)</span>\)</span></p>
<p><strong>SGD with Momentum</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(v_t = \gamma v_{t-1} + \eta \nabla_{\theta} \mathcal{L}(\theta_t)\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\theta_{t+1} = \theta_t - v_t\)</span>\)</span></p>
<p>Where <span class="arithmatex">\(\gamma\)</span> (typically 0.9) is the momentum coefficient.</p>
<p><strong>Nesterov Accelerated Gradient (NAG)</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(v_t = \gamma v_{t-1} + \eta \nabla_{\theta} \mathcal{L}(\theta_t - \gamma v_{t-1})\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\theta_{t+1} = \theta_t - v_t\)</span>\)</span></p>
<h4 id="adaptive-learning-rate-methods">Adaptive Learning Rate Methods</h4>
<p><strong>AdaGrad</strong>: <a href="https://jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></p>
<div class="arithmatex">\[G_t = G_{t-1} + (\nabla_{\theta} \mathcal{L}(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t + \epsilon}} \nabla_{\theta} \mathcal{L}(\theta_t)\]</div>
<p><strong>RMSprop</strong>: <a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</a></p>
<div class="arithmatex">\[E[g^2]_t = \gamma E[g^2]_{t-1} + (1-\gamma) (\nabla_{\theta} \mathcal{L}(\theta_t))^2$$
$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{E[g^2]_t + \epsilon}} \nabla_{\theta} \mathcal{L}(\theta_t)\]</div>
<p><strong>Adam</strong>: <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></p>
<div class="arithmatex">\[m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_{\theta} \mathcal{L}(\theta_t)$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla_{\theta} \mathcal{L}(\theta_t))^2\]</div>
<div class="arithmatex">\[\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t}\]</div>
<div class="arithmatex">\[\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</div>
<p>Typical values: <span class="arithmatex">\(\beta_1 = 0.9\)</span>, <span class="arithmatex">\(\beta_2 = 0.999\)</span>, <span class="arithmatex">\(\epsilon = 10^{-8}\)</span></p>
<p><strong>AdamW</strong>: <a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a></p>
<p>Decouples weight decay from gradient-based update:
<span class="arithmatex">\(<span class="arithmatex">\(\theta_{t+1} = \theta_t - \eta \left(\frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_t\right)\)</span>\)</span></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>

<span class="c1"># Optimizer comparison</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YourModel</span><span class="p">()</span>

<span class="c1"># SGD with momentum</span>
<span class="n">optimizer_sgd</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="c1"># Adam</span>
<span class="n">optimizer_adam</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">))</span>

<span class="c1"># AdamW</span>
<span class="n">optimizer_adamw</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Learning rate scheduling</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span><span class="n">optimizer_adam</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</code></pre></div>
<h3 id="learning-rate-scheduling">Learning Rate Scheduling</h3>
<h4 id="step-decay">Step Decay</h4>
<div class="arithmatex">\[\eta_t = \eta_0 \times \gamma^{\lfloor t/s \rfloor}\]</div>
<p>Where <span class="arithmatex">\(s\)</span> is the step size and <span class="arithmatex">\(\gamma\)</span> is the decay factor.</p>
<h4 id="exponential-decay">Exponential Decay</h4>
<div class="arithmatex">\[\eta_t = \eta_0 \times e^{-\lambda t}\]</div>
<h4 id="cosine-annealing">Cosine Annealing</h4>
<div class="arithmatex">\[\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t}{T}\pi))\]</div>
<h4 id="warm-up-and-restart">Warm-up and Restart</h4>
<p><strong>Linear Warm-up</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(\eta_t = \begin{cases}
\frac{t}{T_{warmup}} \eta_{target} &amp; \text{if } t &lt; T_{warmup} \\
\eta_{target} &amp; \text{otherwise}
\end{cases}\)</span>\)</span></p>
<h3 id="weight-initialization">Weight Initialization</h3>
<h4 id="xavierglorot-initialization">Xavier/Glorot Initialization</h4>
<p><strong>Paper</strong>: <a href="http://proceedings.mlr.press/v9/glorot10a.html">Understanding the difficulty of training deep feedforward neural networks</a></p>
<p>For layer with <span class="arithmatex">\(n_{in}\)</span> inputs and <span class="arithmatex">\(n_{out}\)</span> outputs:</p>
<p><strong>Xavier Normal</strong>: <span class="arithmatex">\(W \sim \mathcal{N}(0, \frac{2}{n_{in} + n_{out}})\)</span></p>
<p><strong>Xavier Uniform</strong>: <span class="arithmatex">\(W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}})\)</span></p>
<h4 id="he-initialization">He Initialization</h4>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<p>Designed for ReLU activations:</p>
<p><strong>He Normal</strong>: <span class="arithmatex">\(W \sim \mathcal{N}(0, \frac{2}{n_{in}})\)</span></p>
<p><strong>He Uniform</strong>: <span class="arithmatex">\(W \sim \mathcal{U}(-\sqrt{\frac{6}{n_{in}}}, \sqrt{\frac{6}{n_{in}}})\)</span></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
        <span class="c1"># He initialization for ReLU</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">kaiming_normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;fan_out&#39;</span><span class="p">,</span> <span class="n">nonlinearity</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">):</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="c1"># Apply initialization</span>
<span class="n">model</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</code></pre></div>
<hr />
<h2 id="regularization-methods">Regularization Methods</h2>
<h3 id="l1-and-l2-regularization">L1 and L2 Regularization</h3>
<h4 id="l2-regularization-weight-decay">L2 Regularization (Weight Decay)</h4>
<div class="arithmatex">\[\mathcal{L}_{total} = \mathcal{L}_{data} + \lambda \sum_{i} w_i^2\]</div>
<p>Gradient update:
<span class="arithmatex">\(<span class="arithmatex">\(\frac{\partial \mathcal{L}_{total}}{\partial w_i} = \frac{\partial \mathcal{L}_{data}}{\partial w_i} + 2\lambda w_i\)</span>\)</span></p>
<h4 id="l1-regularization-lasso">L1 Regularization (Lasso)</h4>
<div class="arithmatex">\[\mathcal{L}_{total} = \mathcal{L}_{data} + \lambda \sum_{i} |w_i|\]</div>
<p>Promotes sparsity in weights.</p>
<h4 id="elastic-net">Elastic Net</h4>
<div class="arithmatex">\[\mathcal{L}_{total} = \mathcal{L}_{data} + \lambda_1 \sum_{i} |w_i| + \lambda_2 \sum_{i} w_i^2\]</div>
<h3 id="dropout">Dropout</h3>
<p><strong>Paper</strong>: <a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p>
<h4 id="standard-dropout">Standard Dropout</h4>
<p>During training:
<span class="arithmatex">\(<span class="arithmatex">\(y_i = \begin{cases}
\frac{x_i}{1-p} &amp; \text{with probability } 1-p \\
0 &amp; \text{with probability } p
\end{cases}\)</span>\)</span></p>
<p>During inference: <span class="arithmatex">\(y_i = x_i\)</span> (no dropout)</p>
<h4 id="inverted-dropout">Inverted Dropout</h4>
<p>Scale during training to avoid scaling during inference:
<span class="arithmatex">\(<span class="arithmatex">\(y_i = \begin{cases}
\frac{x_i}{1-p} &amp; \text{with probability } 1-p \\
0 &amp; \text{with probability } p
\end{cases}\)</span>\)</span></p>
<h4 id="dropconnect">DropConnect</h4>
<p><strong>Paper</strong>: <a href="https://proceedings.mlr.press/v28/wan13.html">Regularization of Neural Networks using DropConnect</a></p>
<p>Instead of dropping activations, drop connections (weights):
<span class="arithmatex">\(<span class="arithmatex">\(y = f((W \odot M)x + b)\)</span>\)</span></p>
<p>Where <span class="arithmatex">\(M\)</span> is a binary mask.</p>
<h3 id="batch-normalization">Batch Normalization</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
<h4 id="algorithm">Algorithm</h4>
<p>For a mini-batch <span class="arithmatex">\(\mathcal{B} = \{x_1, ..., x_m\}\)</span>:</p>
<ol>
<li>
<p><strong>Compute statistics</strong>:
   <span class="arithmatex">\(<span class="arithmatex">\(\mu_{\mathcal{B}} = \frac{1}{m} \sum_{i=1}^m x_i\)</span>\)</span>
   <span class="arithmatex">\(<span class="arithmatex">\(\sigma_{\mathcal{B}}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2\)</span>\)</span></p>
</li>
<li>
<p><strong>Normalize</strong>:
   <span class="arithmatex">\(<span class="arithmatex">\(\hat{x}_i = \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2 + \epsilon}}\)</span>\)</span></p>
</li>
<li>
<p><strong>Scale and shift</strong>:
   <span class="arithmatex">\(<span class="arithmatex">\(y_i = \gamma \hat{x}_i + \beta\)</span>\)</span></p>
</li>
</ol>
<p>Where <span class="arithmatex">\(\gamma\)</span> and <span class="arithmatex">\(\beta\)</span> are learnable parameters.</p>
<h4 id="benefits">Benefits</h4>
<ol>
<li><strong>Faster training</strong>: Higher learning rates possible</li>
<li><strong>Reduced sensitivity to initialization</strong></li>
<li><strong>Regularization effect</strong>: Reduces overfitting</li>
<li><strong>Gradient flow</strong>: Helps with vanishing gradients</li>
</ol>
<h4 id="variants">Variants</h4>
<p><strong>Layer Normalization</strong>: <a href="https://arxiv.org/abs/1607.06450">Layer Normalization</a>
<span class="arithmatex">\(<span class="arithmatex">\(\mu_l = \frac{1}{H} \sum_{i=1}^H x_i^l, \quad \sigma_l^2 = \frac{1}{H} \sum_{i=1}^H (x_i^l - \mu_l)^2\)</span>\)</span></p>
<p><strong>Group Normalization</strong>: <a href="https://arxiv.org/abs/1803.08494">Group Normalization</a></p>
<p><strong>Instance Normalization</strong>: <a href="https://arxiv.org/abs/1607.08022">Instance Normalization: The Missing Ingredient for Fast Stylization</a></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="k">class</span><span class="w"> </span><span class="nc">NormalizationComparison</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="c1"># Different normalization techniques</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">([</span><span class="n">num_features</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>  <span class="c1"># [C, H, W]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">group_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GroupNorm</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_features</span><span class="p">)</span>  <span class="c1"># 8 groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">instance_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">InstanceNorm2d</span><span class="p">(</span><span class="n">num_features</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Choose normalization based on use case</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h3 id="data-augmentation">Data Augmentation</h3>
<h4 id="traditional-augmentations">Traditional Augmentations</h4>
<ol>
<li><strong>Geometric</strong>: Rotation, scaling, translation, flipping</li>
<li><strong>Photometric</strong>: Brightness, contrast, saturation, hue</li>
<li><strong>Noise</strong>: Gaussian noise, salt-and-pepper noise</li>
<li><strong>Occlusion</strong>: Random erasing, cutout</li>
</ol>
<h4 id="advanced-augmentations">Advanced Augmentations</h4>
<p><strong>Mixup</strong>: <a href="https://arxiv.org/abs/1710.09412">mixup: Beyond Empirical Risk Minimization</a></p>
<div class="arithmatex">\[\tilde{x} = \lambda x_i + (1-\lambda) x_j$$
$$\tilde{y} = \lambda y_i + (1-\lambda) y_j\]</div>
<p>Where <span class="arithmatex">\(\lambda \sim \text{Beta}(\alpha, \alpha)\)</span></p>
<p><strong>CutMix</strong>: <a href="https://arxiv.org/abs/1905.04899">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a></p>
<p>Combine patches from two images with proportional labels.</p>
<p><strong>AutoAugment</strong>: <a href="https://arxiv.org/abs/1805.09501">AutoAugment: Learning Augmentation Strategies from Data</a></p>
<p>Use reinforcement learning to find optimal augmentation policies.</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.transforms</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">transforms</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">mixup_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Mixup augmentation&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lam</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">lam</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randperm</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

    <span class="n">mixed_x</span> <span class="o">=</span> <span class="n">lam</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lam</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">y_a</span><span class="p">,</span> <span class="n">y_b</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">mixed_x</span><span class="p">,</span> <span class="n">y_a</span><span class="p">,</span> <span class="n">y_b</span><span class="p">,</span> <span class="n">lam</span>

<span class="c1"># Standard augmentations</span>
<span class="n">transform_train</span> <span class="o">=</span> <span class="n">transforms</span><span class="o">.</span><span class="n">Compose</span><span class="p">([</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomCrop</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">contrast</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">saturation</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">(),</span>
    <span class="n">transforms</span><span class="o">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.4914</span><span class="p">,</span> <span class="mf">0.4822</span><span class="p">,</span> <span class="mf">0.4465</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.2023</span><span class="p">,</span> <span class="mf">0.1994</span><span class="p">,</span> <span class="mf">0.2010</span><span class="p">))</span>
<span class="p">])</span>
</code></pre></div>
<hr />
<h2 id="advanced-training-techniques">Advanced Training Techniques</h2>
<h3 id="transfer-learning">Transfer Learning</h3>
<h4 id="fine-tuning-strategies">Fine-tuning Strategies</h4>
<ol>
<li><strong>Feature Extraction</strong>: Freeze pre-trained layers, train only classifier</li>
<li><strong>Fine-tuning</strong>: Train entire network with lower learning rate</li>
<li><strong>Gradual Unfreezing</strong>: Progressively unfreeze layers during training</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>

<span class="c1"># Load pre-trained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Strategy 1: Feature extraction</span>
<span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">False</span>

<span class="c1"># Replace classifier</span>
<span class="n">model</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

<span class="c1"># Strategy 2: Fine-tuning with different learning rates</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">features</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-4</span><span class="p">},</span>
    <span class="p">{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">model</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">}</span>
<span class="p">],</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</code></pre></div>
<h3 id="multi-gpu-training">Multi-GPU Training</h3>
<h4 id="data-parallelism">Data Parallelism</h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>

<span class="c1"># Simple data parallelism</span>
<span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">DataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
</code></pre></div>
<h4 id="distributed-training">Distributed Training</h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.multiprocessing</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>

<span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">dist</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s2">&quot;nccl&quot;</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="n">setup</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">YourModel</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">rank</span><span class="p">])</span>

    <span class="c1"># Training loop</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
        <span class="c1"># ... training code ...</span>
        <span class="k">pass</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device_count</span><span class="p">()</span>
    <span class="n">mp</span><span class="o">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span> <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>
</code></pre></div>
<h3 id="mixed-precision-training">Mixed Precision Training</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1710.03740">Mixed Precision Training</a></p>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">torch.cuda.amp</span><span class="w"> </span><span class="kn">import</span> <span class="n">autocast</span><span class="p">,</span> <span class="n">GradScaler</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">YourModel</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">GradScaler</span><span class="p">()</span>

<span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="c1"># Forward pass with autocast</span>
    <span class="k">with</span> <span class="n">autocast</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># Backward pass with gradient scaling</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span>
    <span class="n">scaler</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
</code></pre></div>
<hr />
<h2 id="modern-architectures-and-trends">Modern Architectures and Trends</h2>
<h3 id="vision-transformers-vits">Vision Transformers (ViTs)</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
<p><strong>Code</strong>: <a href="https://github.com/google-research/vision_transformer">Vision Transformer</a></p>
<h4 id="architecture">Architecture</h4>
<ol>
<li><strong>Patch Embedding</strong>: Split image into patches and linearly embed</li>
<li><strong>Position Embedding</strong>: Add learnable position embeddings</li>
<li><strong>Transformer Encoder</strong>: Standard transformer blocks</li>
<li><strong>Classification Head</strong>: MLP for final prediction</li>
</ol>
<h4 id="mathematical-formulation_2">Mathematical Formulation</h4>
<p><strong>Patch Embedding</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{z}_0 = [\mathbf{x}_{class}; \mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \cdots; \mathbf{x}_p^N\mathbf{E}] + \mathbf{E}_{pos}\)</span>\)</span></p>
<p>Where:
- <span class="arithmatex">\(\mathbf{x}_p^i \in \mathbb{R}^{P^2 \cdot C}\)</span> is the <span class="arithmatex">\(i\)</span>-th flattened patch
- <span class="arithmatex">\(\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}\)</span> is the patch embedding matrix
- <span class="arithmatex">\(\mathbf{E}_{pos} \in \mathbb{R}^{(N+1) \times D}\)</span> are position embeddings</p>
<p><strong>Transformer Block</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{z}'_l = \text{MSA}(\text{LN}(\mathbf{z}_{l-1})) + \mathbf{z}_{l-1}\)</span>\)</span>
<span class="arithmatex">\(<span class="arithmatex">\(\mathbf{z}_l = \text{MLP}(\text{LN}(\mathbf{z}'_l)) + \mathbf{z}'_l\)</span>\)</span></p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">einops</span><span class="w"> </span><span class="kn">import</span> <span class="n">rearrange</span>

<span class="k">class</span><span class="w"> </span><span class="nc">PatchEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> 
                                   <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># x: (B, C, H, W)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, embed_dim, H/P, W/P)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;b e h w -&gt; b (h w) e&#39;</span><span class="p">)</span>  <span class="c1"># (B, N, embed_dim)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="k">class</span><span class="w"> </span><span class="nc">VisionTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="o">=</span><span class="mi">224</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">in_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> 
                 <span class="n">num_classes</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">12</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span> <span class="o">=</span> <span class="n">PatchEmbedding</span><span class="p">(</span><span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="o">.</span><span class="n">n_patches</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoder</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">TransformerEncoderLayer</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> 
                                     <span class="n">dim_feedforward</span><span class="o">=</span><span class="mi">4</span><span class="o">*</span><span class="n">embed_dim</span><span class="p">,</span> 
                                     <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">num_layers</span><span class="o">=</span><span class="n">depth</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Patch embedding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">patch_embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (B, N, embed_dim)</span>

        <span class="c1"># Add class token</span>
        <span class="n">cls_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cls_token</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">cls_tokens</span><span class="p">,</span> <span class="n">x</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, N+1, embed_dim)</span>

        <span class="c1"># Add position embedding</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embed</span>

        <span class="c1"># Transformer</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Classification</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span>  <span class="c1"># Use class token</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
<h3 id="efficientnet-scaling-cnns-efficiently-2019">EfficientNet: Scaling CNNs Efficiently (2019)</h3>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p>
<p><strong>Authors</strong>: Mingxing Tan, Quoc V. Le (Google)</p>
<p><strong>Code</strong>: <a href="https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet">EfficientNet Implementation</a></p>
<h4 id="compound-scaling">Compound Scaling</h4>
<p>Traditional scaling methods focus on one dimension:
- <strong>Width</strong>: Number of channels
- <strong>Depth</strong>: Number of layers<br />
- <strong>Resolution</strong>: Input image size</p>
<p>EfficientNet proposes <strong>compound scaling</strong> that uniformly scales all three dimensions:</p>
<div class="arithmatex">\[\text{depth: } d = \alpha^\phi$$
$$\text{width: } w = \beta^\phi$$  
$$\text{resolution: } r = \gamma^\phi\]</div>
<p>Subject to: <span class="arithmatex">\(\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2\)</span> and <span class="arithmatex">\(\alpha \geq 1, \beta \geq 1, \gamma \geq 1\)</span></p>
<p>Where <span class="arithmatex">\(\phi\)</span> is the compound coefficient that controls resource availability.</p>
<h4 id="mobile-inverted-bottleneck-mbconv">Mobile Inverted Bottleneck (MBConv)</h4>
<p>EfficientNet uses MBConv blocks with:
1. <strong>Depthwise Separable Convolutions</strong>
2. <strong>Squeeze-and-Excitation (SE) blocks</strong>
3. <strong>Skip connections</strong></p>
<p><strong>MBConv Block</strong>:
<div class="highlight"><pre><span></span><code>Input â†’ 1Ã—1 Conv (expand) â†’ 3Ã—3 DWConv â†’ SE â†’ 1Ã—1 Conv (project) â†’ Output
  â†“                                                                    â†‘
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ skip connection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</code></pre></div></p>
<h4 id="squeeze-and-excitation-se">Squeeze-and-Excitation (SE)</h4>
<p><strong>Paper</strong>: <a href="https://arxiv.org/abs/1709.01507">Squeeze-and-Excitation Networks</a></p>
<p><strong>Mathematical Formulation</strong>:
1. <strong>Squeeze</strong>: Global average pooling
   <span class="arithmatex">\(<span class="arithmatex">\(z_c = \frac{1}{H \times W} \sum_{i=1}^H \sum_{j=1}^W x_{c,i,j}\)</span>\)</span></p>
<ol>
<li>
<p><strong>Excitation</strong>: Two FC layers with sigmoid
   <span class="arithmatex">\(<span class="arithmatex">\(s = \sigma(W_2 \delta(W_1 z))\)</span>\)</span></p>
</li>
<li>
<p><strong>Scale</strong>: Channel-wise multiplication
   <span class="arithmatex">\(<span class="arithmatex">\(\tilde{x}_{c,i,j} = s_c \cdot x_{c,i,j}\)</span>\)</span></p>
</li>
</ol>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SEBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">AdaptiveAvgPool2d</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">excitation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channels</span><span class="p">,</span> <span class="n">channels</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">channels</span> <span class="o">//</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">excitation</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MBConvBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">expand_ratio</span><span class="p">,</span> <span class="n">se_ratio</span><span class="o">=</span><span class="mf">0.25</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_residual</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">in_channels</span> <span class="o">==</span> <span class="n">out_channels</span>
        <span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">expand_ratio</span>

        <span class="n">layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># Expand</span>
        <span class="k">if</span> <span class="n">expand_ratio</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">),</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="p">])</span>

        <span class="c1"># Depthwise</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> 
                     <span class="n">kernel_size</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU6</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="c1"># SE</span>
        <span class="k">if</span> <span class="n">se_ratio</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">SEBlock</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">se_ratio</span><span class="p">)))</span>

        <span class="c1"># Project</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="n">out_channels</span><span class="p">)</span>
        <span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_residual</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
<h3 id="neural-architecture-search-nas">Neural Architecture Search (NAS)</h3>
<h4 id="automl-and-architecture-search">AutoML and Architecture Search</h4>
<p><strong>Papers</strong>:
- <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a>
- <a href="https://arxiv.org/abs/1806.09055">DARTS: Differentiable Architecture Search</a>
- <a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a></p>
<p><strong>Key Approaches</strong>:
1. <strong>Reinforcement Learning</strong>: Use RL to search architecture space
2. <strong>Evolutionary Algorithms</strong>: Evolve architectures through mutations
3. <strong>Differentiable Search</strong>: Make architecture search differentiable
4. <strong>Progressive Search</strong>: Gradually increase complexity</p>
<h4 id="darts-differentiable-architecture-search">DARTS (Differentiable Architecture Search)</h4>
<p><strong>Continuous Relaxation</strong>: Instead of discrete architecture choices, use weighted combinations:</p>
<div class="arithmatex">\[o^{(i,j)} = \sum_{o \in \mathcal{O}} \frac{\exp(\alpha_o^{(i,j)})}{\sum_{o' \in \mathcal{O}} \exp(\alpha_{o'}^{(i,j)})} o(x)\]</div>
<p>Where <span class="arithmatex">\(\alpha\)</span> are architecture parameters learned via gradient descent.</p>
<hr />
<h2 id="semi-supervised-and-self-supervised-learning">Semi-Supervised and Self-Supervised Learning</h2>
<h3 id="semi-supervised-learning">Semi-Supervised Learning</h3>
<h4 id="problem-formulation">Problem Formulation</h4>
<p>Given:
- Labeled data: <span class="arithmatex">\(\mathcal{D}_l = \{(x_i, y_i)\}_{i=1}^{n_l}\)</span>
- Unlabeled data: <span class="arithmatex">\(\mathcal{D}_u = \{x_j\}_{j=1}^{n_u}\)</span> where <span class="arithmatex">\(n_u \gg n_l\)</span></p>
<p>Goal: Learn from both labeled and unlabeled data to improve performance.</p>
<h4 id="consistency-regularization">Consistency Regularization</h4>
<p><strong>Î -Model</strong>: <a href="https://arxiv.org/abs/1610.02242">Temporal Ensembling for Semi-Supervised Learning</a></p>
<div class="arithmatex">\[\mathcal{L} = \mathcal{L}_{supervised} + \lambda \mathcal{L}_{consistency}\]</div>
<p>Where:
<span class="arithmatex">\(<span class="arithmatex">\(\mathcal{L}_{consistency} = \mathbb{E}[||f(x + \epsilon_1) - f(x + \epsilon_2)||^2]\)</span>\)</span></p>
<p><strong>Mean Teacher</strong>: <a href="https://arxiv.org/abs/1703.01780">Mean teachers are better role models</a></p>
<p>Use exponential moving average of student weights as teacher:
<span class="arithmatex">\(<span class="arithmatex">\(\theta'_t = \alpha \theta'_{t-1} + (1-\alpha) \theta_t\)</span>\)</span></p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">MeanTeacher</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_model</span><span class="p">,</span> <span class="n">teacher_model</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.999</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">student</span> <span class="o">=</span> <span class="n">student_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teacher</span> <span class="o">=</span> <span class="n">teacher_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

        <span class="c1"># Initialize teacher with student weights</span>
        <span class="k">for</span> <span class="n">teacher_param</span><span class="p">,</span> <span class="n">student_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">teacher</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                                               <span class="bp">self</span><span class="o">.</span><span class="n">student</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">teacher_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">student_param</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update_teacher</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># EMA update</span>
        <span class="k">for</span> <span class="n">teacher_param</span><span class="p">,</span> <span class="n">student_param</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">teacher</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                                               <span class="bp">self</span><span class="o">.</span><span class="n">student</span><span class="o">.</span><span class="n">parameters</span><span class="p">()):</span>
            <span class="n">teacher_param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">student_param</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">consistency_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">student_output</span><span class="p">,</span> <span class="n">teacher_output</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">student_output</span><span class="p">,</span> <span class="n">teacher_output</span><span class="o">.</span><span class="n">detach</span><span class="p">())</span>
</code></pre></div>
<h4 id="pseudo-labeling">Pseudo-Labeling</h4>
<p><strong>Self-Training</strong>: Use model predictions as pseudo-labels for unlabeled data.</p>
<ol>
<li>Train on labeled data</li>
<li>Predict on unlabeled data</li>
<li>Select high-confidence predictions as pseudo-labels</li>
<li>Retrain on labeled + pseudo-labeled data</li>
</ol>
<p><strong>FixMatch</strong>: <a href="https://arxiv.org/abs/2001.07685">FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence</a></p>
<p>Combines consistency regularization with pseudo-labeling:</p>
<div class="arithmatex">\[\mathcal{L} = \mathcal{L}_s + \lambda_u \frac{1}{\mu B} \sum_{b=1}^{\mu B} \mathbb{1}(\max(q_b) \geq \tau) \mathcal{H}(\hat{q}_b, q_b)\]</div>
<p>Where:
- <span class="arithmatex">\(q_b = p_m(y|\alpha(u_b))\)</span> is prediction on weakly augmented unlabeled data
- <span class="arithmatex">\(\hat{q}_b = p_m(y|\mathcal{A}(u_b))\)</span> is prediction on strongly augmented data
- <span class="arithmatex">\(\tau\)</span> is confidence threshold</p>
<h3 id="self-supervised-learning">Self-Supervised Learning</h3>
<h4 id="contrastive-learning">Contrastive Learning</h4>
<p><strong>SimCLR</strong>: <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a></p>
<p><strong>Objective</strong>: Learn representations by contrasting positive and negative pairs.</p>
<div class="arithmatex">\[\ell_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k)/\tau)}\]</div>
<p>Where <span class="arithmatex">\(\text{sim}(u,v) = u^T v / (||u|| ||v||)\)</span> is cosine similarity.</p>
<p><strong>MoCo</strong>: <a href="https://arxiv.org/abs/1911.05722">Momentum Contrast for Unsupervised Visual Representation Learning</a></p>
<p>Uses momentum-updated encoder and memory bank for consistent negative samples.</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">SimCLR</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_encoder</span><span class="p">,</span> <span class="n">projection_dim</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">base_encoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projector</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">base_encoder</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">base_encoder</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">base_encoder</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">in_features</span><span class="p">,</span> <span class="n">projection_dim</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">base_encoder</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Identity</span><span class="p">()</span>  <span class="c1"># Remove classification head</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projector</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">contrastive_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">z1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">z1</span><span class="p">,</span> <span class="n">z2</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1"># Compute similarity matrix</span>
        <span class="n">sim_matrix</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="o">.</span><span class="n">t</span><span class="p">())</span> <span class="o">/</span> <span class="n">temperature</span>

        <span class="c1"># Create labels for positive pairs</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_size</span><span class="p">,</span>
                           <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># Mask out self-similarity</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">z</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sim_matrix</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">))</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">sim_matrix</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>
</code></pre></div>
<h4 id="masked-languageimage-modeling">Masked Language/Image Modeling</h4>
<p><strong>BERT</strong>: <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers</a></p>
<p><strong>MAE</strong>: <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a></p>
<p>Mask random patches and reconstruct them:</p>
<div class="arithmatex">\[\mathcal{L} = \mathbb{E}[||x_{masked} - \hat{x}_{masked}||^2]\]</div>
<hr />
<h2 id="implementation-guide">Implementation Guide</h2>
<h3 id="setting-up-a-deep-learning-project">Setting Up a Deep Learning Project</h3>
<h4 id="project-structure">Project Structure</h4>
<div class="highlight"><pre><span></span><code>project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ datasets.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ resnet.py
â”‚   â”œâ”€â”€ vit.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py
â”‚   â”œâ”€â”€ losses.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ base.yaml
â”‚   â”œâ”€â”€ resnet50.yaml
â”‚   â””â”€â”€ vit_base.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ inference.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
</code></pre></div>
<h4 id="configuration-management">Configuration Management</h4>
<div class="highlight"><pre><span></span><code><span class="c1"># configs/base.yaml</span>
<span class="n">model</span><span class="p">:</span>
  <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;resnet50&quot;</span>
  <span class="n">num_classes</span><span class="p">:</span> <span class="mi">1000</span>
  <span class="n">pretrained</span><span class="p">:</span> <span class="n">true</span>

<span class="n">data</span><span class="p">:</span>
  <span class="n">dataset</span><span class="p">:</span> <span class="s2">&quot;imagenet&quot;</span>
  <span class="n">batch_size</span><span class="p">:</span> <span class="mi">256</span>
  <span class="n">num_workers</span><span class="p">:</span> <span class="mi">8</span>
  <span class="n">image_size</span><span class="p">:</span> <span class="mi">224</span>

<span class="n">training</span><span class="p">:</span>
  <span class="n">epochs</span><span class="p">:</span> <span class="mi">100</span>
  <span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.1</span>
  <span class="n">optimizer</span><span class="p">:</span> <span class="s2">&quot;sgd&quot;</span>
  <span class="n">momentum</span><span class="p">:</span> <span class="mf">0.9</span>
  <span class="n">weight_decay</span><span class="p">:</span> <span class="mf">1e-4</span>
  <span class="n">scheduler</span><span class="p">:</span> <span class="s2">&quot;cosine&quot;</span>

<span class="c1"># config.py</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">yaml</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Config</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">config</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">config</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">updates</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">updates</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
</code></pre></div>
<h4 id="training-loop-template">Training Loop Template</h4>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tqdm</span><span class="w"> </span><span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">wandb</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Trainer</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span> <span class="o">=</span> <span class="n">train_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_loader</span> <span class="o">=</span> <span class="n">val_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># Setup optimizer</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;sgd&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">momentum</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">momentum</span><span class="p">,</span>
                <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">weight_decay</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="s1">&#39;adam&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
                <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                <span class="n">weight_decay</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">weight_decay</span>
            <span class="p">)</span>

        <span class="c1"># Setup scheduler</span>
        <span class="k">if</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">==</span> <span class="s1">&#39;cosine&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">CosineAnnealingLR</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">T_max</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">epochs</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">==</span> <span class="s1">&#39;step&#39;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="o">.</span><span class="n">StepLR</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">step_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train_epoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="n">pbar</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">pbar</span><span class="p">):</span>
            <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

            <span class="c1"># Update progress bar</span>
            <span class="n">pbar</span><span class="o">.</span><span class="n">set_postfix</span><span class="p">({</span>
                <span class="s1">&#39;Loss&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
                <span class="s1">&#39;Acc&#39;</span><span class="p">:</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mf">100.</span><span class="o">*</span><span class="n">correct</span><span class="o">/</span><span class="n">total</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span>
            <span class="p">})</span>

        <span class="k">return</span> <span class="n">total_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_loader</span><span class="p">),</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">validate</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">val_loader</span><span class="p">:</span>
                <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="n">val_loss</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">total</span> <span class="o">+=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">val_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">val_loader</span><span class="p">)</span>
        <span class="n">val_acc</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="n">total</span>

        <span class="k">return</span> <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">best_acc</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">training</span><span class="o">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">train_loss</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
            <span class="n">val_loss</span><span class="p">,</span> <span class="n">val_acc</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validate</span><span class="p">()</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># Log metrics</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">log</span><span class="p">({</span>
                <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="s1">&#39;train_loss&#39;</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span>
                <span class="s1">&#39;train_acc&#39;</span><span class="p">:</span> <span class="n">train_acc</span><span class="p">,</span>
                <span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">val_loss</span><span class="p">,</span>
                <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">,</span>
                <span class="s1">&#39;lr&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
            <span class="p">})</span>

            <span class="c1"># Save best model</span>
            <span class="k">if</span> <span class="n">val_acc</span> <span class="o">&gt;</span> <span class="n">best_acc</span><span class="p">:</span>
                <span class="n">best_acc</span> <span class="o">=</span> <span class="n">val_acc</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">({</span>
                    <span class="s1">&#39;epoch&#39;</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                    <span class="s1">&#39;model_state_dict&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s1">&#39;optimizer_state_dict&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span>
                    <span class="s1">&#39;best_acc&#39;</span><span class="p">:</span> <span class="n">best_acc</span><span class="p">,</span>
                <span class="p">},</span> <span class="s1">&#39;best_model.pth&#39;</span><span class="p">)</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s1">: Train Loss: </span><span class="si">{</span><span class="n">train_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Train Acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%, &#39;</span>
                  <span class="sa">f</span><span class="s1">&#39;Val Loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">:</span><span class="s1">.4f</span><span class="si">}</span><span class="s1">, Val Acc: </span><span class="si">{</span><span class="n">val_acc</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">%&#39;</span><span class="p">)</span>
</code></pre></div>
<h3 id="debugging-and-monitoring">Debugging and Monitoring</h3>
<h4 id="common-issues-and-solutions">Common Issues and Solutions</h4>
<ol>
<li>
<p><strong>Vanishing/Exploding Gradients</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># Monitor gradient norms</span>
<span class="k">def</span><span class="w"> </span><span class="nf">monitor_gradients</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">param_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">total_norm</span> <span class="o">+=</span> <span class="n">param_norm</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="n">total_norm</span> <span class="o">=</span> <span class="n">total_norm</span> <span class="o">**</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">total_norm</span>

<span class="c1"># Gradient clipping</span>
<span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm_</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">max_norm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Memory Issues</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># Gradient accumulation</span>
<span class="n">accumulation_steps</span> <span class="o">=</span> <span class="mi">4</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">/</span> <span class="n">accumulation_steps</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">accumulation_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</code></pre></div></p>
</li>
<li>
<p><strong>Learning Rate Issues</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># Learning rate finder</span>
<span class="k">def</span><span class="w"> </span><span class="nf">find_lr</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">start_lr</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">end_lr</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">start_lr</span>
    <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">lr</span> <span class="o">*=</span> <span class="p">(</span><span class="n">end_lr</span> <span class="o">/</span> <span class="n">start_lr</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">param_group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="k">if</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="n">end_lr</span><span class="p">:</span>
            <span class="k">break</span>

    <span class="k">return</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">losses</span>
</code></pre></div></p>
</li>
</ol>
<hr />
<h2 id="references-and-resources">References and Resources</h2>
<h3 id="foundational-papers">Foundational Papers</h3>
<h4 id="historical-foundations">Historical Foundations</h4>
<ol>
<li>
<p><strong>McCulloch, W. S., &amp; Pitts, W.</strong> (1943). <a href="https://link.springer.com/article/10.1007/BF02478259">A logical calculus of the ideas immanent in nervous activity</a>. <em>Bulletin of Mathematical Biophysics</em>.</p>
</li>
<li>
<p><strong>Rosenblatt, F.</strong> (1958). <a href="https://psycnet.apa.org/record/1959-09865-001">The perceptron: a probabilistic model for information storage and organization in the brain</a>. <em>Psychological Review</em>.</p>
</li>
<li>
<p><strong>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J.</strong> (1986). <a href="https://www.nature.com/articles/323533a0">Learning representations by back-propagating errors</a>. <em>Nature</em>.</p>
</li>
</ol>
<h4 id="modern-deep-learning">Modern Deep Learning</h4>
<ol>
<li>
<p><strong>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P.</strong> (1998). <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-based learning applied to document recognition</a>. <em>Proceedings of the IEEE</em>.</p>
</li>
<li>
<p><strong>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E.</strong> (2012). <a href="https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">ImageNet classification with deep convolutional neural networks</a>. <em>NIPS</em>.</p>
</li>
<li>
<p><strong>Simonyan, K., &amp; Zisserman, A.</strong> (2014). <a href="https://arxiv.org/abs/1409.1556">Very deep convolutional networks for large-scale image recognition</a>. <em>ICLR</em>.</p>
</li>
<li>
<p><strong>He, K., Zhang, X., Ren, S., &amp; Sun, J.</strong> (2016). <a href="https://arxiv.org/abs/1512.03385">Deep residual learning for image recognition</a>. <em>CVPR</em>.</p>
</li>
<li>
<p><strong>Dosovitskiy, A., et al.</strong> (2020). <a href="https://arxiv.org/abs/2010.11929">An image is worth 16x16 words: Transformers for image recognition at scale</a>. <em>ICLR</em>.</p>
</li>
</ol>
<h4 id="optimization-and-training">Optimization and Training</h4>
<ol>
<li>
<p><strong>Ioffe, S., &amp; Szegedy, C.</strong> (2015). <a href="https://arxiv.org/abs/1502.03167">Batch normalization: Accelerating deep network training by reducing internal covariate shift</a>. <em>ICML</em>.</p>
</li>
<li>
<p><strong>Kingma, D. P., &amp; Ba, J.</strong> (2014). <a href="https://arxiv.org/abs/1412.6980">Adam: A method for stochastic optimization</a>. <em>ICLR</em>.</p>
</li>
<li>
<p><strong>Srivastava, N., et al.</strong> (2014). <a href="https://jmlr.org/papers/v15/srivastava14a.html">Dropout: A simple way to prevent neural networks from overfitting</a>. <em>JMLR</em>.</p>
</li>
</ol>
<h4 id="self-supervised-learning_1">Self-Supervised Learning</h4>
<ol>
<li>
<p><strong>Chen, T., et al.</strong> (2020). <a href="https://arxiv.org/abs/2002.05709">A simple framework for contrastive learning of visual representations</a>. <em>ICML</em>.</p>
</li>
<li>
<p><strong>He, K., et al.</strong> (2022). <a href="https://arxiv.org/abs/2111.06377">Masked autoencoders are scalable vision learners</a>. <em>CVPR</em>.</p>
</li>
</ol>
<h3 id="implementation-resources">Implementation Resources</h3>
<h4 id="frameworks-and-libraries">Frameworks and Libraries</h4>
<ul>
<li><strong>PyTorch</strong>: <a href="https://pytorch.org/">https://pytorch.org/</a></li>
<li><strong>TensorFlow</strong>: <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></li>
<li><strong>JAX</strong>: <a href="https://github.com/google/jax">https://github.com/google/jax</a></li>
<li><strong>Hugging Face Transformers</strong>: <a href="https://huggingface.co/transformers/">https://huggingface.co/transformers/</a></li>
<li><strong>timm (PyTorch Image Models)</strong>: <a href="https://github.com/rwightman/pytorch-image-models">https://github.com/rwightman/pytorch-image-models</a></li>
</ul>
<h4 id="datasets">Datasets</h4>
<ul>
<li><strong>ImageNet</strong>: <a href="http://www.image-net.org/">http://www.image-net.org/</a></li>
<li><strong>CIFAR-10/100</strong>: <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></li>
<li><strong>COCO</strong>: <a href="https://cocodataset.org/">https://cocodataset.org/</a></li>
<li><strong>Open Images</strong>: <a href="https://storage.googleapis.com/openimages/web/index.html">https://storage.googleapis.com/openimages/web/index.html</a></li>
</ul>
<h4 id="tools-and-utilities">Tools and Utilities</h4>
<ul>
<li><strong>Weights &amp; Biases</strong>: <a href="https://wandb.ai/">https://wandb.ai/</a></li>
<li><strong>TensorBoard</strong>: <a href="https://www.tensorflow.org/tensorboard">https://www.tensorflow.org/tensorboard</a></li>
<li><strong>Optuna</strong>: <a href="https://optuna.org/">https://optuna.org/</a></li>
<li><strong>Ray Tune</strong>: <a href="https://docs.ray.io/en/latest/tune/">https://docs.ray.io/en/latest/tune/</a></li>
</ul>
<h3 id="books-and-courses">Books and Courses</h3>
<h4 id="books">Books</h4>
<ol>
<li><strong>Goodfellow, I., Bengio, Y., &amp; Courville, A.</strong> <a href="https://www.deeplearningbook.org/">Deep Learning</a>. <em>MIT Press</em>, 2016.</li>
<li><strong>Bishop, C. M.</strong> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Pattern Recognition and Machine Learning</a>. <em>Springer</em>, 2006.</li>
<li><strong>Murphy, K. P.</strong> <a href="https://probml.github.io/pml-book/">Machine Learning: A Probabilistic Perspective</a>. <em>MIT Press</em>, 2012.</li>
</ol>
<h4 id="online-courses">Online Courses</h4>
<ol>
<li><strong>CS231n: Convolutional Neural Networks for Visual Recognition</strong> - <a href="http://cs231n.stanford.edu/">Stanford</a></li>
<li><strong>CS224n: Natural Language Processing with Deep Learning</strong> - <a href="http://web.stanford.edu/class/cs224n/">Stanford</a></li>
<li><strong>Deep Learning Specialization</strong> - <a href="https://www.coursera.org/specializations/deep-learning">Coursera</a></li>
<li><strong>Fast.ai Practical Deep Learning</strong> - <a href="https://www.fast.ai/">fast.ai</a></li>
</ol>
<hr />
<h2 id="key-takeaways">Key Takeaways</h2>
<h3 id="historical-perspective">Historical Perspective</h3>
<ul>
<li>Deep learning evolved from simple perceptrons to sophisticated architectures through decades of research</li>
<li>Key breakthroughs: backpropagation (1986), CNNs (1990s), AlexNet (2012), ResNet (2015), Transformers (2017)</li>
<li>Each era was enabled by algorithmic innovations, computational advances, and data availability</li>
</ul>
<h3 id="architectural-principles">Architectural Principles</h3>
<ul>
<li><strong>Depth matters</strong>: Deeper networks can learn more complex representations</li>
<li><strong>Skip connections</strong>: Enable training of very deep networks (ResNet)</li>
<li><strong>Attention mechanisms</strong>: Allow models to focus on relevant parts (Transformers)</li>
<li><strong>Efficiency</strong>: Balance between performance and computational cost (EfficientNet)</li>
</ul>
<h3 id="training-best-practices">Training Best Practices</h3>
<ul>
<li><strong>Initialization</strong>: Use appropriate weight initialization (He, Xavier)</li>
<li><strong>Optimization</strong>: Choose suitable optimizers (Adam, AdamW) and learning rate schedules</li>
<li><strong>Regularization</strong>: Prevent overfitting with dropout, batch normalization, data augmentation</li>
<li><strong>Monitoring</strong>: Track gradients, learning curves, and validation metrics</li>
</ul>
<h3 id="modern-trends">Modern Trends</h3>
<ul>
<li><strong>Self-supervised learning</strong>: Learn from unlabeled data</li>
<li><strong>Vision Transformers</strong>: Apply transformer architecture to computer vision</li>
<li><strong>Neural Architecture Search</strong>: Automate architecture design</li>
<li><strong>Efficient training</strong>: Mixed precision, distributed training, gradient accumulation</li>
</ul>
<h3 id="future-directions">Future Directions</h3>
<ul>
<li><strong>Multimodal learning</strong>: Combining vision, language, and other modalities</li>
<li><strong>Few-shot learning</strong>: Learning from limited examples</li>
<li><strong>Continual learning</strong>: Learning new tasks without forgetting old ones</li>
<li><strong>Interpretability</strong>: Understanding what deep networks learn</li>
<li><strong>Sustainability</strong>: Reducing computational and environmental costs</li>
</ul>
<p>Deep learning continues to evolve rapidly, with new architectures, training methods, and applications emerging regularly. The key to success is understanding the fundamental principles while staying current with the latest developments in this dynamic field.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>