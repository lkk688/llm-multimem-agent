# Multimodal Memory LLM Agent

Welcome to the comprehensive documentation for the Multimodal Memory LLM Agent project. This framework provides a modular and extensible architecture for building advanced AI applications with large language models (LLMs), multimodal capabilities, and persistent memory.

## Core Modules

### [Deep Learning Foundations](deep_learning.md)

Comprehensive tutorial on deep learning from fundamentals to modern architectures:

- History of neural networks from perceptrons to deep learning revolution
- Convolutional Neural Networks (CNNs) and major architectures
- Optimization techniques, regularization methods, and advanced training
- Modern architectures, semi-supervised and self-supervised learning
- Mathematical foundations and implementation guides


### [Self-Supervised Learning](self-supervised.md)

Understand the principles and evolution of self-supervised learning:

- Foundations of SSL from word embeddings to modern vision-language models
- Evolution of language models and modality-specific SSL approaches
- Multimodal self-supervised learning and contrastive methods
- Training strategies, scaling laws, and theoretical foundations
- Practical implementation guides and current research directions

### [Transformer Fundamentals](transformers.md)

Learn about the core concepts of Transformer architecture:

- Evolution from RNNs with attention to full Transformer models
- Self-attention mechanisms and multi-head attention
- Encoder-decoder architecture and positional encodings
- Implementation details and code examples

### [Multimodal Embeddings](embeddings.md)

Comprehensive guide to generating embeddings across different modalities:

- Text embeddings from Word2Vec to modern transformer-based approaches
- SentenceTransformers framework and popular models like all-MiniLM-L6-v2
- Siamese/Triplet architectures with various loss functions (triplet, contrastive, MNRL)
- Vision-language models (CLIP, ViT), audio embeddings (Wav2Vec 2.0, Whisper)
- Multimodal fusion techniques and cross-modal understanding

### [LLM Frameworks and Architectures](llm.md)

Dive into the technical details of LLM implementation:

- Evolution from RNNs to Transformer architectures
- Optimization techniques for inference and deployment
- Integration with various LLM providers and frameworks

### [Memory Systems](memory.md)

Understand how persistent memory enhances LLM capabilities:

- Context window management and conversation history
- Vector-based retrieval for semantic search
- Structured knowledge storage and retrieval
- LangChain and LangGraph memory architectures
- Model Context Protocol (MCP) for standardized memory systems

### [Tool Calling and Agent Capabilities](agents.md)

Explore the implementation of LLM agents with tool-calling capabilities:

- Function calling and ReAct (Reasoning and Acting) approaches
- Model Context Protocol (MCP) for standardized context injection
- Multi-agent systems and agentic workflows
- Framework implementations across OpenAI, LangChain, LlamaIndex, AutoGen, and CrewAI
- Tool learning and evaluation benchmarks



### [Multi-Modal Language Models](multi_modal_LM.md)

Explore the evolution and capabilities of multi-modal language models:

- Historical evolution from visual-semantic embeddings to transformer era
- Vision-Language Models (VLMs) including CLIP, BLIP, LLaVA, and Flamingo
- Cross-modal attention mechanisms and mathematical foundations
- Large-scale pre-training approaches and modern architectures


## Advanced Topics

### [Advanced Transformer Techniques](transformers_advanced.md)

Explore cutting-edge modifications and optimizations for Transformers:

- Architectural innovations addressing limitations of original Transformers
- Efficient attention mechanisms for reduced complexity
- Position encoding improvements for longer sequences
- Memory-efficient implementations and inference optimizations

### [GPT Architecture Evolution](gpt_architecture_evolution.md)

Comprehensive analysis of architectural advances from GPT-2 to modern models:

- Evolution from GPT-2 baseline to GPT-oss and GPT-5 architectures
- Key innovations: RoPE, SwiGLU, MoE, GQA, sliding window attention
- MXFP4 quantization and efficiency optimizations
- Practical implementation examples with official OpenAI code
- Comparison with Qwen3 and other modern architectures

### [Inference Optimization](inference_optimization.md)

Discover techniques to optimize LLM inference for production deployment:

- Computational efficiency improvements (KV caching, Flash Attention)
- Memory optimization strategies (quantization, pruning)
- Model compression techniques (distillation, pruning)
- Hardware acceleration and system-level optimizations

### [Autonomous Systems](autonomous_system/autonomous_systems_survey.md)
### [Physical AI in Autonomous Driving](physical_ai_autonomous_driving.md)

## Getting Started

Explore the documentation for each module to understand the architecture, implementation details, and usage examples. The project provides a flexible framework that can be adapted to various use cases and deployment scenarios.
