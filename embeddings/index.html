
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../transformers/">
      
      
        <link rel="next" href="../llm/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.9">
    
    
      
        <title>Multimodal Embeddings - Multimodal Memory LLM and AI Agent</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.4af4bdda.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#multi-modal-embeddings" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-header__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Multimodal Memory LLM and AI Agent
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multimodal Embeddings
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Multimodal Memory LLM and AI Agent" class="md-nav__button md-logo" aria-label="Multimodal Memory LLM and AI Agent" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Multimodal Memory LLM and AI Agent
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Core Modules
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Core Modules
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deep_learning/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Deep Learning Foundations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../self-supervised/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Self-Supervised Learning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Transformer Fundamentals
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    Multimodal Embeddings
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    Multimodal Embeddings
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#embedding-theory-from-word-vectors-to-multimodal-representations" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Theory: From Word Vectors to Multimodal Representations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Embedding Theory: From Word Vectors to Multimodal Representations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-evolution-of-text-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      The Evolution of Text Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Evolution of Text Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word2vec-2013" class="md-nav__link">
    <span class="md-ellipsis">
      Word2Vec (2013)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Word2Vec (2013)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#skip-gram-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Skip-gram Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cbow-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CBOW Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Details
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glove-global-vectors-for-word-representation-2014" class="md-nav__link">
    <span class="md-ellipsis">
      GloVe: Global Vectors for Word Representation (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GloVe: Global Vectors for Word Representation (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weighting-function" class="md-nav__link">
    <span class="md-ellipsis">
      Weighting Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-details_1" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-with-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison with Word2Vec
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contextual-embeddings-bert-and-beyond-2018-present" class="md-nav__link">
    <span class="md-ellipsis">
      Contextual Embeddings: BERT and Beyond (2018-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Contextual Embeddings: BERT and Beyond (2018-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-wise-feed-forward-network" class="md-nav__link">
    <span class="md-ellipsis">
      Position-wise Feed-Forward Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-training Objectives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning-for-downstream-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning for Downstream Tasks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-variants-and-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      BERT Variants and Improvements
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentence-embeddings-2017-present" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Embeddings (2017-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sentence Embeddings (2017-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#early-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Early Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-based-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer-Based Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specialized-sentence-embedding-models" class="md-nav__link">
    <span class="md-ellipsis">
      Specialized Sentence Embedding Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Considerations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentencetransformers-framework" class="md-nav__link">
    <span class="md-ellipsis">
      SentenceTransformers Framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-minilm-l6-v2-deep-dive-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      all-MiniLM-L6-v2: Deep Dive Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#siamese-and-triplet-network-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Siamese and Triplet Network Architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-for-sentence-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions for Sentence Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-training-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Training Techniques
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder-based-embeddings-gpt-and-beyond-2018-present" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder-Based Embeddings: GPT and Beyond (2018-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Decoder-Based Embeddings: GPT and Beyond (2018-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-of-decoder-based-models" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture of Decoder-Based Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-family-evolution" class="md-nav__link">
    <span class="md-ellipsis">
      GPT Family Evolution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-generation-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Generation Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-objectives-for-embedding-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Training Objectives for Embedding Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-decoder-based-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of Decoder-Based Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-of-decoder-based-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages of Decoder-Based Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Challenges and Limitations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-extraction-from-decoder-models" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Extraction from Decoder Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai-text-embeddings-api" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Text Embeddings API
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip-contrastive-language-image-pre-training-2021" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP: Contrastive Language-Image Pre-training (2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-transformer-vit-for-image-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformer (ViT) for Image Embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wav2vec-20-self-supervised-audio-representations" class="md-nav__link">
    <span class="md-ellipsis">
      Wav2Vec 2.0: Self-Supervised Audio Representations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai-whisper-for-audio-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Whisper for Audio Understanding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-fusion-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Fusion Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Fusion Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#early-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Early Fusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#late-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Late Fusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Attention Fusion
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Image Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Neural Networks (CNNs)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks (CNNs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cnn-architecture-components" class="md-nav__link">
    <span class="md-ellipsis">
      CNN Architecture Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#major-cnn-architectures-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Major CNN Architectures for Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-embedding-extraction-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      CNN Embedding Extraction Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-objectives-for-cnn-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Training Objectives for CNN Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-cnn-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of CNN Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-transformers-vit-2020-present" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformers (ViT) (2020-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision Transformers (ViT) (2020-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vit-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      ViT Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-self-attention-in-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head Self-Attention in ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vit-variants-and-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      ViT Variants and Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-strategies-for-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Training Strategies for ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-extraction-from-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Extraction from ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-vit-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of ViT Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages and Limitations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip-contrastive-language-image-pre-training-2021-present" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP: Contrastive Language-Image Pre-training (2021-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLIP: Contrastive Language-Image Pre-training (2021-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-methodology" class="md-nav__link">
    <span class="md-ellipsis">
      Training Methodology
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip-variants-and-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP Variants and Scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-properties-and-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Properties and Extraction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-shot-capabilities" class="md-nav__link">
    <span class="md-ellipsis">
      Zero-Shot Capabilities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-clip-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of CLIP Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations-and-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations and Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-considerations_1" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embeddings_1" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wav2vec-and-wav2vec-20" class="md-nav__link">
    <span class="md-ellipsis">
      Wav2Vec and Wav2Vec 2.0
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#whisper" class="md-nav__link">
    <span class="md-ellipsis">
      Whisper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hubert-and-wavlm" class="md-nav__link">
    <span class="md-ellipsis">
      HuBERT and WavLM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embeddings_1" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#joint-embedding-space-models" class="md-nav__link">
    <span class="md-ellipsis">
      Joint Embedding Space Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Transformers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    <span class="md-ellipsis">
      Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supported-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Frameworks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Supported Frameworks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Text Embedding Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embedding Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embedding Frameworks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    <span class="md-ellipsis">
      Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Text Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Checking Available Frameworks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-applications-of-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Applications of Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Practical Applications of Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#information-retrieval-and-search" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval and Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recommendation-systems" class="md-nav__link">
    <span class="md-ellipsis">
      Recommendation Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-and-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering and Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modal Retrieval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-shot-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Zero-Shot Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture_1" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluating-embedding-quality" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluating Embedding Quality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Evaluating Embedding Quality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intrinsic-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Intrinsic Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intrinsic Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word-similarity-and-relatedness" class="md-nav__link">
    <span class="md-ellipsis">
      Word Similarity and Relatedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analogy-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      Analogy Tasks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-and-visualization" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering and Visualization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extrinsic-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Extrinsic Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Extrinsic Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#information-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-retrieval_1" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modal Retrieval
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-modern-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Modern Embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-directions-in-embedding-research" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions in Embedding Research
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Future Directions in Embedding Research">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multimodal-foundation-models" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Foundation Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficiency-and-compression" class="md-nav__link">
    <span class="md-ellipsis">
      Efficiency and Compression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpretability-and-fairness" class="md-nav__link">
    <span class="md-ellipsis">
      Interpretability and Fairness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compositional-and-hierarchical-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Compositional and Hierarchical Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continual-learning-and-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      Continual Learning and Adaptation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llm/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    LLM Frameworks and Architectures
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../memory/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Systems
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../agents/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Tool Calling and Agent Capabilities
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../multi_modal_LM/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Multi-Modal Language Models
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Advanced Topics
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Advanced Topics
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../transformers_advanced/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Advanced Transformer Techniques
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../inference_optimization/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Inference Optimization
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt_architecture_evolution/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    GPT Architecture Evolution
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../physical_ai_autonomous_driving/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Physical AI in Autonomous Driving
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Notebooks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Notebooks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../notebooks/memory_example/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Memory Example
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#embedding-theory-from-word-vectors-to-multimodal-representations" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Theory: From Word Vectors to Multimodal Representations
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Embedding Theory: From Word Vectors to Multimodal Representations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-evolution-of-text-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      The Evolution of Text Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Evolution of Text Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word2vec-2013" class="md-nav__link">
    <span class="md-ellipsis">
      Word2Vec (2013)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Word2Vec (2013)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#skip-gram-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Skip-gram Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cbow-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CBOW Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimization-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Optimization Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-details" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Details
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#glove-global-vectors-for-word-representation-2014" class="md-nav__link">
    <span class="md-ellipsis">
      GloVe: Global Vectors for Word Representation (2014)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="GloVe: Global Vectors for Word Representation (2014)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#mathematical-foundation" class="md-nav__link">
    <span class="md-ellipsis">
      Mathematical Foundation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#weighting-function" class="md-nav__link">
    <span class="md-ellipsis">
      Weighting Function
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-details_1" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Details
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#comparison-with-word2vec" class="md-nav__link">
    <span class="md-ellipsis">
      Comparison with Word2Vec
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contextual-embeddings-bert-and-beyond-2018-present" class="md-nav__link">
    <span class="md-ellipsis">
      Contextual Embeddings: BERT and Beyond (2018-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Contextual Embeddings: BERT and Beyond (2018-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      Self-Attention Mechanism
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-wise-feed-forward-network" class="md-nav__link">
    <span class="md-ellipsis">
      Position-wise Feed-Forward Network
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pre-training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      Pre-training Objectives
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tokenization" class="md-nav__link">
    <span class="md-ellipsis">
      Tokenization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fine-tuning-for-downstream-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      Fine-tuning for Downstream Tasks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bert-variants-and-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      BERT Variants and Improvements
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentence-embeddings-2017-present" class="md-nav__link">
    <span class="md-ellipsis">
      Sentence Embeddings (2017-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Sentence Embeddings (2017-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#early-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Early Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer-based-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Transformer-Based Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#specialized-sentence-embedding-models" class="md-nav__link">
    <span class="md-ellipsis">
      Specialized Sentence Embedding Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#practical-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Considerations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sentencetransformers-framework" class="md-nav__link">
    <span class="md-ellipsis">
      SentenceTransformers Framework
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-minilm-l6-v2-deep-dive-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      all-MiniLM-L6-v2: Deep Dive Analysis
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#siamese-and-triplet-network-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      Siamese and Triplet Network Architectures
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-for-sentence-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions for Sentence Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advanced-training-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Advanced Training Techniques
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder-based-embeddings-gpt-and-beyond-2018-present" class="md-nav__link">
    <span class="md-ellipsis">
      Decoder-Based Embeddings: GPT and Beyond (2018-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Decoder-Based Embeddings: GPT and Beyond (2018-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture-of-decoder-based-models" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture of Decoder-Based Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gpt-family-evolution" class="md-nav__link">
    <span class="md-ellipsis">
      GPT Family Evolution
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-generation-approaches" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Generation Approaches
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-objectives-for-embedding-generation" class="md-nav__link">
    <span class="md-ellipsis">
      Training Objectives for Embedding Generation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-decoder-based-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of Decoder-Based Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-of-decoder-based-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages of Decoder-Based Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#challenges-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Challenges and Limitations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-extraction-from-decoder-models" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Extraction from Decoder Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai-text-embeddings-api" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Text Embeddings API
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      Vision-Language Models
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip-contrastive-language-image-pre-training-2021" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP: Contrastive Language-Image Pre-training (2021)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-transformer-vit-for-image-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformer (ViT) for Image Embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wav2vec-20-self-supervised-audio-representations" class="md-nav__link">
    <span class="md-ellipsis">
      Wav2Vec 2.0: Self-Supervised Audio Representations
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#openai-whisper-for-audio-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      OpenAI Whisper for Audio Understanding
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-fusion-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Fusion Techniques
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Fusion Techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#early-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Early Fusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#late-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Late Fusion
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-attention-fusion" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Attention Fusion
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Image Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    <span class="md-ellipsis">
      Convolutional Neural Networks (CNNs)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks (CNNs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cnn-architecture-components" class="md-nav__link">
    <span class="md-ellipsis">
      CNN Architecture Components
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#major-cnn-architectures-for-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Major CNN Architectures for Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cnn-embedding-extraction-techniques" class="md-nav__link">
    <span class="md-ellipsis">
      CNN Embedding Extraction Techniques
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-objectives-for-cnn-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Training Objectives for CNN Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-cnn-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of CNN Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-considerations" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vision-transformers-vit-2020-present" class="md-nav__link">
    <span class="md-ellipsis">
      Vision Transformers (ViT) (2020-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Vision Transformers (ViT) (2020-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#vit-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      ViT Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-head-self-attention-in-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Multi-Head Self-Attention in ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vit-variants-and-improvements" class="md-nav__link">
    <span class="md-ellipsis">
      ViT Variants and Improvements
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-strategies-for-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Training Strategies for ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-extraction-from-vit" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Extraction from ViT
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-vit-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of ViT Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-and-limitations" class="md-nav__link">
    <span class="md-ellipsis">
      Advantages and Limitations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip-contrastive-language-image-pre-training-2021-present" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP: Contrastive Language-Image Pre-training (2021-present)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CLIP: Contrastive Language-Image Pre-training (2021-present)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#clip-architecture" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP Architecture
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-methodology" class="md-nav__link">
    <span class="md-ellipsis">
      Training Methodology
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clip-variants-and-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      CLIP Variants and Scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#embedding-properties-and-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      Embedding Properties and Extraction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-shot-capabilities" class="md-nav__link">
    <span class="md-ellipsis">
      Zero-Shot Capabilities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#applications-of-clip-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Applications of CLIP Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#limitations-and-challenges" class="md-nav__link">
    <span class="md-ellipsis">
      Limitations and Challenges
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-considerations_1" class="md-nav__link">
    <span class="md-ellipsis">
      Implementation Considerations
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embeddings_1" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Audio Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#wav2vec-and-wav2vec-20" class="md-nav__link">
    <span class="md-ellipsis">
      Wav2Vec and Wav2Vec 2.0
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#whisper" class="md-nav__link">
    <span class="md-ellipsis">
      Whisper
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hubert-and-wavlm" class="md-nav__link">
    <span class="md-ellipsis">
      HuBERT and WavLM
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embeddings_1" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Multimodal Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#joint-embedding-space-models" class="md-nav__link">
    <span class="md-ellipsis">
      Joint Embedding Space Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-transformers" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Transformers
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#features" class="md-nav__link">
    <span class="md-ellipsis">
      Features
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#supported-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Supported Frameworks
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Supported Frameworks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Text Embedding Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embedding Frameworks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embedding-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embedding Frameworks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    <span class="md-ellipsis">
      Installation
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#usage" class="md-nav__link">
    <span class="md-ellipsis">
      Usage
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Usage">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Text Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#image-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Image Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#audio-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Audio Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal-embedding" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Embedding
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#checking-available-frameworks" class="md-nav__link">
    <span class="md-ellipsis">
      Checking Available Frameworks
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#examples" class="md-nav__link">
    <span class="md-ellipsis">
      Examples
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#practical-applications-of-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Practical Applications of Embeddings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Practical Applications of Embeddings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#information-retrieval-and-search" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval and Search
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recommendation-systems" class="md-nav__link">
    <span class="md-ellipsis">
      Recommendation Systems
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-and-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering and Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modal Retrieval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#zero-shot-learning" class="md-nav__link">
    <span class="md-ellipsis">
      Zero-Shot Learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture_1" class="md-nav__link">
    <span class="md-ellipsis">
      Architecture
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#evaluating-embedding-quality" class="md-nav__link">
    <span class="md-ellipsis">
      Evaluating Embedding Quality
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Evaluating Embedding Quality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intrinsic-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Intrinsic Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Intrinsic Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#word-similarity-and-relatedness" class="md-nav__link">
    <span class="md-ellipsis">
      Word Similarity and Relatedness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#analogy-tasks" class="md-nav__link">
    <span class="md-ellipsis">
      Analogy Tasks
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#clustering-and-visualization" class="md-nav__link">
    <span class="md-ellipsis">
      Clustering and Visualization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#extrinsic-evaluation" class="md-nav__link">
    <span class="md-ellipsis">
      Extrinsic Evaluation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Extrinsic Evaluation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#text-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Text Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#information-retrieval" class="md-nav__link">
    <span class="md-ellipsis">
      Information Retrieval
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-modal-retrieval_1" class="md-nav__link">
    <span class="md-ellipsis">
      Cross-Modal Retrieval
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#benchmarks-for-modern-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Benchmarks for Modern Embeddings
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-directions-in-embedding-research" class="md-nav__link">
    <span class="md-ellipsis">
      Future Directions in Embedding Research
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Future Directions in Embedding Research">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#multimodal-foundation-models" class="md-nav__link">
    <span class="md-ellipsis">
      Multimodal Foundation Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#efficiency-and-compression" class="md-nav__link">
    <span class="md-ellipsis">
      Efficiency and Compression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpretability-and-fairness" class="md-nav__link">
    <span class="md-ellipsis">
      Interpretability and Fairness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compositional-and-hierarchical-embeddings" class="md-nav__link">
    <span class="md-ellipsis">
      Compositional and Hierarchical Embeddings
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continual-learning-and-adaptation" class="md-nav__link">
    <span class="md-ellipsis">
      Continual Learning and Adaptation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="multi-modal-embeddings">Multi-modal Embeddings</h1>
<p>This module provides a unified interface for generating embeddings using various frameworks for text, image, audio, and multimodal data. It supports multiple embedding frameworks and models, making it easy to switch between different embedding solutions.</p>
<h2 id="embedding-theory-from-word-vectors-to-multimodal-representations">Embedding Theory: From Word Vectors to Multimodal Representations</h2>
<p>This section serves as an educational resource on the evolution and theory of embeddings across different modalities.</p>
<h3 id="the-evolution-of-text-embeddings">The Evolution of Text Embeddings</h3>
<h4 id="word2vec-2013">Word2Vec (2013)</h4>
<p>Word2Vec revolutionized NLP by introducing dense vector representations of words based on distributional semantics. Developed by Mikolov et al. at Google, it introduced two architectures:</p>
<ol>
<li><strong>Continuous Bag of Words (CBOW)</strong>: Predicts a target word from surrounding context words</li>
<li><strong>Skip-gram</strong>: Predicts surrounding context words given a target word</li>
</ol>
<p>The key insight was that words appearing in similar contexts tend to have similar meanings, captured by the famous equation:</p>
<div class="arithmatex">\[\vec{v}(\text{"king"}) - \vec{v}(\text{"man"}) + \vec{v}(\text{"woman"}) \approx \vec{v}(\text{"queen"})\]</div>
<h5 id="skip-gram-architecture">Skip-gram Architecture</h5>
<p>The Skip-gram model consists of:
- An input layer of one-hot encoded words
- A hidden layer with N neurons (typically 100-300 dimensions)
- An output layer using softmax to predict context words</p>
<p>The Skip-gram objective function maximizes:</p>
<div class="arithmatex">\[J(\theta) = \frac{1}{T} \sum_{t=1}^{T} \sum_{-c \leq j \leq c, j \neq 0} \log p(w_{t+j}|w_t)\]</div>
<p>where <span class="arithmatex">\(c\)</span> is the context window size and <span class="arithmatex">\(p(w_{t+j}|w_t)\)</span> is modeled using the softmax function:</p>
<div class="arithmatex">\[p(w_O|w_I) = \frac{\exp(v'_{w_O}^T v_{w_I})}{\sum_{w=1}^{W} \exp(v'_{w}^T v_{w_I})}\]</div>
<p>Here, <span class="arithmatex">\(v_{w_I}\)</span> is the input vector for word <span class="arithmatex">\(w_I\)</span> and <span class="arithmatex">\(v'_{w_O}\)</span> is the output vector for word <span class="arithmatex">\(w_O\)</span>.</p>
<h5 id="cbow-architecture">CBOW Architecture</h5>
<p>The CBOW model works in reverse, predicting a target word from context:</p>
<div class="arithmatex">\[p(w_t|w_{t-c},...,w_{t-1},w_{t+1},...,w_{t+c}) = \frac{\exp(v'_{w_t}^T \bar{v})}{\sum_{w=1}^{W} \exp(v'_{w}^T \bar{v})}\]</div>
<p>where <span class="arithmatex">\(\bar{v} = \frac{1}{2c}\sum_{-c \leq j \leq c, j \neq 0} v_{w_{t+j}}\)</span> is the average of context word vectors.</p>
<h5 id="optimization-techniques">Optimization Techniques</h5>
<p>To address computational challenges with large vocabularies, two key techniques were introduced:</p>
<ol>
<li><strong>Negative Sampling</strong>: Instead of updating all output vectors, update only the positive sample and a few (5-20) randomly selected negative samples. The objective becomes:</li>
</ol>
<div class="arithmatex">\[\log \sigma(v'_{w_O}^T v_{w_I}) + \sum_{i=1}^{k} \mathbb{E}_{w_i \sim P_n(w)}[\log \sigma(-v'_{w_i}^T v_{w_I})]\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is the sigmoid function, <span class="arithmatex">\(k\)</span> is the number of negative samples, and <span class="arithmatex">\(P_n(w)\)</span> is the noise distribution.</p>
<ol>
<li><strong>Hierarchical Softmax</strong>: Replaces the flat softmax with a binary tree structure, reducing complexity from O(V) to O(log V). Each internal node has a vector representation, and the probability of a word is the product of probabilities along the path from root to leaf:</li>
</ol>
<div class="arithmatex">\[p(w|w_I) = \prod_{j=1}^{L(w)-1} \sigma(\mathbb{1}\{n(w,j+1) = \text{left}(n(w,j))\} \cdot v'_{n(w,j)}\cdot v_{w_I})\]</div>
<p>where <span class="arithmatex">\(n(w,j)\)</span> is the <span class="arithmatex">\(j\)</span>-th node on the path from root to <span class="arithmatex">\(w\)</span>, and <span class="arithmatex">\(L(w)\)</span> is the path length.</p>
<h5 id="implementation-details">Implementation Details</h5>
<ul>
<li><strong>Subsampling</strong>: Frequent words are randomly discarded during training with probability <span class="arithmatex">\(P(w_i) = 1 - \sqrt{\frac{t}{f(w_i)}}\)</span>, where <span class="arithmatex">\(t\)</span> is a threshold (typically 10^-5) and <span class="arithmatex">\(f(w_i)\)</span> is the word frequency.</li>
<li><strong>Dynamic Context Windows</strong>: The actual window size is randomly sampled between 1 and <span class="arithmatex">\(c\)</span> for each target word.</li>
<li><strong>Learning Rate Scheduling</strong>: Decreasing learning rate as training progresses.</li>
</ul>
<p><strong>Key Papers</strong>: 
- <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a> (Mikolov et al., 2013)
- <a href="https://arxiv.org/abs/1310.4546">Distributed Representations of Words and Phrases and their Compositionality</a> (Mikolov et al., 2013)</p>
<h4 id="glove-global-vectors-for-word-representation-2014">GloVe: Global Vectors for Word Representation (2014)</h4>
<p>GloVe (Global Vectors for Word Representation) combined global matrix factorization with local context window methods. Unlike Word2Vec which is predictive, GloVe is count-based, utilizing word co-occurrence statistics from a corpus.</p>
<h5 id="mathematical-foundation">Mathematical Foundation</h5>
<p>GloVe's approach is based on the insight that ratios of co-occurrence probabilities can encode meaning. For example, the ratio of P(ice|steam)/P(ice|solid) will be small, while P(ice|water)/P(ice|solid) will be closer to 1, revealing semantic relationships.</p>
<p>The model starts by constructing a word-word co-occurrence matrix <span class="arithmatex">\(X\)</span> where <span class="arithmatex">\(X_{ij}\)</span> represents how often word <span class="arithmatex">\(i\)</span> appears in the context of word <span class="arithmatex">\(j\)</span>. The probability of word <span class="arithmatex">\(j\)</span> appearing in the context of word <span class="arithmatex">\(i\)</span> is then <span class="arithmatex">\(P_{ij} = P(j|i) = X_{ij}/X_i\)</span> where <span class="arithmatex">\(X_i = \sum_k X_{ik}\)</span>.</p>
<p>The core of GloVe is minimizing the following cost function:</p>
<div class="arithmatex">\[J = \sum_{i,j=1}^{V} f(X_{ij})(w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log X_{ij})^2\]</div>
<p>where:
- <span class="arithmatex">\(X_{ij}\)</span> is the co-occurrence count between words <span class="arithmatex">\(i\)</span> and <span class="arithmatex">\(j\)</span>
- <span class="arithmatex">\(f(X_{ij})\)</span> is a weighting function that prevents rare co-occurrences from being overweighted
- <span class="arithmatex">\(w_i\)</span> and <span class="arithmatex">\(\tilde{w}_j\)</span> are word vectors and context vectors
- <span class="arithmatex">\(b_i\)</span> and <span class="arithmatex">\(\tilde{b}_j\)</span> are bias terms</p>
<h5 id="weighting-function">Weighting Function</h5>
<p>The weighting function <span class="arithmatex">\(f(X_{ij})\)</span> is crucial for balancing the influence of frequent and rare co-occurrences:</p>
<div class="arithmatex">\[f(x) = \begin{cases}
(x/x_{\max})^\alpha &amp; \text{if } x &lt; x_{\max} \\
1 &amp; \text{otherwise}
\end{cases}\]</div>
<p>where <span class="arithmatex">\(\alpha\)</span> is typically set to 0.75 and <span class="arithmatex">\(x_{\max}\)</span> is often set to 100. This function ensures that:
- Very frequent co-occurrences are not overweighted
- Very rare co-occurrences (which may be noise) do not contribute too much to the loss
- Zero co-occurrences (<span class="arithmatex">\(X_{ij} = 0\)</span>) are excluded entirely from the optimization</p>
<h5 id="implementation-details_1">Implementation Details</h5>
<ol>
<li><strong>Co-occurrence Matrix Construction</strong>:</li>
<li>A fixed context window size (typically 10 words) is used</li>
<li>Context words are weighted by their distance from the target word (e.g., 1/d where d is the distance)</li>
<li>
<p>The matrix is symmetric if using symmetric windows</p>
</li>
<li>
<p><strong>Optimization</strong>:</p>
</li>
<li>AdaGrad is typically used for optimization</li>
<li>Learning rates around 0.05 are common</li>
<li>
<p>Vectors are typically initialized randomly with values between -0.5 and 0.5 divided by the embedding dimension</p>
</li>
<li>
<p><strong>Final Word Vectors</strong>:</p>
</li>
<li>After training, both word vectors <span class="arithmatex">\(w_i\)</span> and context vectors <span class="arithmatex">\(\tilde{w}_j\)</span> are learned</li>
<li>The final word representation is often taken as their sum or average: <span class="arithmatex">\(w_i^{final} = w_i + \tilde{w}_i\)</span></li>
</ol>
<h5 id="comparison-with-word2vec">Comparison with Word2Vec</h5>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>GloVe</th>
<th>Word2Vec</th>
</tr>
</thead>
<tbody>
<tr>
<td>Approach</td>
<td>Count-based with matrix factorization</td>
<td>Prediction-based neural network</td>
</tr>
<tr>
<td>Training Data</td>
<td>Global co-occurrence statistics</td>
<td>Local context windows</td>
</tr>
<tr>
<td>Scalability</td>
<td>Requires storing co-occurrence matrix</td>
<td>Can be trained online</td>
</tr>
<tr>
<td>Parallelization</td>
<td>Easily parallelizable</td>
<td>More challenging to parallelize</td>
</tr>
<tr>
<td>Rare Words</td>
<td>Explicitly handled by weighting function</td>
<td>Implicitly handled by subsampling</td>
</tr>
<tr>
<td>Performance</td>
<td>Often better on analogy tasks</td>
<td>Often better on similarity tasks</td>
</tr>
</tbody>
</table>
<p><strong>Key Papers</strong>: 
- <a href="https://aclanthology.org/D14-1162/">GloVe: Global Vectors for Word Representation</a> (Pennington et al., 2014)
- <a href="https://aclanthology.org/Q15-1016/">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a> (Levy et al., 2015)</p>
<h4 id="contextual-embeddings-bert-and-beyond-2018-present">Contextual Embeddings: BERT and Beyond (2018-present)</h4>
<p>BERT (Bidirectional Encoder Representations from Transformers) marked a paradigm shift from static to contextual embeddings. Unlike Word2Vec and GloVe which assign a single vector to each word, BERT produces dynamic representations based on surrounding context.</p>
<h5 id="architecture">Architecture</h5>
<p>BERT is based on the Transformer architecture, specifically using only the encoder portion. The model comes in two main variants:
- <strong>BERT-base</strong>: 12 layers, 12 attention heads, 768 hidden dimensions (110M parameters)
- <strong>BERT-large</strong>: 24 layers, 16 attention heads, 1024 hidden dimensions (340M parameters)</p>
<p>Each layer consists of:
1. <strong>Multi-head self-attention mechanism</strong>
2. <strong>Position-wise feed-forward network</strong>
3. <strong>Layer normalization and residual connections</strong></p>
<p>The input representation for each token is constructed by summing:
- <strong>Token embeddings</strong>: Learned embeddings for each token in the vocabulary
- <strong>Segment embeddings</strong>: Indicating which segment (sentence A or B) a token belongs to
- <strong>Position embeddings</strong>: Encoding the position of each token in the sequence</p>
<h5 id="self-attention-mechanism">Self-Attention Mechanism</h5>
<p>The core of BERT is the self-attention mechanism, which allows each token to attend to all other tokens in the sequence:</p>
<div class="arithmatex">\[\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div>
<p>where:
- <span class="arithmatex">\(Q = XW^Q\)</span> are the query vectors
- <span class="arithmatex">\(K = XW^K\)</span> are the key vectors
- <span class="arithmatex">\(V = XW^V\)</span> are the value vectors
- <span class="arithmatex">\(X\)</span> is the input matrix
- <span class="arithmatex">\(W^Q\)</span>, <span class="arithmatex">\(W^K\)</span>, <span class="arithmatex">\(W^V\)</span> are learned parameter matrices
- <span class="arithmatex">\(d_k\)</span> is the dimension of the key vectors (scaling factor to prevent vanishing gradients)</p>
<p>BERT uses multi-head attention, which allows the model to jointly attend to information from different representation subspaces:</p>
<div class="arithmatex">\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O\]</div>
<p>where each head is computed as:</p>
<div class="arithmatex">\[\text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)\]</div>
<h5 id="position-wise-feed-forward-network">Position-wise Feed-Forward Network</h5>
<p>After the attention layer, each position passes through an identical feed-forward network:</p>
<div class="arithmatex">\[\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\]</div>
<p>This is applied to each position separately and identically, consisting of two linear transformations with a ReLU activation in between.</p>
<h5 id="pre-training-objectives">Pre-training Objectives</h5>
<p>BERT is pre-trained using two unsupervised tasks:</p>
<ol>
<li><strong>Masked Language Modeling (MLM)</strong>:</li>
<li>Randomly mask 15% of the tokens in each sequence</li>
<li>Of these masked tokens:<ul>
<li>80% are replaced with the [MASK] token</li>
<li>10% are replaced with a random token</li>
<li>10% are left unchanged</li>
</ul>
</li>
<li>The model must predict the original token based only on its context</li>
<li>Loss function: Cross-entropy loss over the masked tokens</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(L_{\text{MLM}} = -\sum_{i \in \text{masked}} \log P(x_i | \tilde{x})\)</span>\)</span></p>
<p>where <span class="arithmatex">\(\tilde{x}\)</span> is the corrupted input and <span class="arithmatex">\(x_i\)</span> is the original token.</p>
<ol>
<li><strong>Next Sentence Prediction (NSP)</strong>:</li>
<li>Given two sentences A and B, predict whether B actually follows A in the original text</li>
<li>50% of the time B is the actual next sentence, 50% it's a random sentence</li>
<li>The [CLS] token representation is used for this binary classification task</li>
<li>Loss function: Binary cross-entropy</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(L_{\text{NSP}} = -\log P(\text{isNext} | \text{[CLS]})\)</span>\)</span></p>
<p>The total pre-training loss is the sum: <span class="arithmatex">\(L = L_{\text{MLM}} + L_{\text{NSP}}\)</span></p>
<h5 id="tokenization">Tokenization</h5>
<p>BERT uses WordPiece tokenization, a subword tokenization method that breaks uncommon words into subword units:</p>
<ol>
<li>Start with a basic vocabulary of common words</li>
<li>Iteratively add the most frequent combinations of characters</li>
<li>Tokens that are not in the vocabulary are split into subwords (marked with ##)</li>
</ol>
<p>Example: "embeddings" might be tokenized as ["em", "##bed", "##ding", "##s"]</p>
<h5 id="fine-tuning-for-downstream-tasks">Fine-tuning for Downstream Tasks</h5>
<p>BERT can be fine-tuned for various NLP tasks with minimal architecture modifications:</p>
<ul>
<li><strong>Sequence Classification</strong>: Add a classification layer on top of the [CLS] token representation</li>
<li><strong>Token Classification</strong>: Use the final hidden states of each token for tasks like NER</li>
<li><strong>Question Answering</strong>: Predict start and end positions of the answer span</li>
<li><strong>Sentence Pair Tasks</strong>: Use the [CLS] token representation with both sentences as input</li>
</ul>
<h5 id="bert-variants-and-improvements">BERT Variants and Improvements</h5>
<ul>
<li><strong>RoBERTa</strong> (Robustly Optimized BERT Approach):</li>
<li>Removes NSP objective</li>
<li>Uses dynamic masking (different masks each epoch)</li>
<li>Trains with larger batches and more data</li>
<li>
<p>Uses byte-level BPE tokenization</p>
</li>
<li>
<p><strong>DistilBERT</strong>:</p>
</li>
<li>40% smaller, 60% faster, retains 97% of BERT's performance</li>
<li>
<p>Uses knowledge distillation during pre-training</p>
</li>
<li>
<p><strong>ALBERT</strong> (A Lite BERT):</p>
</li>
<li>Parameter reduction techniques: factorized embedding parameterization and cross-layer parameter sharing</li>
<li>
<p>Replaces NSP with Sentence Order Prediction (SOP)</p>
</li>
<li>
<p><strong>ELECTRA</strong>:</p>
</li>
<li>Replaced Token Detection instead of MLM</li>
<li>Generator-Discriminator architecture for more efficient pre-training</li>
</ul>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (Devlin et al., 2018)
- <a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a> (Liu et al., 2019)
- <a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a> (Sanh et al., 2019)
- <a href="https://arxiv.org/abs/1909.11942">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a> (Lan et al., 2020)
- <a href="https://arxiv.org/abs/2003.10555">ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators</a> (Clark et al., 2020)</p>
<h4 id="sentence-embeddings-2017-present">Sentence Embeddings (2017-present)</h4>
<p>Sentence embeddings aim to represent entire sentences or paragraphs as fixed-length vectors that capture their semantic meaning. While word embeddings like Word2Vec and GloVe revolutionized word-level representations, sentence embeddings address the need for document-level understanding.</p>
<h5 id="early-approaches">Early Approaches</h5>
<ol>
<li><strong>Bag-of-Words Aggregation</strong>:</li>
<li>Simple averaging of word vectors: <span class="arithmatex">\(\vec{s} = \frac{1}{n}\sum_{i=1}^{n}\vec{w}_i\)</span></li>
<li>TF-IDF weighted averaging: <span class="arithmatex">\(\vec{s} = \frac{\sum_{i=1}^{n}\text{tfidf}(w_i)\vec{w}_i}{\sum_{i=1}^{n}\text{tfidf}(w_i)}\)</span></li>
<li>
<p>Limitations: Loses word order and complex semantic relationships</p>
</li>
<li>
<p><strong>Doc2Vec</strong> (2014):</p>
</li>
<li>Extension of Word2Vec that learns paragraph vectors alongside word vectors</li>
<li>Two variants: Distributed Memory (DM) and Distributed Bag of Words (DBOW)</li>
<li>
<p>Paragraph vectors act as a memory that captures the topic of the paragraph</p>
</li>
<li>
<p><strong>Skip-Thought Vectors</strong> (2015):</p>
</li>
<li>Uses an encoder-decoder architecture</li>
<li>Given a sentence, predicts the previous and next sentences</li>
<li>Encoder's output serves as the sentence embedding</li>
</ol>
<h5 id="transformer-based-approaches">Transformer-Based Approaches</h5>
<ol>
<li><strong>BERT [CLS] Token</strong>:</li>
<li>The [CLS] token from the final layer of BERT can represent the entire sentence</li>
<li>
<p>Limitations: Not optimized for sentence similarity; performs poorly without fine-tuning</p>
</li>
<li>
<p><strong>Sentence-BERT (SBERT)</strong> (2019):</p>
</li>
<li>Fine-tunes BERT/RoBERTa in a siamese/triplet network structure</li>
<li>Uses mean pooling over token embeddings: <span class="arithmatex">\(\vec{s} = \frac{1}{n}\sum_{i=1}^{n}\vec{t}_i\)</span></li>
<li>Dramatically improves performance and efficiency for similarity tasks</li>
</ol>
<p><strong>Architecture</strong>:
   - Identical BERT networks process sentence pairs
   - Pooling layer (usually mean pooling) aggregates token embeddings
   - Optional projection layer maps to the final embedding space</p>
<p><strong>Training Objectives</strong>:</p>
<p>a. <strong>Classification Objective</strong> (NLI datasets):
      - Given premise <span class="arithmatex">\(p\)</span> and hypothesis <span class="arithmatex">\(h\)</span>, predict entailment, contradiction, or neutral
      - Uses concatenation of embeddings: <span class="arithmatex">\([\vec{u}, \vec{v}, |\vec{u}-\vec{v}|]\)</span></p>
<p>b. <strong>Regression Objective</strong> (STS datasets):
      - Predict similarity score between sentence pairs
      - Mean squared error loss: <span class="arithmatex">\(L = (\text{sim}(\vec{u}, \vec{v}) - \text{label})^2\)</span></p>
<p>c. <strong>Triplet Objective</strong>:
      - Uses anchor <span class="arithmatex">\(a\)</span>, positive <span class="arithmatex">\(p\)</span>, and negative <span class="arithmatex">\(n\)</span> sentences
      - Contrastive loss: <span class="arithmatex">\(L(a, p, n) = \max(||f(a) - f(p)||_2 - ||f(a) - f(n)||_2 + \text{margin}, 0)\)</span></p>
<ol>
<li><strong>SimCSE</strong> (2021):</li>
<li>Uses contrastive learning with innovative positive/negative pair creation</li>
<li><strong>Unsupervised SimCSE</strong>: Uses dropout as data augmentation; the same sentence through the encoder twice creates positive pairs</li>
<li><strong>Supervised SimCSE</strong>: Uses NLI datasets where entailment pairs are positives and contradiction pairs are negatives</li>
</ol>
<p><strong>Training Objective</strong>:
   - Contrastive loss with in-batch negatives:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(L_i = -\log \frac{e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_i^+)/\tau}}{\sum_{j=1}^N e^{\text{sim}(\mathbf{h}_i, \mathbf{h}_j^+)/\tau}}\)</span>\)</span></p>
<p>where <span class="arithmatex">\(\mathbf{h}_i\)</span> and <span class="arithmatex">\(\mathbf{h}_i^+\)</span> are embeddings of positive pairs, <span class="arithmatex">\(\tau\)</span> is a temperature parameter, and <span class="arithmatex">\(N\)</span> is the batch size.</p>
<ol>
<li><strong>DeCLUTR</strong> (2021):</li>
<li>Creates positive pairs by sampling different spans from the same document</li>
<li>
<p>Uses contrastive learning with carefully designed span sampling strategies</p>
</li>
<li>
<p><strong>MPNet</strong> and <strong>E5</strong> (2022-2023):</p>
</li>
<li>MPNet combines the strengths of BERT (bidirectional context) and XLNet (permutation-based training)</li>
<li>E5 uses contrastive pre-training on web-scale data with a retrieve-then-rerank approach</li>
</ol>
<h5 id="specialized-sentence-embedding-models">Specialized Sentence Embedding Models</h5>
<ol>
<li><strong>Universal Sentence Encoder (USE)</strong>:</li>
<li>Trained on multiple tasks including NLI, question-answer prediction, and translation</li>
<li>
<p>Two variants: Transformer-based (higher accuracy) and DAN-based (faster inference)</p>
</li>
<li>
<p><strong>LaBSE (Language-agnostic BERT Sentence Embedding)</strong>:</p>
</li>
<li>Trained on 109 languages for cross-lingual sentence retrieval</li>
<li>
<p>Uses translation pairs as positive examples in contrastive learning</p>
</li>
<li>
<p><strong>GTR (Generative Text Retrieval)</strong>:</p>
</li>
<li>Uses T5 encoder for generating sentence embeddings</li>
<li>Trained with contrastive learning on MS MARCO dataset</li>
</ol>
<h5 id="practical-considerations">Practical Considerations</h5>
<ol>
<li><strong>Pooling Strategies</strong>:</li>
<li>Mean pooling: Average of all token embeddings (most common)</li>
<li>Max pooling: Element-wise maximum across token embeddings</li>
<li>CLS pooling: Using only the [CLS] token embedding</li>
<li>
<p>Attention pooling: Weighted average using learned attention weights</p>
</li>
<li>
<p><strong>Normalization</strong>:</p>
</li>
<li>L2 normalization is crucial for cosine similarity calculations</li>
<li>
<p>Some models apply layer normalization before pooling</p>
</li>
<li>
<p><strong>Hard Negative Mining</strong>:</p>
</li>
<li>Finding challenging negative examples improves model performance</li>
<li>Techniques include in-batch negatives, cross-batch negatives, and iterative mining</li>
</ol>
<h5 id="sentencetransformers-framework">SentenceTransformers Framework</h5>
<p><strong>SentenceTransformers</strong> is the most widely adopted framework for sentence embeddings, providing a unified interface for training and using sentence embedding models. Developed by Nils Reimers, it has become the de facto standard for sentence embedding applications.</p>
<p><strong>Architecture and Design</strong>:
- <strong>Modular Design</strong>: Supports various transformer models (BERT, RoBERTa, DistilBERT, etc.) as backbone encoders
- <strong>Flexible Pooling</strong>: Multiple pooling strategies (mean, max, CLS token, weighted mean)
- <strong>Training Pipeline</strong>: Streamlined training with various loss functions and evaluation metrics
- <strong>Model Hub Integration</strong>: Seamless integration with Hugging Face Model Hub</p>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/UKPLab/sentence-transformers">SentenceTransformers GitHub</a></p>
<p><strong>Key Components</strong>:</p>
<ol>
<li>
<p><strong>SentenceTransformer Class</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># Core implementation in sentence_transformers/SentenceTransformer.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SentenceTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">modules</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># Initialize transformer model and pooling layer</span>
</code></pre></div>
   <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/SentenceTransformer.py#L89">Implementation</a></p>
</li>
<li>
<p><strong>Pooling Strategies</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/models/Pooling.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Pooling</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_embedding_dimension</span><span class="p">,</span> <span class="n">pooling_mode</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="c1"># Implements mean, max, cls pooling strategies</span>
</code></pre></div>
   <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Pooling.py">Implementation</a></p>
</li>
</ol>
<h5 id="all-minilm-l6-v2-deep-dive-analysis">all-MiniLM-L6-v2: Deep Dive Analysis</h5>
<p><strong>all-MiniLM-L6-v2</strong> is one of the most popular sentence embedding models, offering an excellent balance between performance and efficiency. It's based on the MiniLM architecture with specific optimizations for sentence-level tasks.</p>
<p><strong>Architecture Details</strong>:
- <strong>Base Model</strong>: DistilBERT-like architecture with 6 layers
- <strong>Hidden Size</strong>: 384 dimensions
- <strong>Attention Heads</strong>: 12
- <strong>Parameters</strong>: ~23M (significantly smaller than BERT-base's 110M)
- <strong>Max Sequence Length</strong>: 512 tokens
- <strong>Output Dimensions</strong>: 384-dimensional sentence embeddings</p>
<p><strong>Training Process</strong>:</p>
<ol>
<li><strong>Knowledge Distillation</strong>: Trained using knowledge distillation from larger teacher models</li>
<li>Teacher models: Multiple large sentence embedding models</li>
<li>Student model: 6-layer MiniLM architecture</li>
<li>
<p>Distillation loss combines multiple objectives</p>
</li>
<li>
<p><strong>Multi-Task Training</strong>: Trained on diverse datasets:</p>
</li>
<li><strong>Natural Language Inference</strong>: SNLI, MultiNLI, XNLI</li>
<li><strong>Semantic Textual Similarity</strong>: STS benchmark datasets</li>
<li><strong>Question-Answer Pairs</strong>: Quora, Stack Exchange, MS MARCO</li>
<li>
<p><strong>Paraphrase Detection</strong>: Various paraphrase datasets</p>
</li>
<li>
<p><strong>Training Objective</strong>:
   <div class="highlight"><pre><span></span><code><span class="c1"># Simplified training objective combining multiple losses</span>
<span class="n">total_loss</span> <span class="o">=</span> <span class="n"></span><span class="err"></span> <span class="o">*</span> <span class="n">nli_loss</span> <span class="o">+</span> <span class="n"></span><span class="err"></span> <span class="o">*</span> <span class="n">sts_loss</span> <span class="o">+</span> <span class="n"></span><span class="err"></span> <span class="o">*</span> <span class="n">qa_loss</span> <span class="o">+</span> <span class="n"></span><span class="err"></span> <span class="o">*</span> <span class="n">distillation_loss</span>
</code></pre></div></p>
</li>
</ol>
<p><strong>Performance Characteristics</strong>:
- <strong>Speed</strong>: ~5x faster than BERT-base for inference
- <strong>Memory</strong>: ~4x less memory usage
- <strong>Quality</strong>: Retains ~95% of larger model performance on most tasks
- <strong>Versatility</strong>: Excellent performance across multiple domains and languages</p>
<p><strong>Model Card</strong>: <a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">all-MiniLM-L6-v2 on Hugging Face</a></p>
<p><strong>Usage Example</strong>:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">sentence_transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="c1"># Load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>

<span class="c1"># Generate embeddings</span>
<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This is an example sentence&#39;</span><span class="p">,</span> <span class="s1">&#39;Each sentence is converted&#39;</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sentences</span><span class="p">)</span>
</code></pre></div></p>
<h5 id="siamese-and-triplet-network-architectures">Siamese and Triplet Network Architectures</h5>
<p><strong>Siamese Networks</strong> and <strong>Triplet Networks</strong> are fundamental architectures for learning similarity-based embeddings, particularly effective for sentence embeddings.</p>
<p><strong>Siamese Network Architecture</strong>:</p>
<p>A Siamese network consists of two identical neural networks (sharing weights) that process two inputs simultaneously:</p>
<div class="highlight"><pre><span></span><code>Input A  [Encoder]  Embedding A
                
                 (shared weights)
                
Input B  [Encoder]  Embedding B
                
                
        [Similarity Function]
                
                
            Similarity Score
</code></pre></div>
<p><strong>Implementation Steps</strong>:</p>
<ol>
<li>
<p><strong>Shared Encoder</strong>: Both inputs pass through the same transformer encoder
   <div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/models/Transformer.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Transformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">):</span>
        <span class="c1"># Process input through transformer layers</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_model</span><span class="p">(</span><span class="o">**</span><span class="n">features</span><span class="p">)</span>
</code></pre></div>
   <a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/models/Transformer.py">Implementation</a></p>
</li>
<li>
<p><strong>Pooling Layer</strong>: Convert token embeddings to sentence embeddings</p>
</li>
<li><strong>Similarity Computation</strong>: Calculate cosine similarity or Euclidean distance</li>
</ol>
<p><strong>Triplet Network Architecture</strong>:</p>
<p>Triplet networks extend Siamese networks to work with three inputs: anchor, positive, and negative examples:</p>
<div class="highlight"><pre><span></span><code>Anchor  [Encoder]  Embedding A
Positive  [Encoder]  Embedding P  
Negative  [Encoder]  Embedding N
                
                
        [Triplet Loss Function]
</code></pre></div>
<p><strong>Training Process</strong>:
1. <strong>Triplet Mining</strong>: Select challenging triplets (hard negatives)
2. <strong>Forward Pass</strong>: Generate embeddings for all three inputs
3. <strong>Loss Calculation</strong>: Apply triplet loss function
4. <strong>Backpropagation</strong>: Update shared encoder weights</p>
<h5 id="loss-functions-for-sentence-embeddings">Loss Functions for Sentence Embeddings</h5>
<p><strong>1. Triplet Loss</strong></p>
<p>Triplet loss ensures that the distance between anchor and positive is smaller than the distance between anchor and negative by a margin:</p>
<div class="arithmatex">\[L_{\text{triplet}}(a, p, n) = \max(0, d(a, p) - d(a, n) + \text{margin})\]</div>
<p>where:
- <span class="arithmatex">\(a\)</span>, <span class="arithmatex">\(p\)</span>, <span class="arithmatex">\(n\)</span> are anchor, positive, and negative embeddings
- <span class="arithmatex">\(d(\cdot, \cdot)\)</span> is the distance function (usually Euclidean or cosine)
- <span class="arithmatex">\(\text{margin}\)</span> is a hyperparameter (typically 0.5)</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/losses/TripletLoss.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TripletLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">distance_metric</span><span class="o">=</span><span class="n">SiameseDistanceMetric</span><span class="o">.</span><span class="n">COSINE</span><span class="p">,</span> <span class="n">triplet_margin</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="c1"># Initialize triplet loss with specified distance metric and margin</span>
</code></pre></div>
<a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/TripletLoss.py">Implementation</a></p>
<p><strong>Triplet Mining Strategies</strong>:
- <strong>Random Triplets</strong>: Randomly sample triplets from the dataset
- <strong>Hard Triplets</strong>: Select triplets where the negative is closer to anchor than positive
- <strong>Semi-Hard Triplets</strong>: Negatives that are farther than positive but within the margin
- <strong>Online Mining</strong>: Mine triplets during training based on current model state</p>
<p><strong>2. Contrastive Loss</strong></p>
<p>Contrastive loss works with pairs of examples, pulling similar pairs together and pushing dissimilar pairs apart:</p>
<div class="arithmatex">\[L_{\text{contrastive}}(x_1, x_2, y) = y \cdot d(x_1, x_2)^2 + (1-y) \cdot \max(0, \text{margin} - d(x_1, x_2))^2\]</div>
<p>where:
- <span class="arithmatex">\(y = 1\)</span> for similar pairs, <span class="arithmatex">\(y = 0\)</span> for dissimilar pairs
- <span class="arithmatex">\(d(x_1, x_2)\)</span> is the Euclidean distance between embeddings
- <span class="arithmatex">\(\text{margin}\)</span> defines the minimum distance for dissimilar pairs</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/losses/ContrastiveLoss.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ContrastiveLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">distance_metric</span><span class="o">=</span><span class="n">SiameseDistanceMetric</span><span class="o">.</span><span class="n">EUCLIDEAN</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="c1"># Initialize contrastive loss with distance metric and margin</span>
</code></pre></div>
<a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/ContrastiveLoss.py">Implementation</a></p>
<p><strong>3. Multiple Negatives Ranking Loss (MNRL)</strong></p>
<p>MNRL is a more efficient alternative to triplet loss, using in-batch negatives to create multiple negative examples:</p>
<div class="arithmatex">\[L_{\text{MNRL}} = -\log \frac{e^{\text{sim}(a, p)/\tau}}{e^{\text{sim}(a, p)/\tau} + \sum_{i=1}^{N} e^{\text{sim}(a, n_i)/\tau}}\]</div>
<p>where:
- <span class="arithmatex">\(a\)</span> is the anchor (query)
- <span class="arithmatex">\(p\)</span> is the positive example
- <span class="arithmatex">\(n_i\)</span> are negative examples (other examples in the batch)
- <span class="arithmatex">\(\tau\)</span> is the temperature parameter
- <span class="arithmatex">\(\text{sim}(\cdot, \cdot)\)</span> is the similarity function (usually cosine similarity)</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/losses/MultipleNegativesRankingLoss.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">MultipleNegativesRankingLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">20.0</span><span class="p">,</span> <span class="n">similarity_fct</span><span class="o">=</span><span class="n">util</span><span class="o">.</span><span class="n">cos_sim</span><span class="p">):</span>
        <span class="c1"># Initialize MNRL with scaling factor and similarity function</span>
</code></pre></div>
<a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py">Implementation</a></p>
<p><strong>Advantages of MNRL</strong>:
- <strong>Efficiency</strong>: Uses all examples in a batch as negatives
- <strong>Scalability</strong>: No need for explicit negative sampling
- <strong>Performance</strong>: Often outperforms triplet loss with proper batch size
- <strong>Simplicity</strong>: Easier to implement and tune than triplet mining strategies</p>
<p><strong>4. CoSENT Loss</strong></p>
<p>CoSENT (Cosine Sentence) loss is designed specifically for sentence similarity tasks:</p>
<div class="arithmatex">\[L_{\text{CoSENT}} = \log(1 + \sum_{i=1}^{N} \sum_{j=1}^{N} \mathbb{1}_{y_i &lt; y_j} e^{\lambda(\cos(u_i, v_i) - \cos(u_j, v_j))})\]</div>
<p>where:
- <span class="arithmatex">\((u_i, v_i)\)</span> and <span class="arithmatex">\((u_j, v_j)\)</span> are sentence pairs
- <span class="arithmatex">\(y_i\)</span> and <span class="arithmatex">\(y_j\)</span> are their similarity labels
- <span class="arithmatex">\(\lambda\)</span> is a scaling factor
- <span class="arithmatex">\(\cos(\cdot, \cdot)\)</span> is cosine similarity</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># sentence_transformers/losses/CoSENTLoss.py</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CoSENTLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">20.0</span><span class="p">):</span>
        <span class="c1"># Initialize CoSENT loss with scaling parameter</span>
</code></pre></div>
<a href="https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/CoSENTLoss.py">Implementation</a></p>
<h5 id="advanced-training-techniques">Advanced Training Techniques</h5>
<p><strong>1. Hard Negative Mining</strong></p>
<p>Hard negative mining improves model performance by focusing on challenging examples:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example implementation of hard negative mining</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mine_hard_negatives</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">anchors</span><span class="p">,</span> <span class="n">candidates</span><span class="p">,</span> <span class="n">top_k</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="c1"># Encode all sentences</span>
    <span class="n">anchor_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">anchors</span><span class="p">)</span>
    <span class="n">candidate_embeddings</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">candidates</span><span class="p">)</span>

    <span class="c1"># Compute similarities</span>
    <span class="n">similarities</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">cos_sim</span><span class="p">(</span><span class="n">anchor_embeddings</span><span class="p">,</span> <span class="n">candidate_embeddings</span><span class="p">)</span>

    <span class="c1"># Select top-k most similar negatives (hardest negatives)</span>
    <span class="n">hard_negatives</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">similarities</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">top_k</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">indices</span>
    <span class="k">return</span> <span class="n">hard_negatives</span>
</code></pre></div>
<p><strong>2. Curriculum Learning</strong></p>
<p>Gradually increase training difficulty by starting with easy examples and progressing to harder ones:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Curriculum learning implementation</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CurriculumSampler</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">difficulty_scores</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">difficulty_scores</span> <span class="o">=</span> <span class="n">difficulty_scores</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_threshold</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Start with easiest 10%</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
        <span class="c1"># Gradually increase difficulty threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_threshold</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="n">epoch</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="c1"># Sample examples below difficulty threshold</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_by_difficulty</span><span class="p">()</span>
</code></pre></div>
<p><strong>3. Data Augmentation for Sentence Embeddings</strong></p>
<ul>
<li><strong>Back-translation</strong>: Translate to another language and back</li>
<li><strong>Paraphrasing</strong>: Use paraphrase generation models</li>
<li><strong>Token-level augmentation</strong>: Random insertion, deletion, substitution</li>
<li><strong>Dropout augmentation</strong>: Different dropout masks for the same sentence</li>
</ul>
<p><strong>Research Directions and Future Work</strong>:</p>
<ol>
<li><strong>Multilingual Sentence Embeddings</strong>:</li>
<li>Cross-lingual alignment techniques</li>
<li>Language-agnostic representation learning</li>
<li>Zero-shot cross-lingual transfer</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/2007.01852">LaBSE</a>, <a href="https://arxiv.org/abs/1812.10464">LASER</a></p>
</li>
<li>
<p><strong>Domain Adaptation</strong>:</p>
</li>
<li>Unsupervised domain adaptation for embeddings</li>
<li>Few-shot learning for new domains</li>
<li>Domain-adversarial training</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/2004.02349">Domain Adaptation</a></p>
</li>
<li>
<p><strong>Efficient Training Methods</strong>:</p>
</li>
<li>Knowledge distillation for smaller models</li>
<li>Progressive training strategies</li>
<li>Mixed precision training</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a>, <a href="https://arxiv.org/abs/1909.10351">TinyBERT</a></p>
</li>
<li>
<p><strong>Evaluation and Benchmarking</strong>:</p>
</li>
<li>Comprehensive evaluation frameworks</li>
<li>Bias detection in sentence embeddings</li>
<li>Robustness testing</li>
<li>Papers: <a href="https://arxiv.org/abs/1803.05449">SentEval</a>, <a href="https://arxiv.org/abs/2210.07316">MTEB</a></li>
</ol>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/1908.10084">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</a> (Reimers &amp; Gurevych, 2019)
- <a href="https://arxiv.org/abs/2104.08821">SimCSE: Simple Contrastive Learning of Sentence Embeddings</a> (Gao et al., 2021)
- <a href="https://arxiv.org/abs/2006.03659">DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations</a> (Giorgi et al., 2021)
- <a href="https://arxiv.org/abs/2212.03533">E5: Text Embeddings by Weakly-Supervised Contrastive Pre-training</a> (Wang et al., 2022)
- <a href="https://arxiv.org/abs/2201.10005">Text and Code Embeddings by Contrastive Pre-Training</a> (Neelakantan et al., 2022)
- <a href="https://arxiv.org/abs/2004.09813">Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation</a> (Reimers &amp; Gurevych, 2020)
- <a href="https://arxiv.org/abs/2210.07316">MTEB: Massive Text Embedding Benchmark</a> (Muennighoff et al., 2022)</p>
<h4 id="decoder-based-embeddings-gpt-and-beyond-2018-present">Decoder-Based Embeddings: GPT and Beyond (2018-present)</h4>
<p>While encoder models like BERT excel at understanding, decoder models like GPT (Generative Pre-trained Transformer) excel at generation. Interestingly, these decoder-based models can also produce high-quality embeddings, despite their architectural differences from traditional embedding models.</p>
<h5 id="architecture-of-decoder-based-models">Architecture of Decoder-Based Models</h5>
<p>GPT and similar decoder-based models use a unidirectional (autoregressive) architecture:</p>
<ol>
<li><strong>Causal Self-Attention</strong>: Each token can only attend to itself and previous tokens, implemented using an attention mask:</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(\text{CausalAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + M}{\sqrt{d_k}}\right)V\)</span>\)</span></p>
<p>where <span class="arithmatex">\(M\)</span> is a mask that sets all values corresponding to future positions to <span class="arithmatex">\(-\infty\)</span>:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(M_{ij} = \begin{cases}
   0 &amp; \text{if } i \geq j \\
   -\infty &amp; \text{if } i &lt; j
   \end{cases}\)</span>\)</span></p>
<ol>
<li>
<p><strong>Position-wise Feed-Forward Network</strong>: Similar to BERT, but with potentially different activation functions (e.g., GPT-2 uses GELU instead of ReLU).</p>
</li>
<li>
<p><strong>Layer Normalization</strong>: Applied before each sub-layer, rather than after (pre-norm vs. post-norm).</p>
</li>
</ol>
<h5 id="gpt-family-evolution">GPT Family Evolution</h5>
<ol>
<li><strong>GPT-1</strong> (2018):</li>
<li>12 layers, 768 hidden dimensions, 12 attention heads (117M parameters)</li>
<li>Pre-trained on BookCorpus (800M words)</li>
<li>
<p>Fine-tuned on specific downstream tasks</p>
</li>
<li>
<p><strong>GPT-2</strong> (2019):</p>
</li>
<li>Scaled up to 1.5B parameters in largest variant</li>
<li>Pre-trained on WebText (40GB of text from 8M web pages)</li>
<li>
<p>Zero-shot task transfer without fine-tuning</p>
</li>
<li>
<p><strong>GPT-3</strong> (2020):</p>
</li>
<li>Massive scale-up to 175B parameters</li>
<li>Pre-trained on Common Crawl, WebText2, Books1, Books2, and Wikipedia</li>
<li>
<p>Few-shot learning capabilities through in-context learning</p>
</li>
<li>
<p><strong>GPT-4</strong> (2023):</p>
</li>
<li>Multimodal capabilities (text and images)</li>
<li>Further scaling and architectural improvements</li>
<li>Significantly improved reasoning capabilities</li>
</ol>
<h5 id="embedding-generation-approaches">Embedding Generation Approaches</h5>
<ol>
<li><strong>Last Hidden State</strong>:</li>
<li>The simplest approach is to use the final hidden state of the last token as the sentence embedding</li>
<li>
<p>Limitation: Heavily biased toward the last tokens in the sequence</p>
</li>
<li>
<p><strong>Mean Pooling</strong>:</p>
</li>
<li>Average the hidden states across all tokens</li>
<li>
<p>More balanced representation of the entire sequence</p>
</li>
<li>
<p><strong>Specialized Embedding Models</strong>:</p>
</li>
<li>OpenAI's <code>text-embedding-ada-002</code> is based on a GPT-like architecture but specifically trained for embedding generation</li>
<li>
<p>Uses contrastive learning objectives similar to those in SimCSE</p>
</li>
<li>
<p><strong>Instruction Tuning</strong>:</p>
</li>
<li>Models like <code>text-embedding-3-large</code> are instruction-tuned to produce embeddings optimized for specific use cases</li>
<li>Can generate different embeddings for the same text based on the provided instruction</li>
</ol>
<h5 id="training-objectives-for-embedding-generation">Training Objectives for Embedding Generation</h5>
<ol>
<li><strong>Contrastive Learning</strong>:</li>
<li>Similar to encoder-based models, using positive and negative pairs</li>
<li>
<p>Often uses retrieval-based tasks during training</p>
</li>
<li>
<p><strong>Dual Encoder Training</strong>:</p>
</li>
<li>Training separate query and document encoders</li>
<li>
<p>Optimizing for retrieval performance</p>
</li>
<li>
<p><strong>Multi-task Learning</strong>:</p>
</li>
<li>Combining generative pre-training with embedding-specific objectives</li>
<li>Balancing between generation quality and embedding quality</li>
</ol>
<h5 id="applications-of-decoder-based-embeddings">Applications of Decoder-Based Embeddings</h5>
<ol>
<li><strong>Semantic Search</strong>:</li>
<li>OpenAI's embeddings are widely used for retrieval-augmented generation (RAG)</li>
<li>
<p>Can capture nuanced semantic relationships better than some encoder-only models</p>
</li>
<li>
<p><strong>Zero-shot Classification</strong>:</p>
</li>
<li>Using embeddings to compare inputs with potential class descriptions</li>
<li>
<p>Leveraging the model's world knowledge encoded in the embeddings</p>
</li>
<li>
<p><strong>Content Recommendation</strong>:</p>
</li>
<li>Representing user preferences and content in the same embedding space</li>
<li>
<p>Capturing subtle semantic relationships for better recommendations</p>
</li>
<li>
<p><strong>Embedding-guided Generation</strong>:</p>
</li>
<li>Using embeddings to guide text generation toward specific semantic goals</li>
<li>Controlling style, tone, or content through embedding space manipulation</li>
</ol>
<h5 id="advantages-of-decoder-based-embeddings">Advantages of Decoder-Based Embeddings</h5>
<ol>
<li>
<p><strong>World Knowledge</strong>: Large decoder models encode vast amounts of world knowledge that can be reflected in their embeddings</p>
</li>
<li>
<p><strong>Contextual Understanding</strong>: Strong ability to disambiguate based on context</p>
</li>
<li>
<p><strong>Adaptability</strong>: Can be prompted or fine-tuned to produce embeddings for specific domains or tasks</p>
</li>
<li>
<p><strong>Alignment with Generation</strong>: When used in retrieval-augmented generation, embeddings from the same model family can provide better alignment</p>
</li>
</ol>
<h5 id="challenges-and-limitations">Challenges and Limitations</h5>
<ol>
<li>
<p><strong>Computational Cost</strong>: Larger models require significant resources</p>
</li>
<li>
<p><strong>Unidirectionality</strong>: The causal attention mechanism may limit bidirectional understanding</p>
</li>
<li>
<p><strong>Embedding Drift</strong>: Embeddings from different versions of models may not be compatible</p>
</li>
<li>
<p><strong>Black-box Nature</strong>: Commercial embeddings like those from OpenAI have limited transparency</p>
</li>
</ol>
<h5 id="embedding-extraction-from-decoder-models">Embedding Extraction from Decoder Models</h5>
<p><strong>Last Token Embeddings</strong>:
For decoder models, embeddings are typically extracted from the last token's hidden state:</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Example with Hugging Face Transformers</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">GPT2Model</span><span class="p">,</span> <span class="n">GPT2Tokenizer</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">GPT2Tokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;gpt2&#39;</span><span class="p">)</span>

<span class="c1"># Add padding token</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_gpt_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Extract last token embeddings</span>
    <span class="n">last_token_embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
    <span class="k">return</span> <span class="n">last_token_embeddings</span>
</code></pre></div>
<p><strong>Mean Pooling for Decoder Models</strong>:
Alternatively, mean pooling can be applied to all token embeddings:</p>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_gpt_embeddings_mean_pooled</span><span class="p">(</span><span class="n">texts</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="c1"># Apply attention mask and mean pool</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span>
    <span class="n">masked_embeddings</span> <span class="o">=</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mean_embeddings</span> <span class="o">=</span> <span class="n">masked_embeddings</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mean_embeddings</span>
</code></pre></div>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/gpt2">Hugging Face Transformers GPT Models</a></p>
<h5 id="openai-text-embeddings-api">OpenAI Text Embeddings API</h5>
<p>OpenAI provides specialized embedding models optimized for various tasks:</p>
<p><strong>text-embedding-ada-002</strong>:
- 1536-dimensional embeddings
- Optimized for semantic search and similarity tasks
- Cost-effective and high-performance</p>
<p><strong>text-embedding-3-small</strong> and <strong>text-embedding-3-large</strong>:
- Newer models with improved performance
- Configurable output dimensions
- Better multilingual support</p>
<div class="highlight"><pre><span></span><code><span class="c1"># OpenAI Embeddings API usage</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">openai</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_openai_embeddings</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s2">&quot;text-embedding-3-small&quot;</span><span class="p">):</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="nb">input</span><span class="o">=</span><span class="n">texts</span><span class="p">,</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;embedding&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">response</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]]</span>
</code></pre></div>
<p><strong>API Documentation</strong>: <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI Embeddings API</a></p>
<p><strong>Key Papers and Resources</strong>:
- <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> (Radford et al., 2018)
- <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> (Radford et al., 2019)
- <a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (Brown et al., 2020)
- <a href="https://arxiv.org/abs/2401.00368">Improving Text Embeddings with Large Language Models</a> (Neelakantan et al., 2024)
- <a href="https://platform.openai.com/docs/guides/embeddings">OpenAI Embeddings Documentation</a></p>
<h3 id="multimodal-embeddings">Multimodal Embeddings</h3>
<p>Multimodal embeddings extend beyond text to incorporate visual, audio, and other modalities, enabling cross-modal understanding and retrieval.</p>
<h4 id="vision-language-models">Vision-Language Models</h4>
<h5 id="clip-contrastive-language-image-pre-training-2021">CLIP: Contrastive Language-Image Pre-training (2021)</h5>
<p><strong>CLIP</strong> revolutionized multimodal understanding by learning joint representations of images and text through contrastive learning.</p>
<p><strong>Architecture</strong>:
- <strong>Text Encoder</strong>: Transformer-based (similar to GPT-2)
- <strong>Image Encoder</strong>: Vision Transformer (ViT) or ResNet
- <strong>Joint Embedding Space</strong>: Both modalities mapped to the same dimensional space</p>
<p><strong>Training Objective</strong>:
CLIP uses contrastive learning on image-text pairs:</p>
<div class="arithmatex">\[L = -\frac{1}{N} \sum_{i=1}^{N} \log \frac{\exp(\text{sim}(I_i, T_i) / \tau)}{\sum_{j=1}^{N} \exp(\text{sim}(I_i, T_j) / \tau)}\]</div>
<p>where:
- <span class="arithmatex">\(I_i\)</span> and <span class="arithmatex">\(T_i\)</span> are image and text embeddings for the <span class="arithmatex">\(i\)</span>-th pair
- <span class="arithmatex">\(\text{sim}(\cdot, \cdot)\)</span> is cosine similarity
- <span class="arithmatex">\(\tau\)</span> is a learnable temperature parameter
- <span class="arithmatex">\(N\)</span> is the batch size</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># Using OpenAI&#39;s CLIP</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">clip</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Load model</span>
<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
<span class="n">model</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">clip</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;ViT-B/32&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Process image and text</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;image.jpg&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">clip</span><span class="o">.</span><span class="n">tokenize</span><span class="p">([</span><span class="s2">&quot;a photo of a cat&quot;</span><span class="p">,</span> <span class="s2">&quot;a photo of a dog&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Generate embeddings</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">image_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_image</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">text_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>

    <span class="c1"># Normalize features</span>
    <span class="n">image_features</span> <span class="o">/=</span> <span class="n">image_features</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">text_features</span> <span class="o">/=</span> <span class="n">text_features</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Calculate similarity</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="p">(</span><span class="mf">100.0</span> <span class="o">*</span> <span class="n">image_features</span> <span class="o">@</span> <span class="n">text_features</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></p>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/openai/CLIP">OpenAI CLIP GitHub</a></p>
<p><strong>Key Features</strong>:
- <strong>Zero-shot Classification</strong>: Can classify images without task-specific training
- <strong>Cross-modal Retrieval</strong>: Find images using text queries and vice versa
- <strong>Robust Representations</strong>: Learned from 400M image-text pairs from the internet</p>
<h5 id="vision-transformer-vit-for-image-embeddings">Vision Transformer (ViT) for Image Embeddings</h5>
<p><strong>Vision Transformer</strong> applies the transformer architecture directly to image patches, treating them as sequences.</p>
<p><strong>Architecture</strong>:
1. <strong>Patch Embedding</strong>: Divide image into fixed-size patches and linearly embed them
2. <strong>Position Embedding</strong>: Add learnable position embeddings to patch embeddings
3. <strong>Transformer Encoder</strong>: Standard transformer layers with self-attention
4. <strong>Classification Head</strong>: MLP head for classification or embedding extraction</p>
<p><strong>Patch Embedding Process</strong>:
<div class="highlight"><pre><span></span><code><span class="c1"># Simplified ViT patch embedding</span>
<span class="k">def</span><span class="w"> </span><span class="nf">create_patch_embeddings</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">patch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="c1"># image shape: (batch_size, channels, height, width)</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span>

    <span class="c1"># Calculate number of patches</span>
    <span class="n">num_patches_h</span> <span class="o">=</span> <span class="n">height</span> <span class="o">//</span> <span class="n">patch_size</span>
    <span class="n">num_patches_w</span> <span class="o">=</span> <span class="n">width</span> <span class="o">//</span> <span class="n">patch_size</span>

    <span class="c1"># Reshape to patches</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span><span class="o">.</span><span class="n">unfold</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">channels</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">)</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">channels</span> <span class="o">*</span> <span class="n">patch_size</span> <span class="o">*</span> <span class="n">patch_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">patches</span>
</code></pre></div></p>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/vit">Hugging Face ViT</a></p>
<p><strong>Usage Example</strong>:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">ViTModel</span><span class="p">,</span> <span class="n">ViTFeatureExtractor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Load model and feature extractor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ViTModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;google/vit-base-patch16-224&#39;</span><span class="p">)</span>
<span class="n">feature_extractor</span> <span class="o">=</span> <span class="n">ViTFeatureExtractor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;google/vit-base-patch16-224&#39;</span><span class="p">)</span>

<span class="c1"># Process image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s1">&#39;image.jpg&#39;</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">feature_extractor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

<span class="c1"># Generate embeddings</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
    <span class="c1"># Use CLS token embedding</span>
    <span class="n">image_embedding</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
</code></pre></div></p>
<h4 id="audio-embeddings">Audio Embeddings</h4>
<h5 id="wav2vec-20-self-supervised-audio-representations">Wav2Vec 2.0: Self-Supervised Audio Representations</h5>
<p><strong>Wav2Vec 2.0</strong> learns powerful audio representations through self-supervised learning on raw audio waveforms.</p>
<p><strong>Architecture</strong>:
1. <strong>Feature Encoder</strong>: CNN layers that process raw audio
2. <strong>Contextualized Representations</strong>: Transformer layers for sequence modeling
3. <strong>Quantization Module</strong>: Discretizes latent representations</p>
<p><strong>Training Objective</strong>:
Contrastive learning with masked prediction:</p>
<div class="arithmatex">\[L = -\log \frac{\exp(\text{sim}(c_t, q_t) / \tau)}{\sum_{\tilde{q} \in Q_t} \exp(\text{sim}(c_t, \tilde{q}) / \tau)}\]</div>
<p>where:
- <span class="arithmatex">\(c_t\)</span> is the contextualized representation at time step <span class="arithmatex">\(t\)</span>
- <span class="arithmatex">\(q_t\)</span> is the quantized target representation
- <span class="arithmatex">\(Q_t\)</span> is the set of distractors</p>
<p><strong>Implementation</strong>:
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">Wav2Vec2Model</span><span class="p">,</span> <span class="n">Wav2Vec2Processor</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">librosa</span>

<span class="c1"># Load model and processor</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Wav2Vec2Model</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">Wav2Vec2Processor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/wav2vec2-base&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_audio_embeddings</span><span class="p">(</span><span class="n">audio_path</span><span class="p">):</span>
    <span class="c1"># Load audio</span>
    <span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">audio_path</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>

    <span class="c1"># Process audio</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">audio</span><span class="p">,</span> <span class="n">sampling_rate</span><span class="o">=</span><span class="mi">16000</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

    <span class="c1"># Generate embeddings</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="c1"># Mean pool over time dimension</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">last_hidden_state</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">embeddings</span>
</code></pre></div></p>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/wav2vec2">Hugging Face Wav2Vec2</a></p>
<h5 id="openai-whisper-for-audio-understanding">OpenAI Whisper for Audio Understanding</h5>
<p><strong>Whisper</strong> is a robust speech recognition model that can also provide audio embeddings:</p>
<div class="highlight"><pre><span></span><code><span class="kn">import</span><span class="w"> </span><span class="nn">whisper</span>

<span class="c1"># Load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;base&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_whisper_embeddings</span><span class="p">(</span><span class="n">audio_path</span><span class="p">):</span>
    <span class="c1"># Load and process audio</span>
    <span class="n">audio</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">load_audio</span><span class="p">(</span><span class="n">audio_path</span><span class="p">)</span>
    <span class="n">audio</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">pad_or_trim</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

    <span class="c1"># Generate mel spectrogram</span>
    <span class="n">mel</span> <span class="o">=</span> <span class="n">whisper</span><span class="o">.</span><span class="n">log_mel_spectrogram</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Encode audio</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">audio_features</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">mel</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">audio_features</span>
</code></pre></div>
<p><strong>Implementation Reference</strong>: <a href="https://github.com/openai/whisper">OpenAI Whisper GitHub</a></p>
<h4 id="multimodal-fusion-techniques">Multimodal Fusion Techniques</h4>
<h5 id="early-fusion">Early Fusion</h5>
<p>Combine features from different modalities at the input level:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">EarlyFusionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_dim</span><span class="p">,</span> <span class="n">image_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">text_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">image_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_features</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="n">text_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_proj</span><span class="p">(</span><span class="n">text_features</span><span class="p">)</span>
        <span class="n">image_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_proj</span><span class="p">(</span><span class="n">image_features</span><span class="p">)</span>

        <span class="c1"># Concatenate and fuse</span>
        <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">text_proj</span><span class="p">,</span> <span class="n">image_proj</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fusion_layer</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<h5 id="late-fusion">Late Fusion</h5>
<p>Combine predictions from separate modality-specific models:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">LateFusionModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_model</span><span class="p">,</span> <span class="n">image_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">text_model</span> <span class="o">=</span> <span class="n">text_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">image_model</span> <span class="o">=</span> <span class="n">image_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fusion_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_input</span><span class="p">,</span> <span class="n">image_input</span><span class="p">):</span>
        <span class="n">text_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">text_model</span><span class="p">(</span><span class="n">text_input</span><span class="p">)</span>
        <span class="n">image_logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">image_model</span><span class="p">(</span><span class="n">image_input</span><span class="p">)</span>

        <span class="c1"># Weighted combination</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fusion_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">fused_logits</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">text_logits</span> <span class="o">+</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">image_logits</span>

        <span class="k">return</span> <span class="n">fused_logits</span>
</code></pre></div>
<h5 id="cross-attention-fusion">Cross-Attention Fusion</h5>
<p>Use attention mechanisms to model cross-modal interactions:</p>
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">CrossAttentionFusion</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MultiheadAttention</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_features</span><span class="p">,</span> <span class="n">image_features</span><span class="p">):</span>
        <span class="c1"># text_features: (seq_len, batch, embed_dim)</span>
        <span class="c1"># image_features: (num_patches, batch, embed_dim)</span>

        <span class="c1"># Cross-attention: text attends to image</span>
        <span class="n">attended_text</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">text_features</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">image_features</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">image_features</span>
        <span class="p">)</span>

        <span class="c1"># Residual connection and layer norm</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">text_features</span> <span class="o">+</span> <span class="n">attended_text</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span>
</code></pre></div>
<p><strong>Research Directions in Multimodal Embeddings</strong>:</p>
<ol>
<li><strong>Large-Scale Multimodal Models</strong>:</li>
<li>DALL-E, DALL-E 2, Stable Diffusion</li>
<li>GPT-4V (Vision), LLaVA, BLIP-2</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/2102.12092">DALL-E</a>, <a href="https://arxiv.org/abs/2304.08485">LLaVA</a></p>
</li>
<li>
<p><strong>Video Understanding</strong>:</p>
</li>
<li>Temporal modeling in video embeddings</li>
<li>Action recognition and video retrieval</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/1904.01766">VideoBERT</a>, <a href="https://arxiv.org/abs/2306.05424">Video-ChatGPT</a></p>
</li>
<li>
<p><strong>3D and Spatial Embeddings</strong>:</p>
</li>
<li>Point cloud representations</li>
<li>3D scene understanding</li>
<li>
<p>Papers: <a href="https://arxiv.org/abs/1612.00593">PointNet</a>, <a href="https://arxiv.org/abs/2003.08934">NeRF</a></p>
</li>
<li>
<p><strong>Efficient Multimodal Training</strong>:</p>
</li>
<li>Parameter-efficient fine-tuning</li>
<li>Modality-specific adapters</li>
<li>Papers: <a href="https://arxiv.org/abs/2005.00247">AdapterFusion</a>, <a href="https://arxiv.org/abs/2106.09685">LoRA</a></li>
</ol>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision (CLIP)</a> (Radford et al., 2021)
- <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ViT)</a> (Dosovitskiy et al., 2021)
- <a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a> (Baevski et al., 2020)
- <a href="https://arxiv.org/abs/2212.04356">Robust Speech Recognition via Large-Scale Weak Supervision (Whisper)</a> (Radford et al., 2022)
- <a href="https://arxiv.org/abs/2201.12086">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a> (Li et al., 2022)</p>
<h3 id="image-embeddings">Image Embeddings</h3>
<h4 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h4>
<p>CNNs revolutionized computer vision by learning hierarchical features from images. The convolutional operation is defined as:</p>
<div class="arithmatex">\[S(i, j) = (I * K)(i, j) = \sum_m \sum_n I(i+m, j+n) K(m, n)\]</div>
<p>where <span class="arithmatex">\(I\)</span> is the input image, <span class="arithmatex">\(K\)</span> is the kernel, and <span class="arithmatex">\(S\)</span> is the output feature map.</p>
<h5 id="cnn-architecture-components">CNN Architecture Components</h5>
<ol>
<li><strong>Convolutional Layers</strong>: The core building block that applies filters to detect features:</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(\mathbf{h}_{i,j,d} = \sum_{c=1}^{C} \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} \mathbf{W}_{m,n,c,d} \cdot \mathbf{x}_{i+m, j+n, c} + \mathbf{b}_d\)</span>\)</span></p>
<p>where:
   - <span class="arithmatex">\(\mathbf{h}_{i,j,d}\)</span> is the output at position <span class="arithmatex">\((i,j)\)</span> for the <span class="arithmatex">\(d\)</span>-th output channel
   - <span class="arithmatex">\(\mathbf{W}\)</span> is the kernel of size <span class="arithmatex">\(k \times k \times C \times D\)</span> (height, width, input channels, output channels)
   - <span class="arithmatex">\(\mathbf{x}\)</span> is the input tensor
   - <span class="arithmatex">\(\mathbf{b}_d\)</span> is the bias term for the <span class="arithmatex">\(d\)</span>-th output channel
   - <span class="arithmatex">\(C\)</span> is the number of input channels</p>
<ol>
<li><strong>Pooling Layers</strong>: Reduce spatial dimensions while preserving important features:</li>
<li>Max Pooling: <span class="arithmatex">\(\mathbf{h}_{i,j} = \max_{0\leq m&lt;s, 0\leq n&lt;s} \mathbf{x}_{s\cdot i+m, s\cdot j+n}\)</span></li>
<li>Average Pooling: <span class="arithmatex">\(\mathbf{h}_{i,j} = \frac{1}{s^2}\sum_{m=0}^{s-1} \sum_{n=0}^{s-1} \mathbf{x}_{s\cdot i+m, s\cdot j+n}\)</span></li>
</ol>
<p>where <span class="arithmatex">\(s\)</span> is the stride/pool size.</p>
<ol>
<li><strong>Normalization Layers</strong>:</li>
<li>Batch Normalization: <span class="arithmatex">\(\hat{\mathbf{x}} = \frac{\mathbf{x} - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \cdot \gamma + \beta\)</span></li>
<li>
<p>Layer Normalization: Normalizes across channels for each sample</p>
</li>
<li>
<p><strong>Activation Functions</strong>:</p>
</li>
<li>ReLU: <span class="arithmatex">\(f(x) = \max(0, x)\)</span></li>
<li>Leaky ReLU: <span class="arithmatex">\(f(x) = \max(\alpha x, x)\)</span> where <span class="arithmatex">\(\alpha\)</span> is a small constant</li>
<li>
<p>ELU: <span class="arithmatex">\(f(x) = \begin{cases} x &amp; \text{if } x &gt; 0 \\ \alpha(e^x - 1) &amp; \text{if } x \leq 0 \end{cases}\)</span></p>
</li>
<li>
<p><strong>Fully Connected Layers</strong>: Transform feature maps into embeddings:</p>
</li>
<li><span class="arithmatex">\(\mathbf{h} = \mathbf{W} \cdot \mathbf{x} + \mathbf{b}\)</span></li>
</ol>
<p>Models like ResNet introduced skip connections to address the vanishing gradient problem:</p>
<div class="arithmatex">\[y = F(x, \{W_i\}) + x\]</div>
<p>where <span class="arithmatex">\(F\)</span> represents the residual mapping to be learned.</p>
<h5 id="major-cnn-architectures-for-embeddings">Major CNN Architectures for Embeddings</h5>
<ol>
<li><strong>AlexNet</strong> (2012):</li>
<li>5 convolutional layers, 3 fully connected layers</li>
<li>First major CNN success on ImageNet</li>
<li>60 million parameters</li>
<li>
<p>Introduced ReLU activations, dropout, and data augmentation</p>
</li>
<li>
<p><strong>VGG</strong> (2014):</p>
</li>
<li>Simple, uniform architecture with 33 convolutions</li>
<li>Very deep (16-19 layers)</li>
<li>138 million parameters (VGG-16)</li>
<li>
<p>Embedding dimension: 4096 (fc7 layer)</p>
</li>
<li>
<p><strong>ResNet</strong> (2015):</p>
</li>
<li>Introduced residual connections: <span class="arithmatex">\(\mathbf{h} = F(\mathbf{x}) + \mathbf{x}\)</span></li>
<li>Solved vanishing gradient problem in very deep networks</li>
<li>Variants from 18 to 152 layers</li>
<li>
<p>Embedding dimension: 2048 (final layer before classification)</p>
</li>
<li>
<p><strong>Inception/GoogLeNet</strong> (2014):</p>
</li>
<li>Multi-scale processing using parallel convolutions</li>
<li>Efficient use of parameters (6.8 million)</li>
<li>
<p>Embedding dimension: 1024 (pool5 layer)</p>
</li>
<li>
<p><strong>EfficientNet</strong> (2019):</p>
</li>
<li>Compound scaling of depth, width, and resolution</li>
<li>State-of-the-art performance with fewer parameters</li>
<li>Variants from B0 (5.3M parameters) to B7 (66M parameters)</li>
<li>Embedding dimension: varies by model size (1280 for B0)</li>
</ol>
<h5 id="cnn-embedding-extraction-techniques">CNN Embedding Extraction Techniques</h5>
<ol>
<li><strong>Global Average Pooling (GAP)</strong>:</li>
<li>Average all spatial locations in the final convolutional layer</li>
<li><span class="arithmatex">\(\mathbf{h}_c = \frac{1}{H \times W} \sum_{i=1}^{H} \sum_{j=1}^{W} \mathbf{x}_{i,j,c}\)</span></li>
<li>Dimension equals number of channels in final conv layer</li>
<li>
<p>Spatially invariant representation</p>
</li>
<li>
<p><strong>Global Max Pooling (GMP)</strong>:</p>
</li>
<li>Take maximum activation across spatial dimensions</li>
<li>
<p>More sensitive to distinctive features</p>
</li>
<li>
<p><strong>Fully Connected Layer Activations</strong>:</p>
</li>
<li>Use activations from penultimate layer (before classification)</li>
<li>
<p>Higher dimensional but more discriminative</p>
</li>
<li>
<p><strong>Multi-level Feature Aggregation</strong>:</p>
</li>
<li>Combine features from multiple layers for richer representation</li>
<li><span class="arithmatex">\(\mathbf{h} = [\text{GAP}(\mathbf{x}^{(l_1)}), \text{GAP}(\mathbf{x}^{(l_2)}), ..., \text{GAP}(\mathbf{x}^{(l_n)})]\)</span></li>
<li>Captures both low-level and high-level features</li>
</ol>
<h5 id="training-objectives-for-cnn-embeddings">Training Objectives for CNN Embeddings</h5>
<ol>
<li><strong>Supervised Classification</strong>:</li>
<li>Traditional cross-entropy loss: <span class="arithmatex">\(L = -\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(p_{i,c})\)</span></li>
<li>
<p>Embeddings emerge as a byproduct of classification training</p>
</li>
<li>
<p><strong>Metric Learning</strong>:</p>
</li>
<li>Contrastive loss: <span class="arithmatex">\(L = \sum_{i=1}^{N} \sum_{j=1}^{N} y_{i,j} d(\mathbf{h}_i, \mathbf{h}_j)^2 + (1-y_{i,j}) \max(0, m - d(\mathbf{h}_i, \mathbf{h}_j))^2\)</span></li>
<li>Triplet loss: <span class="arithmatex">\(L = \sum_{i=1}^{N} \max(0, d(\mathbf{h}_i, \mathbf{h}_i^+) - d(\mathbf{h}_i, \mathbf{h}_i^-) + m)\)</span></li>
<li>
<p>N-pair loss, angular loss, etc.</p>
</li>
<li>
<p><strong>Self-supervised Learning</strong>:</p>
</li>
<li>Pretext tasks: rotation prediction, jigsaw puzzles, colorization</li>
<li>Contrastive predictive coding</li>
<li>SimCLR, MoCo, BYOL, etc.</li>
</ol>
<h5 id="applications-of-cnn-embeddings">Applications of CNN Embeddings</h5>
<ol>
<li><strong>Image Retrieval</strong>:</li>
<li>Content-based image retrieval systems</li>
<li>Reverse image search</li>
<li>
<p>Product recommendation</p>
</li>
<li>
<p><strong>Face Recognition</strong>:</p>
</li>
<li>FaceNet, ArcFace, CosFace use CNN embeddings</li>
<li>
<p>Verification via embedding distance</p>
</li>
<li>
<p><strong>Transfer Learning</strong>:</p>
</li>
<li>Feature extraction for downstream tasks</li>
<li>
<p>Fine-tuning on domain-specific data</p>
</li>
<li>
<p><strong>Image Clustering and Organization</strong>:</p>
</li>
<li>Unsupervised grouping of similar images</li>
<li>Visual data exploration</li>
</ol>
<h5 id="implementation-considerations">Implementation Considerations</h5>
<ol>
<li><strong>Feature Normalization</strong>:</li>
<li>L2 normalization: <span class="arithmatex">\(\hat{\mathbf{h}} = \frac{\mathbf{h}}{\|\mathbf{h}\|_2}\)</span></li>
<li>
<p>Improves performance in similarity calculations</p>
</li>
<li>
<p><strong>Dimensionality Reduction</strong>:</p>
</li>
<li>PCA, t-SNE, or UMAP for visualization</li>
<li>
<p>Linear projection layers for efficiency</p>
</li>
<li>
<p><strong>Data Augmentation</strong>:</p>
</li>
<li>Random crops, flips, rotations, color jittering</li>
<li>
<p>Improves robustness and generalization</p>
</li>
<li>
<p><strong>Fine-tuning Strategies</strong>:</p>
</li>
<li>Layer-wise learning rates</li>
<li>Progressive unfreezing</li>
</ol>
<p><strong>Key Papers</strong>:
- <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (Krizhevsky et al., 2012)
- <a href="https://arxiv.org/abs/1409.1556">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> (Simonyan &amp; Zisserman, 2014)
- <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a> (He et al., 2015)
- <a href="https://arxiv.org/abs/1905.11946">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</a> (Tan &amp; Le, 2019)
- <a href="https://arxiv.org/abs/2002.05709">A Simple Framework for Contrastive Learning of Visual Representations</a> (Chen et al., 2020)</p>
<h4 id="vision-transformers-vit-2020-present">Vision Transformers (ViT) (2020-present)</h4>
<p>Vision Transformers (ViT) revolutionized computer vision by adapting the Transformer architecture from NLP to images, demonstrating that self-attention mechanisms can effectively process visual data without convolutional operations.</p>
<h5 id="vit-architecture">ViT Architecture</h5>
<ol>
<li><strong>Image Patching and Embedding</strong>:</li>
<li>The input image <span class="arithmatex">\(x \in \mathbb{R}^{H \times W \times C}\)</span> is divided into <span class="arithmatex">\(N\)</span> non-overlapping patches <span class="arithmatex">\(x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}\)</span></li>
<li>Typically, patches are of size <span class="arithmatex">\(P \times P\)</span> (e.g., 1616 pixels)</li>
<li>
<p>Each patch is flattened and linearly projected to a <span class="arithmatex">\(D\)</span>-dimensional embedding space: <span class="arithmatex">\(E \in \mathbb{R}^{(P^2 \cdot C) \times D}\)</span></p>
</li>
<li>
<p><strong>Sequence Construction</strong>:</p>
</li>
<li>A learnable classification token <span class="arithmatex">\(x_{class} \in \mathbb{R}^D\)</span> is prepended to the sequence</li>
<li>Position embeddings <span class="arithmatex">\(E_{pos} \in \mathbb{R}^{(N+1) \times D}\)</span> are added to retain positional information</li>
<li>
<p>The resulting sequence is: <span class="arithmatex">\(<span class="arithmatex">\(z_0 = [x_{class}; x_p^1 E; x_p^2 E; ...; x_p^N E] + E_{pos}\)</span>\)</span></p>
</li>
<li>
<p><strong>Transformer Encoder</strong>:</p>
</li>
<li>The sequence is processed through <span class="arithmatex">\(L\)</span> Transformer encoder blocks</li>
<li>
<p>Each block contains:</p>
<ul>
<li>Multi-head self-attention (MSA): <span class="arithmatex">\(\text{MSA}(\text{LN}(z_{l-1}))\)</span></li>
<li>Layer normalization (LN): <span class="arithmatex">\(\text{LN}(z)\)</span></li>
<li>MLP with GELU activation: <span class="arithmatex">\(\text{MLP}(\text{LN}(z'))\)</span></li>
<li>Residual connections: <span class="arithmatex">\(z_l = \text{MLP}(\text{LN}(z')) + z'\)</span> where <span class="arithmatex">\(z' = \text{MSA}(\text{LN}(z_{l-1})) + z_{l-1}\)</span></li>
</ul>
</li>
<li>
<p><strong>Output Representation</strong>:</p>
</li>
<li>For classification, the representation of the classification token from the final layer <span class="arithmatex">\(z_L^0\)</span> is used</li>
<li>For embedding generation, either the classification token or a pooled representation of all patch tokens can be used</li>
</ol>
<h5 id="multi-head-self-attention-in-vit">Multi-Head Self-Attention in ViT</h5>
<p>The self-attention mechanism in ViT follows the standard Transformer formulation:</p>
<ol>
<li><strong>Query, Key, Value Projections</strong>:</li>
<li>
<p><span class="arithmatex">\(Q = z W_Q\)</span>, <span class="arithmatex">\(K = z W_K\)</span>, <span class="arithmatex">\(V = z W_V\)</span> where <span class="arithmatex">\(W_Q, W_K, W_V \in \mathbb{R}^{D \times d_k}\)</span></p>
</li>
<li>
<p><strong>Attention Calculation</strong>:</p>
</li>
<li>
<p><span class="arithmatex">\(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span></p>
</li>
<li>
<p><strong>Multi-Head Mechanism</strong>:</p>
</li>
<li><span class="arithmatex">\(\text{MSA}(z) = [\text{head}_1; \text{head}_2; ...; \text{head}_h]W^O\)</span></li>
<li><span class="arithmatex">\(\text{head}_i = \text{Attention}(zW_Q^i, zW_K^i, zW_V^i)\)</span></li>
<li><span class="arithmatex">\(W^O \in \mathbb{R}^{(h \cdot d_k) \times D}\)</span></li>
</ol>
<h5 id="vit-variants-and-improvements">ViT Variants and Improvements</h5>
<ol>
<li><strong>DeiT</strong> (Data-efficient Image Transformer):</li>
<li>Introduced distillation token and teacher-student training</li>
<li>Enabled training on smaller datasets without extensive pre-training</li>
<li>
<p>Distillation loss: <span class="arithmatex">\(L = \alpha L_{CE}(y_{cls}, y) + \beta L_{CE}(y_{dist}, y) + \gamma L_{KL}(y_{dist}, y_{teacher})\)</span></p>
</li>
<li>
<p><strong>Swin Transformer</strong>:</p>
</li>
<li>Hierarchical architecture with shifted windows</li>
<li>Computational complexity reduced from <span class="arithmatex">\(O(N^2)\)</span> to <span class="arithmatex">\(O(N)\)</span></li>
<li>
<p>Window-based self-attention: <span class="arithmatex">\(\text{Attention}(Q_w, K_w, V_w)\)</span> for each window <span class="arithmatex">\(w\)</span></p>
</li>
<li>
<p><strong>CvT</strong> (Convolutional vision Transformer):</p>
</li>
<li>Incorporates convolutional projections for tokens</li>
<li>
<p>Combines strengths of CNNs and Transformers</p>
</li>
<li>
<p><strong>MViT</strong> (Multiscale Vision Transformer):</p>
</li>
<li>Pooling-based dimension reduction across layers</li>
<li>
<p>Creates a pyramid of feature resolutions</p>
</li>
<li>
<p><strong>ViT-G</strong> (Giant):</p>
</li>
<li>Scaled up to 2 billion parameters</li>
<li>Pre-trained on JFT-3B dataset</li>
<li>State-of-the-art performance on many benchmarks</li>
</ol>
<h5 id="training-strategies-for-vit">Training Strategies for ViT</h5>
<ol>
<li><strong>Pre-training Approaches</strong>:</li>
<li>Supervised pre-training on large labeled datasets (e.g., JFT-300M)</li>
<li>Self-supervised pre-training (e.g., DINO, MAE, BEiT)</li>
<li>
<p>Hybrid approaches combining different objectives</p>
</li>
<li>
<p><strong>Self-Supervised Learning for ViT</strong>:</p>
</li>
<li>
<p><strong>DINO</strong> (Self-Distillation with No Labels):</p>
<ul>
<li>Uses a teacher-student architecture</li>
<li>Momentum encoder and multi-crop strategy</li>
<li>Loss: <span class="arithmatex">\(L = -\sum_i p_t^i \log p_s^i\)</span> where <span class="arithmatex">\(p_t\)</span> and <span class="arithmatex">\(p_s\)</span> are teacher and student probability distributions</li>
</ul>
</li>
<li>
<p><strong>MAE</strong> (Masked Autoencoders):</p>
<ul>
<li>Randomly masks a high proportion of image patches (e.g., 75%)</li>
<li>Reconstructs the masked patches using a lightweight decoder</li>
<li>Loss: <span class="arithmatex">\(L = \frac{1}{|M|} \sum_{i \in M} ||x_i - \hat{x}_i||_2^2\)</span> where <span class="arithmatex">\(M\)</span> is the set of masked patches</li>
</ul>
</li>
<li>
<p><strong>BEiT</strong> (BERT Pre-training of Image Transformers):</p>
<ul>
<li>Predicts visual tokens from a discrete VAE instead of raw pixels</li>
<li>Adapts the MLM objective from BERT</li>
</ul>
</li>
<li>
<p><strong>Fine-tuning Techniques</strong>:</p>
</li>
<li>Layer-wise learning rate decay</li>
<li>Head regularization</li>
<li>Stochastic depth</li>
<li>Mixup and CutMix augmentations</li>
</ol>
<h5 id="embedding-extraction-from-vit">Embedding Extraction from ViT</h5>
<ol>
<li><strong>CLS Token Embedding</strong>:</li>
<li>Use the final layer representation of the classification token: <span class="arithmatex">\(h_{CLS} = z_L^0\)</span></li>
<li>
<p>Simple but effective for many tasks</p>
</li>
<li>
<p><strong>Mean Patch Embedding</strong>:</p>
</li>
<li>Average the final layer representations of all patch tokens: <span class="arithmatex">\(h_{mean} = \frac{1}{N} \sum_{i=1}^{N} z_L^i\)</span></li>
<li>
<p>More comprehensive representation of the entire image</p>
</li>
<li>
<p><strong>Attention-Weighted Embedding</strong>:</p>
</li>
<li>Weight patch tokens by their attention scores to the CLS token</li>
<li>
<p><span class="arithmatex">\(h_{att} = \sum_{i=1}^{N} \alpha_i z_L^i\)</span> where <span class="arithmatex">\(\alpha_i\)</span> are attention weights</p>
</li>
<li>
<p><strong>Multi-layer Aggregation</strong>:</p>
</li>
<li>Combine representations from multiple layers</li>
<li><span class="arithmatex">\(h_{multi} = \sum_{l=1}^{L} w_l \cdot \text{Pool}(z_l)\)</span></li>
<li>Captures both low-level and high-level features</li>
</ol>
<h5 id="applications-of-vit-embeddings">Applications of ViT Embeddings</h5>
<ol>
<li><strong>Image Retrieval</strong>:</li>
<li>DINO embeddings show strong performance for instance-level retrieval</li>
<li>
<p>Self-supervised ViT embeddings capture semantic similarities effectively</p>
</li>
<li>
<p><strong>Zero-shot Transfer</strong>:</p>
</li>
<li>ViT embeddings generalize well to unseen domains and tasks</li>
<li>
<p>Particularly effective when pre-trained on diverse, large-scale datasets</p>
</li>
<li>
<p><strong>Visual Localization</strong>:</p>
</li>
<li>Attention maps from ViT can localize objects without explicit supervision</li>
<li>
<p>Useful for weakly supervised object detection</p>
</li>
<li>
<p><strong>Image Segmentation</strong>:</p>
</li>
<li>Patch-level embeddings can be used for semantic segmentation</li>
<li>
<p>Self-attention maps provide object boundary information</p>
</li>
<li>
<p><strong>Cross-modal Applications</strong>:</p>
</li>
<li>ViT embeddings can be aligned with text embeddings (as in CLIP)</li>
<li>Enables text-to-image retrieval and generation</li>
</ol>
<h5 id="advantages-and-limitations">Advantages and Limitations</h5>
<p><strong>Advantages</strong>:
- Global receptive field from the first layer
- Strong scaling properties with model and data size
- Flexibility in handling variable input resolutions
- State-of-the-art performance when properly trained</p>
<p><strong>Limitations</strong>:
- Quadratic complexity with respect to sequence length
- Data hunger (requires more training data than CNNs)
- Positional encoding limitations for very high resolutions
- Computationally intensive training</p>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/2010.11929">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a> (Dosovitskiy et al., 2020)
- <a href="https://arxiv.org/abs/2012.12877">Training data-efficient image transformers &amp; distillation through attention</a> (Touvron et al., 2021)
- <a href="https://arxiv.org/abs/2103.14030">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a> (Liu et al., 2021)
- <a href="https://arxiv.org/abs/2104.14294">Emerging Properties in Self-Supervised Vision Transformers</a> (Caron et al., 2021)
- <a href="https://arxiv.org/abs/2111.06377">Masked Autoencoders Are Scalable Vision Learners</a> (He et al., 2021)</p>
<h4 id="clip-contrastive-language-image-pre-training-2021-present">CLIP: Contrastive Language-Image Pre-training (2021-present)</h4>
<p>CLIP (Contrastive Language-Image Pre-training) represents a breakthrough in multimodal learning by aligning visual and textual representations in a shared embedding space through contrastive learning at scale. This approach enables remarkable zero-shot capabilities and has become a foundation for numerous downstream applications.</p>
<h5 id="clip-architecture">CLIP Architecture</h5>
<p>CLIP consists of two parallel encoders:</p>
<ol>
<li><strong>Image Encoder</strong>:</li>
<li>Can be either a CNN (ResNet) or a Vision Transformer (ViT)</li>
<li>Processes an image <span class="arithmatex">\(I\)</span> to produce an image embedding <span class="arithmatex">\(i = E_I(I) \in \mathbb{R}^d\)</span></li>
<li>The embedding is L2-normalized: <span class="arithmatex">\(\hat{i} = i / \|i\|_2\)</span></li>
<li>
<p>ViT variants generally outperform ResNet variants</p>
</li>
<li>
<p><strong>Text Encoder</strong>:</p>
</li>
<li>Transformer-based architecture similar to GPT</li>
<li>Processes text <span class="arithmatex">\(T\)</span> to produce a text embedding <span class="arithmatex">\(t = E_T(T) \in \mathbb{R}^d\)</span></li>
<li>The embedding is L2-normalized: <span class="arithmatex">\(\hat{t} = t / \|t\|_2\)</span></li>
<li>
<p>Uses causal attention masks but takes the final token's representation</p>
</li>
<li>
<p><strong>Projection Layers</strong>:</p>
</li>
<li>Both encoders include a final linear projection layer to map to the shared embedding space</li>
<li>These projections align the dimensionality and distribution of the embeddings</li>
</ol>
<h5 id="training-methodology">Training Methodology</h5>
<ol>
<li><strong>Contrastive Learning Objective</strong>:</li>
<li>CLIP uses a symmetric cross-entropy loss over cosine similarities</li>
<li>For a batch of <span class="arithmatex">\(N\)</span> (image, text) pairs, the loss is:</li>
</ol>
<p><span class="arithmatex">\(<span class="arithmatex">\(L = \frac{1}{2}\left(L_{i\rightarrow t} + L_{t\rightarrow i}\right)\)</span>\)</span></p>
<p>where:</p>
<p><span class="arithmatex">\(<span class="arithmatex">\(L_{i\rightarrow t} = -\frac{1}{N}\sum_{m=1}^{N} \log \frac{\exp(\text{sim}(i_m, t_m)/\tau)}{\sum_{n=1}^N \exp(\text{sim}(i_m, t_n)/\tau)}\)</span>\)</span></p>
<p><span class="arithmatex">\(<span class="arithmatex">\(L_{t\rightarrow i} = -\frac{1}{N}\sum_{m=1}^{N} \log \frac{\exp(\text{sim}(t_m, i_m)/\tau)}{\sum_{n=1}^N \exp(\text{sim}(t_m, i_n)/\tau)}\)</span>\)</span></p>
<ul>
<li><span class="arithmatex">\(\text{sim}(i, t) = i^T t\)</span> is the cosine similarity between normalized embeddings</li>
<li>
<p><span class="arithmatex">\(\tau\)</span> is a learnable temperature parameter that scales the logits</p>
</li>
<li>
<p><strong>Training Data</strong>:</p>
</li>
<li>400 million (image, text) pairs collected from the internet</li>
<li>Minimal filtering for English text and image dimensions</li>
<li>No human annotation or curation</li>
<li>
<p>Wide diversity of concepts, styles, and domains</p>
</li>
<li>
<p><strong>Training Process</strong>:</p>
</li>
<li>Trained from scratch (no pre-training)</li>
<li>Adam optimizer with decoupled weight decay</li>
<li>Cosine learning rate schedule with warmup</li>
<li>Mixed-precision training</li>
<li>Large batch sizes (32,768 pairs)</li>
</ul>
<h5 id="clip-variants-and-scaling">CLIP Variants and Scaling</h5>
<ol>
<li><strong>Model Scales</strong>:</li>
<li>ResNet variants: ResNet-50, ResNet-101, ResNet-504, ResNet-5016, ResNet-5064</li>
<li>ViT variants: ViT-B/32, ViT-B/16, ViT-L/14, ViT-L/14@336px</li>
<li>
<p>Largest model has 428 million parameters</p>
</li>
<li>
<p><strong>Improved Variants</strong>:</p>
</li>
<li><strong>OpenCLIP</strong>: Open-source implementation with additional training on LAION datasets</li>
<li><strong>CLIP-ViT-H</strong>: Larger model with ViT-H/14 architecture</li>
<li><strong>DeCLIP</strong>: Adds self-supervised objectives to improve with less data</li>
<li><strong>SLIP</strong>: Combines contrastive language-image pre-training with self-supervised learning</li>
<li>
<p><strong>EVA-CLIP</strong>: Enhanced visual representation with masked image modeling</p>
</li>
<li>
<p><strong>Efficiency Improvements</strong>:</p>
</li>
<li><strong>LiT</strong> (Locked-image Tuning): Freezes pre-trained image encoder and only trains text encoder</li>
<li><strong>FLAVA</strong>: Unified foundation model for joint vision-and-language understanding</li>
</ol>
<h5 id="embedding-properties-and-extraction">Embedding Properties and Extraction</h5>
<ol>
<li><strong>Embedding Dimensionality</strong>:</li>
<li>Typically 512 or 768 dimensions depending on model size</li>
<li>
<p>Embeddings are L2-normalized to lie on a unit hypersphere</p>
</li>
<li>
<p><strong>Extraction Methods</strong>:</p>
</li>
<li><strong>Image Embeddings</strong>: Forward pass through image encoder + projection</li>
<li><strong>Text Embeddings</strong>: Forward pass through text encoder + projection</li>
<li>
<p>Both can be used independently for unimodal tasks</p>
</li>
<li>
<p><strong>Embedding Properties</strong>:</p>
</li>
<li>Semantic alignment between modalities</li>
<li>Compositional understanding (e.g., "a red cube on a blue sphere")</li>
<li>Robust to distribution shifts</li>
<li>Captures both fine-grained and abstract concepts</li>
</ol>
<h5 id="zero-shot-capabilities">Zero-Shot Capabilities</h5>
<ol>
<li><strong>Classification</strong>:</li>
<li>Construct text prompts for each class (e.g., "a photo of a {class}")</li>
<li>Encode each prompt with the text encoder</li>
<li>Encode the query image with the image encoder</li>
<li>
<p>Predict the class with highest cosine similarity</p>
</li>
<li>
<p><strong>Prompt Engineering</strong>:</p>
</li>
<li>Performance can be significantly improved with better prompts</li>
<li>Ensemble of prompts (e.g., "a photo of a {class}", "a picture of a {class}", etc.)</li>
<li>
<p>Context-specific prompts (e.g., "a satellite image of a {class}")</p>
</li>
<li>
<p><strong>Few-Shot Learning</strong>:</p>
</li>
<li>CLIP embeddings can be used as features for linear probing</li>
<li>Requires significantly fewer examples than traditional approaches</li>
</ol>
<h5 id="applications-of-clip-embeddings">Applications of CLIP Embeddings</h5>
<ol>
<li><strong>Cross-Modal Retrieval</strong>:</li>
<li>Text-to-image search: Find images matching a text description</li>
<li>Image-to-text search: Generate captions or find relevant text</li>
<li>
<p>Enables semantic search beyond keyword matching</p>
</li>
<li>
<p><strong>Zero-Shot Recognition</strong>:</p>
</li>
<li>Object classification without task-specific training</li>
<li>Domain adaptation across visual distributions</li>
<li>
<p>Out-of-distribution detection</p>
</li>
<li>
<p><strong>Content Creation</strong>:</p>
</li>
<li>Guidance for text-to-image generation models (DALL-E, Stable Diffusion)</li>
<li>Image editing through textual directions</li>
<li>
<p>Style transfer based on textual descriptions</p>
</li>
<li>
<p><strong>Multimodal Understanding</strong>:</p>
</li>
<li>Visual question answering</li>
<li>Image captioning</li>
<li>
<p>Visual reasoning</p>
</li>
<li>
<p><strong>Representation Learning</strong>:</p>
</li>
<li>Foundation for fine-tuning on downstream tasks</li>
<li>Transfer learning to specialized domains</li>
<li>Feature extraction for classical ML pipelines</li>
</ol>
<h5 id="limitations-and-challenges">Limitations and Challenges</h5>
<ol>
<li><strong>Biases</strong>:</li>
<li>Reflects and potentially amplifies biases in internet data</li>
<li>Social biases (gender, race, etc.) are encoded in the embeddings</li>
<li>
<p>Geographical and cultural biases due to data distribution</p>
</li>
<li>
<p><strong>Reasoning Limitations</strong>:</p>
</li>
<li>Limited understanding of spatial relationships</li>
<li>Struggles with counting and numerical reasoning</li>
<li>
<p>Difficulty with fine-grained visual details</p>
</li>
<li>
<p><strong>Computational Requirements</strong>:</p>
</li>
<li>Large models require significant compute for training</li>
<li>
<p>Inference can be resource-intensive for real-time applications</p>
</li>
<li>
<p><strong>Domain Gaps</strong>:</p>
</li>
<li>Performance drops on specialized domains (medical, scientific, etc.)</li>
<li>May require domain-specific fine-tuning</li>
</ol>
<h5 id="implementation-considerations_1">Implementation Considerations</h5>
<ol>
<li><strong>Prompt Design</strong>:</li>
<li>Critical for optimal performance</li>
<li>Domain-specific prompts often work better</li>
<li>
<p>Ensembling multiple prompts improves robustness</p>
</li>
<li>
<p><strong>Embedding Caching</strong>:</p>
</li>
<li>Pre-compute embeddings for efficiency in retrieval systems</li>
<li>
<p>Approximate nearest neighbor search for large-scale applications</p>
</li>
<li>
<p><strong>Fine-tuning Strategies</strong>:</p>
</li>
<li>Linear probing vs. full fine-tuning</li>
<li>Adapter layers for parameter-efficient tuning</li>
<li>Domain-specific contrastive tuning</li>
</ol>
<p><strong>Key Papers and Resources</strong>:
- <a href="https://arxiv.org/abs/2103.00020">Learning Transferable Visual Models From Natural Language Supervision</a> (Radford et al., 2021)
- <a href="https://arxiv.org/abs/2102.05918">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a> (Jia et al., 2021)
- <a href="https://arxiv.org/abs/2111.07991">LiT: Zero-Shot Transfer with Locked-image Text Tuning</a> (Zhai et al., 2022)
- <a href="https://arxiv.org/abs/2112.04482">FLAVA: A Foundational Language And Vision Alignment Model</a> (Singh et al., 2022)
- <a href="https://arxiv.org/abs/2303.15389">EVA-CLIP: Improved Training Techniques for CLIP at Scale</a> (Sun et al., 2023)</p>
<h3 id="audio-embeddings_1">Audio Embeddings</h3>
<h4 id="wav2vec-and-wav2vec-20">Wav2Vec and Wav2Vec 2.0</h4>
<p>Wav2Vec learns representations from raw audio by solving a contrastive task that requires distinguishing true future audio samples from distractors. Wav2Vec 2.0 extends this with a masked prediction task similar to BERT's MLM.</p>
<p>The contrastive loss in Wav2Vec 2.0 is:</p>
<div class="arithmatex">\[L_c = -\log \frac{\exp(\text{sim}(c_t, q_t)/\kappa)}{\sum_{\tilde{t} \in \{t\} \cup N_t} \exp(\text{sim}(c_{\tilde{t}}, q_t)/\kappa)}\]</div>
<p>where <span class="arithmatex">\(c_t\)</span> is the true quantized latent speech representation, <span class="arithmatex">\(q_t\)</span> is the context network output, and <span class="arithmatex">\(N_t\)</span> is a set of distractors.</p>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/1904.05862">wav2vec: Unsupervised Pre-training for Speech Recognition</a> (Schneider et al., 2019)
- <a href="https://arxiv.org/abs/2006.11477">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a> (Baevski et al., 2020)</p>
<h4 id="whisper">Whisper</h4>
<p>Whisper is a robust speech recognition system trained on a large and diverse dataset of audio-text pairs. It uses a sequence-to-sequence Transformer architecture with an encoder-decoder design:</p>
<ol>
<li>The encoder processes the audio spectrograms</li>
<li>The decoder generates text transcriptions autoregressively</li>
</ol>
<p>Whisper's encoder uses a convolutional frontend to process the mel spectrogram before the Transformer layers:</p>
<div class="arithmatex">\[X_0 = \text{Conv2d}(\text{MelSpectrogram}(\text{audio}))\]</div>
<p>Followed by Transformer encoder layers:</p>
<div class="arithmatex">\[X_{l+1} = X_l + \text{Attention}(\text{LayerNorm}(X_l)) + \text{FFN}(\text{LayerNorm}(X_l + \text{Attention}(\text{LayerNorm}(X_l))))\]</div>
<p><strong>Key Paper</strong>: <a href="https://arxiv.org/abs/2212.04356">Robust Speech Recognition via Large-Scale Weak Supervision</a> (Radford et al., 2022)</p>
<h4 id="hubert-and-wavlm">HuBERT and WavLM</h4>
<p>HuBERT (Hidden-Unit BERT) applies masked prediction to audio by first clustering the continuous speech signal into discrete units. WavLM extends HuBERT with denoising and speaker disentanglement objectives.</p>
<p>The HuBERT pre-training objective is:</p>
<div class="arithmatex">\[L = \sum_{t \in M} \log p(c_t | \tilde{X})\]</div>
<p>where <span class="arithmatex">\(M\)</span> is the set of masked indices, <span class="arithmatex">\(c_t\)</span> is the cluster assignment of the true frame, and <span class="arithmatex">\(\tilde{X}\)</span> is the masked input sequence.</p>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/2106.07447">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a> (Hsu et al., 2021)
- <a href="https://arxiv.org/abs/2110.13900">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a> (Chen et al., 2021)</p>
<h3 id="multimodal-embeddings_1">Multimodal Embeddings</h3>
<p>Multimodal embeddings aim to create unified representations across different modalities (text, image, audio). The key challenge is aligning these diverse modalities in a shared semantic space.</p>
<h4 id="joint-embedding-space-models">Joint Embedding Space Models</h4>
<p>These models project different modalities into a common embedding space where semantically similar content is positioned closely regardless of modality.</p>
<p>The alignment objective often uses contrastive learning:</p>
<div class="arithmatex">\[L = \sum_{i=1}^N \sum_{j=1}^N -y_{ij} \log \frac{\exp(\text{sim}(x_i, x_j)/\tau)}{\sum_{k=1}^N \exp(\text{sim}(x_i, x_k)/\tau)}\]</div>
<p>where <span class="arithmatex">\(y_{ij} = 1\)</span> if <span class="arithmatex">\(x_i\)</span> and <span class="arithmatex">\(x_j\)</span> are semantically related across modalities, and 0 otherwise.</p>
<h4 id="multimodal-transformers">Multimodal Transformers</h4>
<p>Models like CLIP, ALIGN, and FLAVA use separate encoders for different modalities followed by alignment layers. More recent approaches like Flamingo and GPT-4 integrate multiple modalities more deeply within a single architecture.</p>
<p>The cross-attention mechanism often used in these models is:</p>
<div class="arithmatex">\[\text{CrossAttention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\]</div>
<p>where <span class="arithmatex">\(Q\)</span> comes from one modality and <span class="arithmatex">\(K, V\)</span> from another.</p>
<p><strong>Key Papers</strong>:
- <a href="https://arxiv.org/abs/2112.04482">FLAVA: A Foundational Language And Vision Alignment Model</a> (Singh et al., 2022)
- <a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a> (Alayrac et al., 2022)
- <a href="https://arxiv.org/abs/2305.05665">ImageBind: One Embedding Space To Bind Them All</a> (Girdhar et al., 2023)</p>
<h2 id="features">Features</h2>
<ul>
<li><strong>Multiple Frameworks</strong>: Support for various embedding frameworks including SentenceTransformers, OpenAI, Google Gemini, CLIP, Wav2Vec2, Whisper, and more.</li>
<li><strong>Modality Support</strong>: Text, image, audio, and multimodal embedding capabilities with a consistent interface.</li>
<li><strong>Unified Interface</strong>: Consistent API across different frameworks and modalities.</li>
<li><strong>Dynamic Framework Detection</strong>: Automatically detects available frameworks based on installed packages.</li>
<li><strong>Batch Processing</strong>: Efficient batch embedding generation for multiple inputs.</li>
<li><strong>Similarity Calculation</strong>: Built-in methods for calculating cosine similarity between embeddings.</li>
</ul>
<h2 id="supported-frameworks">Supported Frameworks</h2>
<h3 id="text-embedding-frameworks">Text Embedding Frameworks</h3>
<ul>
<li><strong>SentenceTransformers</strong>: High-quality text embeddings using Hugging Face models</li>
<li><strong>OpenAI</strong>: State-of-the-art embeddings via OpenAI's API</li>
<li><strong>Google Gemini</strong>: Google's embedding models</li>
<li><strong>Jina</strong>: Jina AI's embedding models</li>
<li><strong>NVIDIA NeMo</strong>: NVIDIA's NV-Embed models</li>
<li><strong>Stella</strong>: Stella AI's embedding models</li>
<li><strong>ModernBERT</strong>: Modern BERT-based embedding models</li>
<li><strong>Cohere</strong>: Cohere's embedding models</li>
<li><strong>HuggingFace</strong>: Direct access to Hugging Face's embedding models</li>
</ul>
<h3 id="image-embedding-frameworks">Image Embedding Frameworks</h3>
<ul>
<li><strong>CLIP</strong>: OpenAI's CLIP models for image embeddings</li>
<li><strong>OpenAI</strong>: OpenAI's image embedding API</li>
<li><strong>Google Gemini</strong>: Google's multimodal embedding models</li>
<li><strong>PyTorch Image Models (timm)</strong>: Various image models from the timm library</li>
<li><strong>Vision Transformer (ViT)</strong>: Transformer-based image embedding models</li>
<li><strong>ResNet</strong>: ResNet-based image embedding models</li>
</ul>
<h3 id="audio-embedding-frameworks">Audio Embedding Frameworks</h3>
<ul>
<li><strong>Wav2Vec2</strong>: Facebook AI's self-supervised speech representation models</li>
<li><strong>Whisper</strong>: OpenAI's speech recognition and transcription models</li>
<li><strong>HuBERT</strong>: Facebook AI's self-supervised speech representation models</li>
<li><strong>WavLM</strong>: Microsoft's state-of-the-art speech representation model</li>
<li><strong>Data2Vec</strong>: Facebook AI's multi-modal self-supervised model</li>
<li><strong>OpenAI</strong>: OpenAI's audio embedding API</li>
<li><strong>Google Gemini</strong>: Google's multimodal embedding models</li>
</ul>
<h2 id="installation">Installation</h2>
<p>The core module has minimal dependencies, but each framework requires its own dependencies to be installed.</p>
<div class="highlight"><pre><span></span><code><span class="c1"># Core dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>numpy<span class="w"> </span>pillow<span class="w"> </span>matplotlib

<span class="c1"># SentenceTransformers</span>
pip<span class="w"> </span>install<span class="w"> </span>sentence-transformers

<span class="c1"># OpenAI</span>
pip<span class="w"> </span>install<span class="w"> </span>openai

<span class="c1"># Google Gemini</span>
pip<span class="w"> </span>install<span class="w"> </span>google-generativeai

<span class="c1"># CLIP</span>
pip<span class="w"> </span>install<span class="w"> </span>ftfy<span class="w"> </span>regex<span class="w"> </span>tqdm<span class="w"> </span>git+https://github.com/openai/CLIP.git

<span class="c1"># PyTorch Image Models</span>
pip<span class="w"> </span>install<span class="w"> </span>timm

<span class="c1"># Vision Transformer</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers

<span class="c1"># ResNet</span>
pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision

<span class="c1"># Audio dependencies</span>
pip<span class="w"> </span>install<span class="w"> </span>torchaudio<span class="w"> </span>librosa<span class="w"> </span>soundfile

<span class="c1"># Wav2Vec2, Whisper, HuBERT, WavLM, Data2Vec</span>
pip<span class="w"> </span>install<span class="w"> </span>transformers
</code></pre></div>
<h2 id="usage">Usage</h2>
<h3 id="text-embedding">Text Embedding</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llm_multi_core.embedder</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_text_embedder</span>

<span class="c1"># Create a text embedder with SentenceTransformers</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">create_text_embedder</span><span class="p">(</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;sentence-transformers&quot;</span><span class="p">)</span>

<span class="c1"># Generate embedding for a single text</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">)</span>

<span class="c1"># Generate embeddings for multiple texts</span>
<span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="s2">&quot;How are you?&quot;</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">texts</span><span class="p">)</span>

<span class="c1"># Calculate similarity between two texts</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Hello, world!&quot;</span><span class="p">,</span> <span class="s2">&quot;Hi, world!&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity: </span><span class="si">{</span><span class="n">similarity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="image-embedding">Image Embedding</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llm_multi_core.embedder</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_image_embedder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>

<span class="c1"># Create an image embedder with CLIP</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">create_image_embedder</span><span class="p">(</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;clip&quot;</span><span class="p">)</span>

<span class="c1"># Generate embedding for a single image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;image.jpg&quot;</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="c1"># Generate embeddings for multiple images</span>
<span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;image_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.jpg&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

<span class="c1"># Calculate similarity between two images</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;image1.jpg&quot;</span><span class="p">,</span> <span class="s2">&quot;image2.jpg&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity: </span><span class="si">{</span><span class="n">similarity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="audio-embedding">Audio Embedding</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llm_multi_core.embedder</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_audio_embedder</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">librosa</span>

<span class="c1"># Create an audio embedder with Wav2Vec2</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">create_audio_embedder</span><span class="p">(</span><span class="n">framework</span><span class="o">=</span><span class="s2">&quot;wav2vec2&quot;</span><span class="p">)</span>

<span class="c1"># Generate embedding for a single audio file</span>
<span class="n">audio</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;audio.wav&quot;</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">audio</span><span class="p">)</span>

<span class="c1"># Generate embeddings for multiple audio files</span>
<span class="n">audio_files</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;audio_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.wav&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)]</span>
<span class="n">audio_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">file</span> <span class="ow">in</span> <span class="n">audio_files</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">audio_data</span><span class="p">)</span>

<span class="c1"># Calculate similarity between two audio files</span>
<span class="n">similarity</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;audio1.wav&quot;</span><span class="p">,</span> <span class="s2">&quot;audio2.wav&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Similarity: </span><span class="si">{</span><span class="n">similarity</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="multimodal-embedding">Multimodal Embedding</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llm_multi_core.embedder</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_multimodal_embedder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">PIL</span><span class="w"> </span><span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">librosa</span>

<span class="c1"># Create a multimodal embedder</span>
<span class="n">embedder</span> <span class="o">=</span> <span class="n">create_multimodal_embedder</span><span class="p">(</span>
    <span class="n">text_framework</span><span class="o">=</span><span class="s2">&quot;sentence-transformers&quot;</span><span class="p">,</span>
    <span class="n">image_framework</span><span class="o">=</span><span class="s2">&quot;clip&quot;</span><span class="p">,</span>
    <span class="n">audio_framework</span><span class="o">=</span><span class="s2">&quot;wav2vec2&quot;</span>
<span class="p">)</span>

<span class="c1"># Generate embeddings for mixed inputs</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;A beautiful sunset&quot;</span><span class="p">,</span>  <span class="c1"># Text</span>
    <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;sunset.jpg&quot;</span><span class="p">),</span>  <span class="c1"># Image</span>
    <span class="s2">&quot;A cute puppy&quot;</span><span class="p">,</span>  <span class="c1"># Text</span>
    <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="s2">&quot;puppy.jpg&quot;</span><span class="p">),</span>  <span class="c1"># Image</span>
    <span class="n">librosa</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;bird_chirping.wav&quot;</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16000</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># Audio</span>
<span class="p">]</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">embed_batch</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Calculate similarity between different modalities</span>
<span class="n">similarity_text_image</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;A beautiful sunset&quot;</span><span class="p">,</span> <span class="s2">&quot;sunset.jpg&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text-Image Similarity: </span><span class="si">{</span><span class="n">similarity_text_image</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">similarity_image_audio</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;sunset.jpg&quot;</span><span class="p">,</span> <span class="s2">&quot;bird_chirping.wav&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Image-Audio Similarity: </span><span class="si">{</span><span class="n">similarity_image_audio</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">similarity_text_audio</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="s2">&quot;Bird sounds&quot;</span><span class="p">,</span> <span class="s2">&quot;bird_chirping.wav&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Text-Audio Similarity: </span><span class="si">{</span><span class="n">similarity_text_audio</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h3 id="checking-available-frameworks">Checking Available Frameworks</h3>
<div class="highlight"><pre><span></span><code><span class="kn">from</span><span class="w"> </span><span class="nn">llm_multi_core.embedder</span><span class="w"> </span><span class="kn">import</span> <span class="n">get_available_embedders</span>

<span class="c1"># Get available frameworks for all modalities</span>
<span class="n">available</span> <span class="o">=</span> <span class="n">get_available_embedders</span><span class="p">()</span>

<span class="c1"># Print available text frameworks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Available Text Frameworks:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">framework</span><span class="p">,</span> <span class="n">available</span> <span class="ow">in</span> <span class="n">available</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">status</span> <span class="o">=</span> <span class="s2">&quot;Available&quot;</span> <span class="k">if</span> <span class="n">available</span> <span class="k">else</span> <span class="s2">&quot;Not available&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">framework</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print available image frameworks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Available Image Frameworks:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">framework</span><span class="p">,</span> <span class="n">available</span> <span class="ow">in</span> <span class="n">available</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">status</span> <span class="o">=</span> <span class="s2">&quot;Available&quot;</span> <span class="k">if</span> <span class="n">available</span> <span class="k">else</span> <span class="s2">&quot;Not available&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">framework</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Print available audio frameworks</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Available Audio Frameworks:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">framework</span><span class="p">,</span> <span class="n">available</span> <span class="ow">in</span> <span class="n">available</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">status</span> <span class="o">=</span> <span class="s2">&quot;Available&quot;</span> <span class="k">if</span> <span class="n">available</span> <span class="k">else</span> <span class="s2">&quot;Not available&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">framework</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
<h2 id="examples">Examples</h2>
<p>See the <code>examples.py</code> file for complete examples of using the embedder module with different frameworks and modalities.</p>
<h2 id="practical-applications-of-embeddings">Practical Applications of Embeddings</h2>
<h3 id="information-retrieval-and-search">Information Retrieval and Search</h3>
<p>Embeddings enable semantic search beyond keyword matching. Documents and queries are embedded in the same vector space, allowing retrieval based on semantic similarity rather than lexical overlap.</p>
<p>The retrieval process typically involves:</p>
<ol>
<li>Offline indexing: Embed all documents in a collection</li>
<li>Query processing: Embed the user query</li>
<li>Similarity search: Find documents with embeddings closest to the query embedding</li>
</ol>
<p>The similarity score between query <span class="arithmatex">\(q\)</span> and document <span class="arithmatex">\(d\)</span> is often computed as:</p>
<div class="arithmatex">\[\text{score}(q, d) = \frac{\vec{q} \cdot \vec{d}}{||\vec{q}|| \cdot ||\vec{d}||}\]</div>
<h3 id="recommendation-systems">Recommendation Systems</h3>
<p>Embeddings can represent users and items in a shared space, enabling content-based and collaborative filtering approaches. The recommendation score is often the dot product of user and item embeddings:</p>
<div class="arithmatex">\[\text{score}(u, i) = \vec{u} \cdot \vec{i}\]</div>
<h3 id="clustering-and-classification">Clustering and Classification</h3>
<p>Embeddings transform raw data into a space where traditional distance-based algorithms can capture semantic relationships. For clustering, algorithms like K-means can be applied directly to embeddings:</p>
<div class="arithmatex">\[\text{cluster}_k = \arg\min_{\mu_k} \sum_{x_i \in S_k} ||x_i - \mu_k||^2\]</div>
<p>where <span class="arithmatex">\(S_k\)</span> is the set of points in cluster <span class="arithmatex">\(k\)</span> and <span class="arithmatex">\(\mu_k\)</span> is the centroid.</p>
<h3 id="cross-modal-retrieval">Cross-Modal Retrieval</h3>
<p>Multimodal embeddings enable searching across modalities, such as finding images based on text descriptions or retrieving audio clips that match a textual query.</p>
<h3 id="zero-shot-learning">Zero-Shot Learning</h3>
<p>Models like CLIP enable classifying images into arbitrary categories without specific training examples, by comparing image embeddings with text embeddings of class names.</p>
<h2 id="architecture_1">Architecture</h2>
<p>The embedder module is organized into the following components:</p>
<ul>
<li><strong>BaseEmbedder</strong>: Abstract base class defining the common interface for all embedders.</li>
<li><strong>TextEmbedder</strong>: Implementation for text embedding using various frameworks.</li>
<li><strong>ImageEmbedder</strong>: Implementation for image embedding using various frameworks.</li>
<li><strong>AudioEmbedder</strong>: Implementation for audio embedding using various frameworks.</li>
<li><strong>MultiModalEmbedder</strong>: Implementation for multimodal embedding, combining text, image, and audio embedders.</li>
</ul>
<h2 id="evaluating-embedding-quality">Evaluating Embedding Quality</h2>
<p>Assessing the quality of embeddings is crucial for both research and practical applications. Different evaluation methods are appropriate for different modalities and use cases.</p>
<h3 id="intrinsic-evaluation">Intrinsic Evaluation</h3>
<p>Intrinsic evaluation measures how well embeddings capture semantic relationships without considering downstream tasks.</p>
<h4 id="word-similarity-and-relatedness">Word Similarity and Relatedness</h4>
<p>For word embeddings, standard benchmarks include:</p>
<ul>
<li><strong>WordSim-353</strong>: Measures correlation between human similarity judgments and cosine similarity of word embeddings</li>
<li><strong>SimLex-999</strong>: Focuses on similarity rather than relatedness</li>
<li><strong>MEN</strong>: Contains 3,000 word pairs with human-assigned similarity scores</li>
</ul>
<p>The evaluation metric is typically Spearman's rank correlation coefficient:</p>
<div class="arithmatex">\[\rho = 1 - \frac{6\sum d_i^2}{n(n^2-1)}\]</div>
<p>where <span class="arithmatex">\(d_i\)</span> is the difference between the ranks of corresponding values and <span class="arithmatex">\(n\)</span> is the number of pairs.</p>
<h4 id="analogy-tasks">Analogy Tasks</h4>
<p>Analogy tasks evaluate whether embeddings capture relational similarities, such as "man is to woman as king is to queen."</p>
<p>The accuracy is calculated as:</p>
<div class="arithmatex">\[\text{Accuracy} = \frac{\text{Number of correctly solved analogies}}{\text{Total number of analogies}}\]</div>
<h4 id="clustering-and-visualization">Clustering and Visualization</h4>
<p>Techniques like t-SNE and UMAP can visualize embeddings in 2D or 3D space, allowing qualitative assessment of how well semantically similar items cluster together.</p>
<h3 id="extrinsic-evaluation">Extrinsic Evaluation</h3>
<p>Extrinsic evaluation measures how well embeddings perform on downstream tasks.</p>
<h4 id="text-classification">Text Classification</h4>
<p>Embeddings are used as features for classifiers, with performance measured using metrics like accuracy, F1-score, and AUC:</p>
<div class="arithmatex">\[F1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}\]</div>
<h4 id="information-retrieval">Information Retrieval</h4>
<p>Embeddings are evaluated on retrieval tasks using metrics like Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG):</p>
<div class="arithmatex">\[\text{NDCG@k} = \frac{\text{DCG@k}}{\text{IDCG@k}}\]</div>
<p>where:</p>
<div class="arithmatex">\[\text{DCG@k} = \sum_{i=1}^{k} \frac{\text{rel}_i}{\log_2(i+1)}\]</div>
<h4 id="cross-modal-retrieval_1">Cross-Modal Retrieval</h4>
<p>For multimodal embeddings, evaluation often involves retrieving items of one modality given a query in another modality (e.g., text-to-image retrieval). Metrics include Recall@K and Median Rank.</p>
<h3 id="benchmarks-for-modern-embeddings">Benchmarks for Modern Embeddings</h3>
<ul>
<li><strong>MTEB (Massive Text Embedding Benchmark)</strong>: Evaluates text embeddings across 56 datasets spanning classification, clustering, retrieval, and more</li>
<li><strong>BEIR (Benchmarking IR)</strong>: Focuses on zero-shot information retrieval across diverse domains</li>
<li><strong>CLIP Score</strong>: Measures alignment between images and text in multimodal models</li>
<li><strong>ImageNet</strong>: Standard benchmark for image embeddings</li>
<li><strong>SUPERB (Speech processing Universal PERformance Benchmark)</strong>: Evaluates speech representations across various tasks</li>
</ul>
<h2 id="future-directions-in-embedding-research">Future Directions in Embedding Research</h2>
<p>The field of embeddings continues to evolve rapidly. Here are some promising research directions:</p>
<h3 id="multimodal-foundation-models">Multimodal Foundation Models</h3>
<p>Models that can seamlessly process and align multiple modalities (text, image, audio, video, 3D) in a single architecture are becoming increasingly important. Research is focusing on:</p>
<ul>
<li><strong>Cross-modal transfer learning</strong>: Leveraging knowledge from one modality to improve representations in another</li>
<li><strong>Unified representation spaces</strong>: Creating embedding spaces that maintain semantic relationships across all modalities</li>
<li><strong>Emergent capabilities</strong>: Understanding how multimodal training leads to capabilities not present in single-modality models</li>
</ul>
<h3 id="efficiency-and-compression">Efficiency and Compression</h3>
<p>As embedding models grow larger, research on making them more efficient becomes crucial:</p>
<ul>
<li><strong>Distillation</strong>: Transferring knowledge from large teacher models to smaller student models</li>
<li><strong>Quantization</strong>: Reducing the precision of model weights (e.g., from 32-bit to 8-bit or 4-bit)</li>
<li><strong>Pruning</strong>: Removing less important weights or neurons from models</li>
<li><strong>Sparse representations</strong>: Using embeddings where most dimensions are zero</li>
</ul>
<h3 id="interpretability-and-fairness">Interpretability and Fairness</h3>
<p>Understanding what information is encoded in embeddings and ensuring they are fair and unbiased:</p>
<ul>
<li><strong>Probing tasks</strong>: Designing experiments to determine what linguistic or visual concepts are captured in embeddings</li>
<li><strong>Debiasing techniques</strong>: Methods to remove unwanted social biases from embeddings</li>
<li><strong>Causal analysis</strong>: Understanding how embeddings relate to causal factors in the data</li>
</ul>
<h3 id="compositional-and-hierarchical-embeddings">Compositional and Hierarchical Embeddings</h3>
<p>Developing embeddings that better capture compositional structure:</p>
<ul>
<li><strong>Hierarchical representations</strong>: Embeddings that represent information at multiple levels of abstraction</li>
<li><strong>Compositional generalization</strong>: Creating embeddings that generalize to novel combinations of familiar concepts</li>
<li><strong>Structured representations</strong>: Incorporating explicit structure (e.g., graphs, trees) into embedding spaces</li>
</ul>
<h3 id="continual-learning-and-adaptation">Continual Learning and Adaptation</h3>
<p>Enabling embedding models to adapt to new data and tasks without forgetting:</p>
<ul>
<li><strong>Parameter-efficient fine-tuning</strong>: Methods like LoRA, adapters, and prompt tuning</li>
<li><strong>Rehearsal mechanisms</strong>: Techniques to prevent catastrophic forgetting</li>
<li><strong>Meta-learning</strong>: Learning to learn, enabling rapid adaptation to new tasks</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "..", "features": [], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>